{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxCXGWY70IaOW96l2ch96n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/dl/Hopfield_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hopfield Network Model Background"
      ],
      "metadata": {
        "id": "RgxbYQRE-cng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hopfield neural network, introduced by John Hopfield in 1982, is a type of recurrent artificial neural network used for pattern recognition, optimization, and associative memory tasks. It is a form of content-addressable memory, meaning it can retrieve stored patterns by their content rather than using explicit addresses. The network is fully connected, and each neuron is connected to every other neuron in the network, forming a symmetric weight matrix.\n",
        "\n",
        "Here are some pros and cons of the Hopfield neural network:\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "1. Content-addressable memory: One of the significant advantages of Hopfield networks is their ability to retrieve stored patterns based on their content rather than using explicit memory addresses. This makes them suitable for tasks involving pattern completion or pattern recognition.\n",
        "\n",
        "2. Simple architecture: Hopfield networks have a relatively straightforward structure, which makes them easy to implement and understand. They are particularly attractive for smaller-scale problems.\n",
        "\n",
        "3. Robustness: The network can handle noisy or incomplete input patterns and can often still recall the correct stored pattern, thanks to the dynamics of its attractor states.\n",
        "\n",
        "4. Energy function: Hopfield networks can be described by an energy function, and during operation, the network tends to converge to local energy minima, which correspond to stored patterns. This property is used in optimization problems.\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "1. Limited capacity: One significant limitation of Hopfield networks is their limited storage capacity. As the number of stored patterns increases, the network can suffer from spurious or false attractors, leading to degraded performance.\n",
        "\n",
        "2. Slow convergence: The convergence time of Hopfield networks can be relatively slow, especially for large networks or complex patterns.\n",
        "\n",
        "3. Lack of explicit learning: Hopfield networks do not have a structured learning process like modern artificial neural networks. The patterns are typically stored directly in the weights, and there is no supervised or unsupervised learning involved.\n",
        "\n",
        "4. Non-flexible connections: The fully connected nature of Hopfield networks may lead to some inefficiencies, especially when dealing with sparse patterns or when specific patterns should not be associated.\n",
        "\n",
        "**When to use Hopfield neural networks**:\n",
        "\n",
        "1. Associative memory tasks: Hopfield networks are well-suited for tasks involving pattern association and recall, making them useful in applications like autoassociative memory or content-addressable memory systems.\n",
        "\n",
        "2. Pattern completion: If you have incomplete or noisy patterns and want to retrieve the closest matching stored pattern, Hopfield networks can be effective in completing the missing information.\n",
        "\n",
        "3. Optimization problems: The Hopfield network's energy function can be exploited to solve certain optimization problems, such as the traveling salesman problem or the quadratic assignment problem.\n",
        "\n",
        "4. Small-scale problems: Due to their limited storage capacity and slow convergence, Hopfield networks are more suitable for smaller-scale problems where these limitations are less critical.\n",
        "\n",
        "In summary, Hopfield neural networks are a simple and effective choice for associative memory tasks, pattern completion, and certain optimization problems, especially when dealing with smaller-scale applications. However, their limited capacity and lack of structured learning make them less suitable for large-scale and complex tasks that can be better addressed by modern neural network architectures."
      ],
      "metadata": {
        "id": "nyJRscQI9yux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "f2Z_VTfwBxmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class HopfieldNetwork:\n",
        "    def __init__(self, num_neurons):\n",
        "        self.num_neurons = num_neurons\n",
        "        self.weights = np.zeros((num_neurons, num_neurons))\n",
        "\n",
        "    def train(self, patterns):\n",
        "        num_patterns = len(patterns)\n",
        "        for pattern in patterns:\n",
        "            pattern = np.reshape(pattern, (self.num_neurons, 1))\n",
        "            self.weights += np.dot(pattern, pattern.T)\n",
        "            np.fill_diagonal(self.weights, 0)\n",
        "\n",
        "    def predict(self, pattern, max_iterations=100, async_update=True):\n",
        "        pattern = np.reshape(pattern, (self.num_neurons, 1))\n",
        "        iteration = 0\n",
        "\n",
        "        while iteration < max_iterations:\n",
        "            if async_update:\n",
        "                index = np.random.randint(0, self.num_neurons)\n",
        "                activation = np.dot(self.weights[index, :], pattern)\n",
        "                pattern[index] = np.sign(activation)\n",
        "            else:\n",
        "                new_pattern = np.dot(self.weights, pattern)\n",
        "                pattern = np.sign(new_pattern)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        return pattern\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a Hopfield network with 4 neurons\n",
        "    num_neurons = 4\n",
        "    hopfield_net = HopfieldNetwork(num_neurons)\n",
        "\n",
        "    # Define the patterns to be learned\n",
        "    patterns = [\n",
        "        [1, 1, 1, -1],\n",
        "        [1, -1, -1, 1],\n",
        "        [-1, 1, 1, 1]\n",
        "    ]\n",
        "\n",
        "    # Train the network with the patterns\n",
        "    hopfield_net.train(patterns)\n",
        "\n",
        "    # Test the network by recalling patterns\n",
        "    test_pattern = [1, 1, 1, -1]\n",
        "    result = hopfield_net.predict(test_pattern)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Test Pattern: \", test_pattern)\n",
        "    print(\"Recovered Pattern: \", np.reshape(result, (1, num_neurons)))\n"
      ],
      "metadata": {
        "id": "IAKrCEXcX7eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "ipFoLY9NHIgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Import necessary libraries:\n",
        "   - `import numpy as np`: Imports the NumPy library, which is used for numerical operations and array handling.\n",
        "\n",
        "2. Define the HopfieldNetwork class:\n",
        "   - `class HopfieldNetwork:`: Defines the HopfieldNetwork class.\n",
        "\n",
        "3. Initialize the network:\n",
        "   - `def __init__(self, num_neurons)`: The constructor takes the number of neurons (units) in the network as an input and initializes the network with zero weights.\n",
        "   - `self.num_neurons = num_neurons`: Saves the number of neurons as a class attribute.\n",
        "   - `self.weights = np.zeros((num_neurons, num_neurons))`: Initializes the weight matrix as a 2D NumPy array of zeros with dimensions `(num_neurons, num_neurons)`. The weights represent the connections between neurons.\n",
        "\n",
        "4. Train the network:\n",
        "   - `def train(self, patterns)`: This method trains the Hopfield network using the Hebbian learning rule. The input `patterns` is a list of binary patterns (vectors) that the network should learn to recognize.\n",
        "   - `num_patterns = len(patterns)`: Computes the number of patterns in the input list.\n",
        "   - For each pattern in the list:\n",
        "     - `pattern = np.reshape(pattern, (self.num_neurons, 1))`: Reshapes the pattern into a column vector of size `(num_neurons, 1)`. This is necessary for matrix multiplication in the weight update step.\n",
        "     - `self.weights += np.dot(pattern, pattern.T)`: Updates the weights using the outer product of the pattern with itself. This implements the Hebbian learning rule, where the weight between two neurons is strengthened if they fire together.\n",
        "     - `np.fill_diagonal(self.weights, 0)`: Sets the diagonal elements of the weight matrix to zero to avoid self-connections (neuron with itself).\n",
        "\n",
        "5. Predict using the network:\n",
        "   - `def predict(self, pattern, max_iterations=100, async_update=True)`: This method performs pattern recall (prediction) using the trained network.\n",
        "   - `pattern = np.reshape(pattern, (self.num_neurons, 1))`: Reshapes the input pattern into a column vector of size `(num_neurons, 1)`.\n",
        "   - The method then enters a loop for a maximum of `max_iterations` iterations (default is 100).\n",
        "     - If `async_update` is set to `True`, it selects a random neuron (index) from the network and updates its value asynchronously using the sign function. The activation of a neuron is determined by the dot product of its weight row and the input pattern. The sign function returns 1 if the activation is positive or zero, and -1 if it is negative.\n",
        "     - If `async_update` is set to `False`, it performs a synchronous update by computing the dot product of the weight matrix with the current pattern. The new pattern is obtained by applying the sign function element-wise to the result.\n",
        "   - The loop continues until the specified `max_iterations` is reached.\n",
        "\n",
        "6. Example usage:\n",
        "   - The code demonstrates the usage of the HopfieldNetwork class with a simple example.\n",
        "   - It creates a Hopfield network with 4 neurons.\n",
        "   - It defines three binary patterns to be learned (`patterns`).\n",
        "   - It trains the network with the patterns using the `train` method.\n",
        "   - It then tests the network by recalling one of the learned patterns (`test_pattern`) using the `predict` method.\n",
        "   - The result is printed to the console, showing the original test pattern and the pattern recovered by the network.\n",
        "\n",
        "This simple Hopfield Network example demonstrates the basic principles of pattern recall using the Hebbian learning rule. Note that the network has its limitations and is primarily used for small binary pattern recognition tasks."
      ],
      "metadata": {
        "id": "EUKSl4rXYC_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "7rQSZYTJSVYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A real-world example of the Hopfield Network model in a healthcare setting is the use of Hopfield Networks for medical image restoration or denoising. Medical images obtained from various imaging modalities (such as MRI, CT, or X-ray) are often corrupted by noise during the acquisition process. Noise can negatively impact the accuracy of diagnosis and subsequent medical decisions. Hopfield Networks can be employed to effectively denoise and restore such medical images.\n",
        "\n",
        "Here's how the Hopfield Network can be used for medical image denoising:\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "   - The first step involves acquiring the medical images using the respective imaging modalities.\n",
        "   - The acquired images may be corrupted with random noise, which can make accurate analysis challenging.\n",
        "   - Before feeding the images into the Hopfield Network, the images may need to be normalized and resized to a fixed input size.\n",
        "\n",
        "2. **Image Representation:**\n",
        "   - Each medical image is represented as a binary pattern (0 or 1) in the Hopfield Network.\n",
        "   - The pixels in the image can be thresholded to convert grayscale images to binary patterns.\n",
        "\n",
        "3. **Hopfield Network Setup:**\n",
        "   - The Hopfield Network is a recurrent neural network with a single layer of binary neurons that act as both input and output.\n",
        "   - The network is fully connected, and each neuron is connected to all other neurons.\n",
        "   - The neurons are updated synchronously following a defined update rule.\n",
        "\n",
        "4. **Training the Hopfield Network:**\n",
        "   - The training of the Hopfield Network involves storing the corrupted images as attractors (stable states) in the network.\n",
        "   - Each image is presented to the network multiple times until the network settles into a stable state.\n",
        "\n",
        "5. **Image Restoration (Denoising):**\n",
        "   - To denoise a corrupted medical image, the noisy image is presented to the Hopfield Network as input.\n",
        "   - The network is allowed to settle, and it will converge to the closest attractor (stable state) that represents the denoised image.\n",
        "   - The network dynamics eventually remove the noise and restore the image to a cleaner version.\n",
        "\n",
        "6. **Postprocessing and Analysis:**\n",
        "   - The denoised medical image obtained from the Hopfield Network can be postprocessed to enhance visualization and clarity.\n",
        "   - The denoised image can be used for medical analysis, diagnosis, or any other specific application, leading to more accurate and reliable results.\n",
        "\n",
        "Using Hopfield Networks for medical image denoising can help improve the quality of images, leading to better diagnostic accuracy and enhancing the effectiveness of healthcare practices. However, it's important to note that Hopfield Networks have limitations in scalability and may not be suitable for large-scale or high-dimensional datasets commonly encountered in modern medical imaging. Other deep learning-based denoising methods, such as convolutional neural networks (CNNs) and variational autoencoders (VAEs), are often preferred for large-scale medical image restoration tasks due to their superior performance and scalability."
      ],
      "metadata": {
        "id": "0x1ntSaR-870"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "o9mzI5R3ZHOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the Hopfield Network model?\n",
        "   The Hopfield Network is a type of recurrent artificial neural network designed to store and recall patterns. It was introduced by John Hopfield in 1982. The network consists of binary threshold units interconnected in a fully connected manner, and it has the ability to converge to stable states based on the patterns it has learned.\n",
        "\n",
        "2. How does the Hopfield Network store information?\n",
        "   The Hopfield Network stores information in the form of attractors or stable states. When a pattern is presented to the network during the learning phase, the connection weights between the neurons are adjusted to represent that pattern. These adjusted weights act as energy minima, and the network will tend to converge to these stable states when presented with similar patterns.\n",
        "\n",
        "3. What is the recall process in a Hopfield Network?\n",
        "   The recall process in a Hopfield Network involves presenting a partial or corrupted version of a stored pattern to the network. The network then attempts to converge to the nearest stored pattern or a stable state that is similar to the input pattern.\n",
        "\n",
        "4. Can the Hopfield Network be used for content-addressable memory?\n",
        "   Yes, the Hopfield Network is often used for content-addressable memory. Content-addressable memory allows the network to retrieve stored patterns based on their content rather than their specific addresses. When given a partial or noisy input, the network recalls the most similar stored pattern.\n",
        "\n",
        "5. What are the limitations of the Hopfield Network?\n",
        "   One major limitation of the Hopfield Network is the capacity to store patterns. The number of patterns that can be accurately stored is limited, and the network may suffer from spurious patterns, which are unintended attractors that were not part of the original training set. Additionally, convergence to stable states can be slow for large networks.\n",
        "\n",
        "6. How is energy used in the Hopfield Network?\n",
        "   The Hopfield Network is associated with an energy function, and the convergence of the network to stable states is driven by energy minimization. During recall, the network iteratively updates the states of its neurons to lower the energy until it reaches a stable state.\n",
        "\n",
        "7. What are some applications of the Hopfield Network?\n",
        "   The Hopfield Network has been used in various applications, including content-addressable memory, optimization problems, pattern recognition, and combinatorial optimization tasks. It has also been studied as a model for associative memory and analog computing.\n",
        "\n",
        "8. How is the Hopfield Network different from other neural networks?\n",
        "   The Hopfield Network is a type of recurrent neural network (RNN), which means it has feedback connections that allow information to flow in loops. Unlike feedforward neural networks, Hopfield Networks have bi-directional connections, which enable them to store and recall patterns using attractor states.\n",
        "\n",
        "9. Can the Hopfield Network be used for continuous-valued data?\n",
        "   Originally, the Hopfield Network was designed for binary data. However, it has been extended to handle continuous-valued data using various techniques, such as using sigmoid activation functions or bipolar representations.\n",
        "\n",
        "10. Are there any modern variants of the Hopfield Network?\n",
        "   Yes, there have been several variations and extensions of the Hopfield Network proposed over the years. Some of these include Bidirectional Associative Memories (BAM), Discrete-Time Hopfield Networks, and Continuous-Time Hopfield Networks."
      ],
      "metadata": {
        "id": "6haWXTg_mMcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "DSjU0IxP5Ngp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:** What is the main purpose of a Hopfield Network?\n",
        "\n",
        "a) Image classification  \n",
        "b) Natural language processing  \n",
        "c) Pattern recognition and associative memory  \n",
        "d) Reinforcement learning  \n",
        "\n",
        "**Question 2:** In a Hopfield Network, what type of connections exist between neurons?\n",
        "\n",
        "a) One-way connections  \n",
        "b) No connections  \n",
        "c) Recurrent connections  \n",
        "d) Random connections  \n",
        "\n",
        "**Question 3:** What is the primary activation function used in Hopfield Networks?\n",
        "\n",
        "a) Rectified Linear Unit (ReLU)  \n",
        "b) Sigmoid  \n",
        "c) Hyperbolic Tangent (tanh)  \n",
        "d) Softmax  \n",
        "\n",
        "**Question 4:** How does a Hopfield Network store information?\n",
        "\n",
        "a) In a separate memory unit  \n",
        "b) As weights in connection strengths  \n",
        "c) In the form of reinforcement signals  \n",
        "d) As external memory pointers  \n",
        "\n",
        "**Question 5:** Which property ensures that the energy of the Hopfield Network decreases during training?\n",
        "\n",
        "a) Positive feedback  \n",
        "b) Negative feedback  \n",
        "c) Gradient descent  \n",
        "d) Energy minimization  \n",
        "\n",
        "**Question 6:** What is the energy function used in the Hopfield Network?\n",
        "\n",
        "a) Mean squared error  \n",
        "b) Cross-entropy  \n",
        "c) Boltzmann energy  \n",
        "d) Hamming distance  \n",
        "\n",
        "**Question 7:** What happens during the retrieval phase of a Hopfield Network?\n",
        "\n",
        "a) Neurons are updated randomly  \n",
        "b) Neurons are updated sequentially  \n",
        "c) Neurons are updated in parallel  \n",
        "d) Neurons remain unchanged  \n",
        "\n",
        "**Question 8:** What is the \"attractor\" in the context of a Hopfield Network?\n",
        "\n",
        "a) A type of neuron with higher connectivity  \n",
        "b) An energy barrier that prevents convergence  \n",
        "c) A stable pattern or state the network converges to  \n",
        "d) A dynamic learning rate  \n",
        "\n",
        "**Question 9:** Which statement about Hopfield Networks is true?\n",
        "\n",
        "a) They are well-suited for handling large-scale image datasets.  \n",
        "b) They always converge to the correct stored pattern.  \n",
        "c) They are primarily used for training deep neural networks.  \n",
        "d) They can exhibit spurious states during retrieval.  \n",
        "\n",
        "**Question 10:** What is the maximum number of patterns a Hopfield Network can reliably store?\n",
        "\n",
        "a) Equal to the number of neurons in the network  \n",
        "b) Approximately 2 times the number of neurons  \n",
        "c) It depends on the learning rate used  \n",
        "d) There is no theoretical limit  \n",
        "\n",
        "**Answers:**\n",
        "\n",
        "1. c) Pattern recognition and associative memory\n",
        "2. c) Recurrent connections\n",
        "3. b) Sigmoid\n",
        "4. b) As weights in connection strengths\n",
        "5. d) Energy minimization\n",
        "6. c) Boltzmann energy\n",
        "7. c) Neurons are updated in parallel\n",
        "8. c) A stable pattern or state the network converges to\n",
        "9. d) They can exhibit spurious states during retrieval.\n",
        "10. b) Approximately 2 times the number of neurons"
      ],
      "metadata": {
        "id": "CGqYhN0q5Phv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "ZwzoGAVRtHUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Medical Image Retrieval System**:\n",
        "    - Task: Given a database of medical images, use a Hopfield network to store these images. Then, by using a partial or noisy version of one of these images, retrieve the most similar image from memory.\n",
        "    - Extensions: Students could compare the results with conventional image retrieval methods.\n",
        "\n",
        "2. **Patient History Pattern Recognition**:\n",
        "    - Task: Use patient data history (like symptoms over time) to form patterns. If a new patient presents with partial symptoms, use the network to suggest possible evolutions of the symptom pattern based on historical data.\n",
        "    \n",
        "3. **Disease Diagnosis Using Symptoms**:\n",
        "    - Task: Store known symptom patterns of various diseases. When presented with a noisy or incomplete symptom pattern, the Hopfield network should try to determine the most likely disease.\n",
        "    \n",
        "4. **Predicting Medication Side Effects**:\n",
        "    - Task: Based on stored patterns of medications and their side effects, use a Hopfield network to predict the possible side effects when a patient is given a combination of medications.\n",
        "\n",
        "5. **Genetic Data Pattern Recognition**:\n",
        "    - Task: Using genetic data (like gene sequences), store known patterns associated with certain hereditary diseases. When given partial or noisy genetic data, predict the likelihood of the associated condition.\n",
        "\n",
        "6. **Medical Equipment Calibration**:\n",
        "    - Task: Certain medical equipment might need calibration or pattern recognition (e.g., ECG, EEG machines). Students could simulate how Hopfield networks can be used to recognize and remember calibration patterns.\n",
        "\n",
        "7. **Mental Health Pattern Analysis**:\n",
        "    - Task: Use patient mood logs or psychological profile patterns. The Hopfield network can try to identify periods of stability or potential mood swings based on incomplete or noisy logs.\n",
        "\n",
        "8. **Patient Feedback Analysis**:\n",
        "    - Task: Given patterns of patient feedback (e.g., satisfaction surveys, post-treatment reviews), the network can try to determine overall patient satisfaction or predict likely feedback from incomplete data.\n",
        "\n",
        "9. **Epidemic Spread Pattern Recognition**:\n",
        "    - Task: Based on historical data of how certain epidemics spread (e.g., influenza, COVID-19), use the network to predict potential future spread patterns when given partial current data.\n",
        "\n",
        "10. **Drug Interaction Pattern Recognition**:\n",
        "    - Task: Store patterns of known drug interactions. When presented with a combination of drugs, the Hopfield network should identify potential harmful interactions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HcRIIaktI5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "bI4e1xFENPzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple working example of a Hopfield Network using a healthcare-related dataset. However, please note that Hopfield Networks are more commonly used for associative memory tasks rather than traditional machine learning tasks. In this example, I'll demonstrate how a Hopfield Network can be used for image denoising, which can have applications in medical image processing.\n",
        "\n",
        "Let's assume we have a dataset of medical images that might have been corrupted by noise. We'll use the Hopfield Network to denoise these images."
      ],
      "metadata": {
        "id": "Svu6CSCGNNWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic medical image with noise\n",
        "original_image = np.array([[1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                           [1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                           [1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                           [0, 0, 0, 1, 1, 1, 0, 0],\n",
        "                           [0, 0, 0, 1, 1, 1, 0, 0],\n",
        "                           [0, 0, 0, 1, 1, 1, 0, 0],\n",
        "                           [0, 0, 0, 0, 0, 0, 1, 1],\n",
        "                           [0, 0, 0, 0, 0, 0, 1, 1]])\n",
        "\n",
        "# Add noise to the image\n",
        "noisy_image = original_image + np.random.randint(-1, 2, size=original_image.shape)\n",
        "\n",
        "# Create a Hopfield Network\n",
        "def train_hopfield_network(patterns):\n",
        "    num_neurons = patterns.shape[1]\n",
        "    weights = np.zeros((num_neurons, num_neurons))\n",
        "\n",
        "    for pattern in patterns:\n",
        "        weights += np.outer(pattern, pattern)\n",
        "\n",
        "    np.fill_diagonal(weights, 0)\n",
        "    return weights\n",
        "\n",
        "def update_neuron(weights, state):\n",
        "    s = np.dot(weights, state)\n",
        "    return np.sign(s)\n",
        "\n",
        "# Training the Hopfield Network with the original image\n",
        "patterns = np.reshape(original_image, (1, -1))\n",
        "weights = train_hopfield_network(patterns)\n",
        "\n",
        "# Recover the image using the noisy image as input\n",
        "recovered_image = noisy_image.copy().flatten()\n",
        "for _ in range(10):\n",
        "    recovered_image = update_neuron(weights, recovered_image)\n",
        "\n",
        "recovered_image = np.reshape(recovered_image, noisy_image.shape)\n",
        "\n",
        "# Plot the original, noisy, and recovered images\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(131)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(original_image, cmap='gray')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('Noisy Image')\n",
        "plt.imshow(noisy_image, cmap='gray')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('Recovered Image')\n",
        "plt.imshow(recovered_image, cmap='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2A5F9sicM0KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first generate a synthetic medical image and then add random noise to it. We train a Hopfield Network using the original image as the training pattern. After training, we use the noisy image as input to the Hopfield Network to recover a denoised version of the image.\n",
        "\n",
        "Keep in mind that this is a simple illustrative example. Real-world healthcare applications of Hopfield Networks might involve more complex use cases, such as medical image segmentation or data completion tasks."
      ],
      "metadata": {
        "id": "19QpMS_kNV0S"
      }
    }
  ]
}