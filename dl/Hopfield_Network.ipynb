{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdRR+PFzeZN7fJaeNmUH2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/Hopfield_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "RgxbYQRE-cng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hopfield neural network, introduced by John Hopfield in 1982, is a type of recurrent artificial neural network used for pattern recognition, optimization, and associative memory tasks. It is a form of content-addressable memory, meaning it can retrieve stored patterns by their content rather than using explicit addresses. The network is fully connected, and each neuron is connected to every other neuron in the network, forming a symmetric weight matrix.\n",
        "\n",
        "Here are some pros and cons of the Hopfield neural network:\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "1. Content-addressable memory: One of the significant advantages of Hopfield networks is their ability to retrieve stored patterns based on their content rather than using explicit memory addresses. This makes them suitable for tasks involving pattern completion or pattern recognition.\n",
        "\n",
        "2. Simple architecture: Hopfield networks have a relatively straightforward structure, which makes them easy to implement and understand. They are particularly attractive for smaller-scale problems.\n",
        "\n",
        "3. Robustness: The network can handle noisy or incomplete input patterns and can often still recall the correct stored pattern, thanks to the dynamics of its attractor states.\n",
        "\n",
        "4. Energy function: Hopfield networks can be described by an energy function, and during operation, the network tends to converge to local energy minima, which correspond to stored patterns. This property is used in optimization problems.\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "1. Limited capacity: One significant limitation of Hopfield networks is their limited storage capacity. As the number of stored patterns increases, the network can suffer from spurious or false attractors, leading to degraded performance.\n",
        "\n",
        "2. Slow convergence: The convergence time of Hopfield networks can be relatively slow, especially for large networks or complex patterns.\n",
        "\n",
        "3. Lack of explicit learning: Hopfield networks do not have a structured learning process like modern artificial neural networks. The patterns are typically stored directly in the weights, and there is no supervised or unsupervised learning involved.\n",
        "\n",
        "4. Non-flexible connections: The fully connected nature of Hopfield networks may lead to some inefficiencies, especially when dealing with sparse patterns or when specific patterns should not be associated.\n",
        "\n",
        "**When to use Hopfield neural networks**:\n",
        "\n",
        "1. Associative memory tasks: Hopfield networks are well-suited for tasks involving pattern association and recall, making them useful in applications like autoassociative memory or content-addressable memory systems.\n",
        "\n",
        "2. Pattern completion: If you have incomplete or noisy patterns and want to retrieve the closest matching stored pattern, Hopfield networks can be effective in completing the missing information.\n",
        "\n",
        "3. Optimization problems: The Hopfield network's energy function can be exploited to solve certain optimization problems, such as the traveling salesman problem or the quadratic assignment problem.\n",
        "\n",
        "4. Small-scale problems: Due to their limited storage capacity and slow convergence, Hopfield networks are more suitable for smaller-scale problems where these limitations are less critical.\n",
        "\n",
        "In summary, Hopfield neural networks are a simple and effective choice for associative memory tasks, pattern completion, and certain optimization problems, especially when dealing with smaller-scale applications. However, their limited capacity and lack of structured learning make them less suitable for large-scale and complex tasks that can be better addressed by modern neural network architectures."
      ],
      "metadata": {
        "id": "nyJRscQI9yux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "f2Z_VTfwBxmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class HopfieldNetwork:\n",
        "    def __init__(self, num_neurons):\n",
        "        self.num_neurons = num_neurons\n",
        "        self.weights = np.zeros((num_neurons, num_neurons))\n",
        "\n",
        "    def train(self, patterns):\n",
        "        num_patterns = len(patterns)\n",
        "        for pattern in patterns:\n",
        "            pattern = np.reshape(pattern, (self.num_neurons, 1))\n",
        "            self.weights += np.dot(pattern, pattern.T)\n",
        "            np.fill_diagonal(self.weights, 0)\n",
        "\n",
        "    def predict(self, pattern, max_iterations=100, async_update=True):\n",
        "        pattern = np.reshape(pattern, (self.num_neurons, 1))\n",
        "        iteration = 0\n",
        "\n",
        "        while iteration < max_iterations:\n",
        "            if async_update:\n",
        "                index = np.random.randint(0, self.num_neurons)\n",
        "                activation = np.dot(self.weights[index, :], pattern)\n",
        "                pattern[index] = np.sign(activation)\n",
        "            else:\n",
        "                new_pattern = np.dot(self.weights, pattern)\n",
        "                pattern = np.sign(new_pattern)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        return pattern\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a Hopfield network with 4 neurons\n",
        "    num_neurons = 4\n",
        "    hopfield_net = HopfieldNetwork(num_neurons)\n",
        "\n",
        "    # Define the patterns to be learned\n",
        "    patterns = [\n",
        "        [1, 1, 1, -1],\n",
        "        [1, -1, -1, 1],\n",
        "        [-1, 1, 1, 1]\n",
        "    ]\n",
        "\n",
        "    # Train the network with the patterns\n",
        "    hopfield_net.train(patterns)\n",
        "\n",
        "    # Test the network by recalling patterns\n",
        "    test_pattern = [1, 1, 1, -1]\n",
        "    result = hopfield_net.predict(test_pattern)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Test Pattern: \", test_pattern)\n",
        "    print(\"Recovered Pattern: \", np.reshape(result, (1, num_neurons)))\n"
      ],
      "metadata": {
        "id": "IAKrCEXcX7eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "ipFoLY9NHIgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Import necessary libraries:\n",
        "   - `import numpy as np`: Imports the NumPy library, which is used for numerical operations and array handling.\n",
        "\n",
        "2. Define the HopfieldNetwork class:\n",
        "   - `class HopfieldNetwork:`: Defines the HopfieldNetwork class.\n",
        "\n",
        "3. Initialize the network:\n",
        "   - `def __init__(self, num_neurons)`: The constructor takes the number of neurons (units) in the network as an input and initializes the network with zero weights.\n",
        "   - `self.num_neurons = num_neurons`: Saves the number of neurons as a class attribute.\n",
        "   - `self.weights = np.zeros((num_neurons, num_neurons))`: Initializes the weight matrix as a 2D NumPy array of zeros with dimensions `(num_neurons, num_neurons)`. The weights represent the connections between neurons.\n",
        "\n",
        "4. Train the network:\n",
        "   - `def train(self, patterns)`: This method trains the Hopfield network using the Hebbian learning rule. The input `patterns` is a list of binary patterns (vectors) that the network should learn to recognize.\n",
        "   - `num_patterns = len(patterns)`: Computes the number of patterns in the input list.\n",
        "   - For each pattern in the list:\n",
        "     - `pattern = np.reshape(pattern, (self.num_neurons, 1))`: Reshapes the pattern into a column vector of size `(num_neurons, 1)`. This is necessary for matrix multiplication in the weight update step.\n",
        "     - `self.weights += np.dot(pattern, pattern.T)`: Updates the weights using the outer product of the pattern with itself. This implements the Hebbian learning rule, where the weight between two neurons is strengthened if they fire together.\n",
        "     - `np.fill_diagonal(self.weights, 0)`: Sets the diagonal elements of the weight matrix to zero to avoid self-connections (neuron with itself).\n",
        "\n",
        "5. Predict using the network:\n",
        "   - `def predict(self, pattern, max_iterations=100, async_update=True)`: This method performs pattern recall (prediction) using the trained network.\n",
        "   - `pattern = np.reshape(pattern, (self.num_neurons, 1))`: Reshapes the input pattern into a column vector of size `(num_neurons, 1)`.\n",
        "   - The method then enters a loop for a maximum of `max_iterations` iterations (default is 100).\n",
        "     - If `async_update` is set to `True`, it selects a random neuron (index) from the network and updates its value asynchronously using the sign function. The activation of a neuron is determined by the dot product of its weight row and the input pattern. The sign function returns 1 if the activation is positive or zero, and -1 if it is negative.\n",
        "     - If `async_update` is set to `False`, it performs a synchronous update by computing the dot product of the weight matrix with the current pattern. The new pattern is obtained by applying the sign function element-wise to the result.\n",
        "   - The loop continues until the specified `max_iterations` is reached.\n",
        "\n",
        "6. Example usage:\n",
        "   - The code demonstrates the usage of the HopfieldNetwork class with a simple example.\n",
        "   - It creates a Hopfield network with 4 neurons.\n",
        "   - It defines three binary patterns to be learned (`patterns`).\n",
        "   - It trains the network with the patterns using the `train` method.\n",
        "   - It then tests the network by recalling one of the learned patterns (`test_pattern`) using the `predict` method.\n",
        "   - The result is printed to the console, showing the original test pattern and the pattern recovered by the network.\n",
        "\n",
        "This simple Hopfield Network example demonstrates the basic principles of pattern recall using the Hebbian learning rule. Note that the network has its limitations and is primarily used for small binary pattern recognition tasks."
      ],
      "metadata": {
        "id": "EUKSl4rXYC_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "7rQSZYTJSVYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A real-world example of the Hopfield Network model in a healthcare setting is the use of Hopfield Networks for medical image restoration or denoising. Medical images obtained from various imaging modalities (such as MRI, CT, or X-ray) are often corrupted by noise during the acquisition process. Noise can negatively impact the accuracy of diagnosis and subsequent medical decisions. Hopfield Networks can be employed to effectively denoise and restore such medical images.\n",
        "\n",
        "Here's how the Hopfield Network can be used for medical image denoising:\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "   - The first step involves acquiring the medical images using the respective imaging modalities.\n",
        "   - The acquired images may be corrupted with random noise, which can make accurate analysis challenging.\n",
        "   - Before feeding the images into the Hopfield Network, the images may need to be normalized and resized to a fixed input size.\n",
        "\n",
        "2. **Image Representation:**\n",
        "   - Each medical image is represented as a binary pattern (0 or 1) in the Hopfield Network.\n",
        "   - The pixels in the image can be thresholded to convert grayscale images to binary patterns.\n",
        "\n",
        "3. **Hopfield Network Setup:**\n",
        "   - The Hopfield Network is a recurrent neural network with a single layer of binary neurons that act as both input and output.\n",
        "   - The network is fully connected, and each neuron is connected to all other neurons.\n",
        "   - The neurons are updated synchronously following a defined update rule.\n",
        "\n",
        "4. **Training the Hopfield Network:**\n",
        "   - The training of the Hopfield Network involves storing the corrupted images as attractors (stable states) in the network.\n",
        "   - Each image is presented to the network multiple times until the network settles into a stable state.\n",
        "\n",
        "5. **Image Restoration (Denoising):**\n",
        "   - To denoise a corrupted medical image, the noisy image is presented to the Hopfield Network as input.\n",
        "   - The network is allowed to settle, and it will converge to the closest attractor (stable state) that represents the denoised image.\n",
        "   - The network dynamics eventually remove the noise and restore the image to a cleaner version.\n",
        "\n",
        "6. **Postprocessing and Analysis:**\n",
        "   - The denoised medical image obtained from the Hopfield Network can be postprocessed to enhance visualization and clarity.\n",
        "   - The denoised image can be used for medical analysis, diagnosis, or any other specific application, leading to more accurate and reliable results.\n",
        "\n",
        "Using Hopfield Networks for medical image denoising can help improve the quality of images, leading to better diagnostic accuracy and enhancing the effectiveness of healthcare practices. However, it's important to note that Hopfield Networks have limitations in scalability and may not be suitable for large-scale or high-dimensional datasets commonly encountered in modern medical imaging. Other deep learning-based denoising methods, such as convolutional neural networks (CNNs) and variational autoencoders (VAEs), are often preferred for large-scale medical image restoration tasks due to their superior performance and scalability."
      ],
      "metadata": {
        "id": "0x1ntSaR-870"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "o9mzI5R3ZHOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the Hopfield Network model?\n",
        "   The Hopfield Network is a type of recurrent artificial neural network designed to store and recall patterns. It was introduced by John Hopfield in 1982. The network consists of binary threshold units interconnected in a fully connected manner, and it has the ability to converge to stable states based on the patterns it has learned.\n",
        "\n",
        "2. How does the Hopfield Network store information?\n",
        "   The Hopfield Network stores information in the form of attractors or stable states. When a pattern is presented to the network during the learning phase, the connection weights between the neurons are adjusted to represent that pattern. These adjusted weights act as energy minima, and the network will tend to converge to these stable states when presented with similar patterns.\n",
        "\n",
        "3. What is the recall process in a Hopfield Network?\n",
        "   The recall process in a Hopfield Network involves presenting a partial or corrupted version of a stored pattern to the network. The network then attempts to converge to the nearest stored pattern or a stable state that is similar to the input pattern.\n",
        "\n",
        "4. Can the Hopfield Network be used for content-addressable memory?\n",
        "   Yes, the Hopfield Network is often used for content-addressable memory. Content-addressable memory allows the network to retrieve stored patterns based on their content rather than their specific addresses. When given a partial or noisy input, the network recalls the most similar stored pattern.\n",
        "\n",
        "5. What are the limitations of the Hopfield Network?\n",
        "   One major limitation of the Hopfield Network is the capacity to store patterns. The number of patterns that can be accurately stored is limited, and the network may suffer from spurious patterns, which are unintended attractors that were not part of the original training set. Additionally, convergence to stable states can be slow for large networks.\n",
        "\n",
        "6. How is energy used in the Hopfield Network?\n",
        "   The Hopfield Network is associated with an energy function, and the convergence of the network to stable states is driven by energy minimization. During recall, the network iteratively updates the states of its neurons to lower the energy until it reaches a stable state.\n",
        "\n",
        "7. What are some applications of the Hopfield Network?\n",
        "   The Hopfield Network has been used in various applications, including content-addressable memory, optimization problems, pattern recognition, and combinatorial optimization tasks. It has also been studied as a model for associative memory and analog computing.\n",
        "\n",
        "8. How is the Hopfield Network different from other neural networks?\n",
        "   The Hopfield Network is a type of recurrent neural network (RNN), which means it has feedback connections that allow information to flow in loops. Unlike feedforward neural networks, Hopfield Networks have bi-directional connections, which enable them to store and recall patterns using attractor states.\n",
        "\n",
        "9. Can the Hopfield Network be used for continuous-valued data?\n",
        "   Originally, the Hopfield Network was designed for binary data. However, it has been extended to handle continuous-valued data using various techniques, such as using sigmoid activation functions or bipolar representations.\n",
        "\n",
        "10. Are there any modern variants of the Hopfield Network?\n",
        "   Yes, there have been several variations and extensions of the Hopfield Network proposed over the years. Some of these include Bidirectional Associative Memories (BAM), Discrete-Time Hopfield Networks, and Continuous-Time Hopfield Networks."
      ],
      "metadata": {
        "id": "6haWXTg_mMcF"
      }
    }
  ]
}