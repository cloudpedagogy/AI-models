{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGQoMQe/APkXcdMy6RprZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/BERT_(Bidirectional_Encoder_Representations_from_Transformers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "wNx-ZHFs2evH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a popular neural network model introduced by Google in 2018. It is based on the Transformer architecture and has revolutionized natural language processing (NLP) tasks by pre-training a language representation model on a large corpus of text in an unsupervised manner and then fine-tuning it on specific downstream tasks.\n",
        "\n",
        "Here are some of the pros and cons of BERT:\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "1. **Bidirectional Context**: BERT captures the contextual information of words by processing the entire sentence bidirectionally, taking both left and right context into account. This allows it to have a deeper understanding of the context and meaning of words.\n",
        "\n",
        "2. **State-of-the-art Performance**: BERT achieved state-of-the-art results on various NLP tasks, including text classification, named entity recognition, question-answering, and more. It demonstrated impressive generalization capabilities across different domains and languages.\n",
        "\n",
        "3. **Pre-training and Fine-tuning**: The two-step process of pre-training on a large corpus and then fine-tuning on specific tasks makes it easy to apply BERT to various downstream tasks without requiring a large amount of task-specific labeled data.\n",
        "\n",
        "4. **Contextual Embeddings**: BERT generates contextual word embeddings, which means the representation of a word can vary based on its context. This is a significant improvement over traditional word embeddings like Word2Vec or GloVe, which generate static embeddings for words.\n",
        "\n",
        "5. **Open-source Implementation**: BERT and its variants are open-source, making it accessible to researchers and developers. It paved the way for the development of many other transformer-based models.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "1. **Computational Complexity**: BERT is a large model, and training it requires significant computational resources and time. In its original form, BERT has 110 million parameters, which can be challenging for small-scale setups.\n",
        "\n",
        "2. **Memory Requirements**: The large model size of BERT makes it memory-intensive. Fine-tuning BERT on certain tasks may require high GPU memory, limiting its deployment on low-resource devices.\n",
        "\n",
        "3. **Lack of Dynamic Context**: Although BERT is capable of capturing contextual information, it still processes the entire sentence at once, which may not be ideal for tasks where dynamic context is crucial (e.g., online chat responses).\n",
        "\n",
        "4. **Cannot Handle Long Texts**: Due to the computational constraints, BERT has a maximum token limit (e.g., 512 tokens). This makes it unsuitable for very long documents or sequences, requiring techniques like truncation or sliding windows.\n",
        "\n",
        "**When to use BERT:**\n",
        "\n",
        "BERT is an excellent choice in various scenarios:\n",
        "\n",
        "1. **Text Classification**: BERT performs well in tasks where the context of the whole sentence is essential, such as sentiment analysis, intent classification, and document categorization.\n",
        "\n",
        "2. **Question-Answering**: BERT is useful for question-answering tasks, where it can encode the question and passage to find the correct answer.\n",
        "\n",
        "3. **Named Entity Recognition (NER)**: BERT's contextual embeddings are effective for NER, where it can recognize entities (e.g., names, dates, locations) in a sentence.\n",
        "\n",
        "4. **Text Similarity and Semantic Similarity**: BERT can be used to measure semantic similarity between sentences, which finds applications in search engines, recommendation systems, and more.\n",
        "\n",
        "5. **Language Generation**: BERT's contextual embeddings can be used as input to language generation models for tasks like text completion, text summarization, and machine translation.\n",
        "\n",
        "In summary, if you have a task involving natural language understanding or generation and have enough computational resources for training and inference, BERT or its variants can be a powerful choice to achieve state-of-the-art results. However, for smaller-scale projects or tasks with strict memory constraints, you might consider using smaller transformer-based models or more efficient architectures."
      ],
      "metadata": {
        "id": "WQb7Dybl-1-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "vAOk6EpJCDg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dJ6wClBaoTSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample data for text classification\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"BERT is great for natural language processing tasks.\",\n",
        "    \"Transformers library makes using BERT easy.\",\n",
        "]\n",
        "labels = [1, 1, 0]  # Example labels (0 or 1 for binary classification)\n",
        "\n",
        "# Tokenize input sentences and convert them into input tensors\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Create a DataLoader to efficiently handle the data during training\n",
        "dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Training loop (for demonstration purposes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, target_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Avg. Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Example inference\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_sentence = \"BERT is a powerful language model.\"\n"
      ],
      "metadata": {
        "id": "WqEg6wcse-J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "42XBQZrsQdOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 1: Import the required libraries:\n",
        "```python\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "```\n",
        "\n",
        "In this step, we import the necessary libraries for working with PyTorch, the Hugging Face `transformers` library, and PyTorch's DataLoader and TensorDataset classes.\n",
        "\n",
        "Step 2: Load pre-trained BERT tokenizer and model:\n",
        "```python\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "```\n",
        "\n",
        "Here, we load the pre-trained BERT tokenizer and model. The 'bert-base-uncased' variant of BERT is used, which is a base version of the model trained on uncased text.\n",
        "\n",
        "Step 3: Prepare sample data for text classification:\n",
        "```python\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"BERT is great for natural language processing tasks.\",\n",
        "    \"Transformers library makes using BERT easy.\",\n",
        "]\n",
        "labels = [1, 1, 0]\n",
        "```\n",
        "\n",
        "In this step, we define some sample input sentences and corresponding labels for text classification. The labels are binary (0 or 1) in this example.\n",
        "\n",
        "Step 4: Tokenize input sentences and convert them into input tensors:\n",
        "```python\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "labels = torch.tensor(labels)\n",
        "```\n",
        "\n",
        "Here, we use the pre-trained tokenizer to tokenize the input sentences. The tokenizer converts the sentences into token IDs and adds special tokens (e.g., [CLS], [SEP]) necessary for BERT. Additionally, the tokenizer pads the sentences to the same length and truncates them if needed. The `inputs` dictionary contains the tokenized and encoded sentences in PyTorch tensors.\n",
        "\n",
        "Step 5: Create a DataLoader for efficient data handling during training:\n",
        "```python\n",
        "dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "```\n",
        "\n",
        "In this step, we create a PyTorch `TensorDataset` using the tokenized input IDs, attention masks, and labels. The `TensorDataset` allows us to work with tensors in PyTorch. We then create a `DataLoader`, which efficiently handles the data during training by batching it into groups of size 2 (`batch_size=2`) and shuffling the data for each epoch (`shuffle=True`).\n",
        "\n",
        "Step 6: Set the model to training mode:\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "\n",
        "This line sets the BERT model to training mode using `model.train()`. This is necessary before starting the training loop.\n",
        "\n",
        "Step 7: Training loop (for demonstration purposes):\n",
        "```python\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, target_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Avg. Loss: {total_loss / len(dataloader)}\")\n",
        "```\n",
        "\n",
        "Here, we define a simple training loop that iterates over the dataset for a fixed number of epochs (3 in this example). For each epoch, we iterate through batches of data from the DataLoader (`dataloader`). For each batch, we perform the following steps:\n",
        "- Reset the gradients of the model's parameters using `optimizer.zero_grad()`.\n",
        "- Pass the input data to the BERT model (`model(input_ids, attention_mask=attention_mask, labels=target_labels)`) to get the model's outputs and compute the loss with respect to the target labels.\n",
        "- Accumulate the total loss for the epoch (`total_loss += loss.item()`).\n",
        "- Perform backpropagation to compute the gradients of the loss with respect to the model's parameters (`loss.backward()`).\n",
        "- Update the model's parameters using the Adam optimizer (`optimizer.step()`).\n",
        "\n",
        "Finally, we print the average loss for each epoch.\n",
        "\n",
        "Step 8: Example inference:\n",
        "```python\n",
        "model.eval()\n",
        "test_sentence = \"BERT is a powerful language model.\"\n",
        "```\n",
        "\n",
        "In this step, we set the BERT model to evaluation mode using `model.eval()`. This is necessary before making predictions or performing inference. We also define an example test sentence `test_sentence` for which we want to perform inference.\n",
        "\n",
        "Note: The code provided here is for demonstration purposes and may require modifications depending on your specific dataset and task."
      ],
      "metadata": {
        "id": "ArIKhmwCVXJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "acDEmUcbYDeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a healthcare setting, BERT (Bidirectional Encoder Representations from Transformers) can be used for various natural language processing (NLP) tasks to extract valuable information from clinical text data. Here's a real-world example of how BERT can be applied in the healthcare domain:\n",
        "\n",
        "**Clinical Text Classification: Identifying Medical Conditions**\n",
        "\n",
        "Problem: Given a large corpus of electronic health records (EHRs) containing patient clinical notes, we want to automatically identify and classify specific medical conditions mentioned in the notes.\n",
        "\n",
        "Solution using BERT:\n",
        "\n",
        "1. Data Preprocessing:\n",
        "   - Gather a dataset of labeled clinical notes, where each note is associated with a specific medical condition label (e.g., diabetes, hypertension, pneumonia).\n",
        "   - Preprocess the clinical notes to remove sensitive information and standardize the text format.\n",
        "\n",
        "2. Fine-tuning BERT:\n",
        "   - Load a pre-trained BERT model (e.g., BERT-base) and its corresponding tokenizer from the Hugging Face `transformers` library.\n",
        "   - Modify the output layer of BERT to match the number of medical condition classes (e.g., using `BertForSequenceClassification`).\n",
        "   - Use the labeled clinical notes and their corresponding medical condition labels to fine-tune the BERT model. During fine-tuning, the BERT model learns to capture the contextual representations of words and phrases specific to the medical condition classification task.\n",
        "\n",
        "3. Model Evaluation:\n",
        "   - Split the labeled dataset into training, validation, and test sets.\n",
        "   - Train the fine-tuned BERT model on the training set and use the validation set to tune hyperparameters and prevent overfitting.\n",
        "   - Evaluate the model's performance on the test set using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "4. Model Deployment:\n",
        "   - Once the fine-tuned BERT model demonstrates satisfactory performance on the test set, deploy it to a production environment.\n",
        "   - In the production environment, the model can be used to automatically analyze new incoming clinical notes, classify them into relevant medical conditions, and assist healthcare professionals in patient care.\n",
        "\n",
        "Example Use Case:\n",
        "- A hospital's EHR system receives a new patient's clinical notes during admission.\n",
        "- The deployed BERT model processes the incoming notes and automatically identifies potential medical conditions mentioned in the text.\n",
        "- The model may identify conditions like \"Type 2 diabetes,\" \"Hypertension,\" or \"Pneumonia\" based on the text patterns learned during fine-tuning.\n",
        "- Healthcare professionals can then use this information to prioritize patient care, optimize treatment plans, and monitor patients more effectively.\n",
        "\n",
        "By leveraging BERT's contextual language representations and transfer learning capabilities, healthcare organizations can efficiently analyze vast amounts of clinical text data to extract relevant medical information, aiding in early disease detection, decision-making, and patient management."
      ],
      "metadata": {
        "id": "ujn3M4YU7UOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "ToYFMNGPZzzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. What is BERT, and how does it work?\n",
        "   BERT is a transformer-based language model developed by Google. It is pre-trained on a large corpus of text using a masked language model (MLM) and next sentence prediction (NSP) objectives. BERT uses a bidirectional approach, allowing it to understand the context of words by considering both the left and right contexts in a sentence.\n",
        "\n",
        "2. How is BERT different from traditional language models?\n",
        "   Traditional language models, like LSTMs and GRUs, process text sequentially from left to right. BERT, on the other hand, processes words in a bidirectional manner, enabling it to better capture the contextual information and relationships between words.\n",
        "\n",
        "3. What is pre-training and fine-tuning in the context of BERT?\n",
        "   Pre-training refers to the initial training of BERT on a large corpus of text to learn general language representations. Fine-tuning, on the other hand, is the process of taking the pre-trained BERT model and adapting it to a specific NLP task by training it on task-specific data with a smaller learning rate.\n",
        "\n",
        "4. What are the benefits of using BERT in NLP tasks?\n",
        "   BERT has shown remarkable performance improvements in various NLP tasks, such as question-answering, sentiment analysis, natural language inference, and named entity recognition. Its contextual understanding allows it to grasp nuances and complex linguistic patterns.\n",
        "\n",
        "5. Can BERT handle multiple languages?\n",
        "   Yes, BERT can be trained and used for multilingual applications. By pre-training on a diverse multilingual corpus, BERT can learn to handle multiple languages effectively.\n",
        "\n",
        "6. How does BERT handle out-of-vocabulary words?\n",
        "   BERT tokenizes input text into subwords or word pieces. For out-of-vocabulary words, it breaks them down into smaller subwords to retain some context and meaning.\n",
        "\n",
        "7. What is the impact of BERT on transfer learning?\n",
        "   BERT's pre-training and fine-tuning approach has revolutionized transfer learning in NLP. Pre-training on a large dataset allows BERT to learn general language representations, which can then be fine-tuned on specific tasks with smaller datasets, leading to improved performance.\n",
        "\n",
        "8. What are some limitations of BERT?\n",
        "   While BERT is powerful, it still has limitations. It requires substantial computing resources due to its large size, and fine-tuning on specific tasks can be time-consuming. Additionally, BERT's bidirectional nature makes it unsuitable for tasks requiring real-time processing or incremental updates.\n",
        "\n",
        "9. What are some popular variations of BERT?\n",
        "   Apart from BERT itself, there are other transformer-based models like GPT-3, RoBERTa, and ALBERT, each with unique architectural improvements and pre-training strategies.\n",
        "\n",
        "10. Is BERT the best model for all NLP tasks?\n",
        "    While BERT has achieved significant success, it may not always be the best model for every NLP task. Depending on the task, data size, and resources, other models or custom architectures might perform better.\n",
        "\n",
        "Remember, BERT has paved the way for numerous advancements in NLP, and ongoing research is continually improving upon its limitations and expanding its applications."
      ],
      "metadata": {
        "id": "LsVM4lyQWP1q"
      }
    }
  ]
}