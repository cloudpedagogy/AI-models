{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQW9zhN7qwfb0ChYwYpX/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/ResNet_(Residual_Network).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet (Residual Network) Model Background"
      ],
      "metadata": {
        "id": "SlraLV59BOwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet, short for Residual Network, is a deep convolutional neural network architecture that was introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in 2015. It was a groundbreaking architecture that significantly improved the performance of deep neural networks, especially when dealing with very deep structures, containing tens or hundreds of layers.\n",
        "\n",
        "**Key characteristics of ResNet**:\n",
        "\n",
        "1. Residual Blocks: The fundamental building block of ResNet is the residual block. It introduces the concept of skip connections or shortcuts that allow information to bypass certain layers. The main idea is to learn the residual between the input and the output of a block, making it easier for the network to optimize and reducing the vanishing/exploding gradient problem.\n",
        "\n",
        "2. Deep Architecture: ResNet can be considerably deeper than traditional neural networks without suffering from degradation issues. Deeper networks are generally expected to perform better since they can learn more complex features, but without the use of residual connections, training very deep networks becomes difficult due to the vanishing gradient problem.\n",
        "\n",
        "**Pros of ResNet**:\n",
        "\n",
        "1. Improved Training: Residual connections enable easier training of very deep neural networks. The network can better optimize the underlying mapping and learn complex patterns, leading to better performance.\n",
        "\n",
        "2. Avoidance of Degradation: Traditional deep networks without residual connections may experience performance degradation as they go deeper, but ResNet effectively addresses this issue.\n",
        "\n",
        "3. State-of-the-art Performance: ResNet and its variations have achieved state-of-the-art results on various computer vision tasks, including image classification, object detection, and segmentation.\n",
        "\n",
        "4. Transfer Learning: Pre-trained ResNet models on large datasets (e.g., ImageNet) can be used as a starting point for transfer learning in various applications, even with limited labeled data.\n",
        "\n",
        "**Cons of ResNet**:\n",
        "\n",
        "1. Computational Complexity: Deeper networks, including ResNet, are more computationally expensive during training and inference due to the increased number of layers.\n",
        "\n",
        "2. Overfitting: When applying ResNet to small datasets, there is a risk of overfitting due to its large capacity. Proper regularization techniques should be employed in such cases.\n",
        "\n",
        "3. Memory Requirements: Training very deep ResNet models can require significant GPU memory, which might be a limitation for users with limited resources.\n",
        "\n",
        "**When to use ResNet**:\n",
        "\n",
        "ResNet is recommended in the following scenarios:\n",
        "\n",
        "1. Large Datasets: When you have access to large datasets, ResNet's ability to learn from vast amounts of data can lead to superior performance.\n",
        "\n",
        "2. Deep Architectures: When you need to use very deep neural networks to solve complex tasks, ResNet is an excellent choice to mitigate the challenges associated with deep architectures.\n",
        "\n",
        "3. Computer Vision Tasks: ResNet has shown exceptional performance in various computer vision tasks, such as image classification, object detection, and semantic segmentation.\n",
        "\n",
        "4. Transfer Learning: If you need a powerful pre-trained model for transfer learning, using a pre-trained ResNet model can be advantageous, especially if your target task is related to computer vision.\n",
        "\n",
        "5. State-of-the-Art Performance: If you aim to achieve state-of-the-art results on benchmark datasets in computer vision, ResNet-based architectures are worth exploring."
      ],
      "metadata": {
        "id": "rJ9MdsfbAYG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "WWdDlgODbMkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create a ResNet instance with 4 layers (BasicBlock) and 10 classes (assuming CIFAR-10)\n",
        "resnet = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "# Random input data for testing\n",
        "input_data = torch.randn(1, 3, 32, 32)  # Assuming CIFAR-10 image size (32x32)\n",
        "\n",
        "# Get the model's output\n",
        "output = resnet(input_data)\n",
        "print(output.shape)  # Output shape: (1, 10) for CIFAR-10 classification\n"
      ],
      "metadata": {
        "id": "f9im_dwHx4t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "F9qaRGlg055e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Importing Libraries:** The code starts by importing the required libraries, which are `torch` (PyTorch), `torch.nn` (the module that provides various neural network layers), and `torch.optim` (for optimization algorithms).\n",
        "\n",
        "2. **Defining BasicBlock Class:** The code defines a class called `BasicBlock`, which is a fundamental building block of the ResNet architecture. A `BasicBlock` contains two convolutional layers with batch normalization and a skip connection (shortcut connection) to preserve gradients during training.\n",
        "\n",
        "3. **Defining ResNet Class:** Next, the code defines the main `ResNet` class, which consists of several layers of `BasicBlock`. It also contains the necessary layers for preprocessing and the final fully connected layer for classification.\n",
        "\n",
        "4. **Initializing ResNet:** An instance of the `ResNet` class is created with the following parameters:\n",
        "   - `BasicBlock`: The building block used for constructing the ResNet.\n",
        "   - `[2, 2, 2, 2]`: A list specifying the number of `BasicBlock` layers in each stage. For example, there are two `BasicBlock` layers in `layer1`, two in `layer2`, and so on.\n",
        "   - `num_classes=10`: The number of classes for the classification task. In this case, it's assumed to be 10 for the CIFAR-10 dataset.\n",
        "\n",
        "5. **Defining Forward Pass:** The `forward` method is defined for the `ResNet` class, which outlines how the input data flows through the layers of the network during the forward pass.\n",
        "\n",
        "6. **Creating a ResNet Instance:** An instance of the ResNet model is created with 4 layers of `BasicBlock` and 10 classes (assuming CIFAR-10).\n",
        "\n",
        "7. **Input Data:** A random input tensor (`input_data`) is generated for testing purposes. It represents a single image with 3 channels (RGB) and a size of 32x32 pixels, assuming it's from the CIFAR-10 dataset.\n",
        "\n",
        "8. **Forward Pass:** The `input_data` is passed through the `resnet` model using `output = resnet(input_data)`. This executes the forward pass, and the model processes the input data through all the layers to produce the output logits.\n",
        "\n",
        "9. **Output Shape:** Finally, the shape of the output tensor (`output`) is printed, which should be `(1, 10)` in this case, indicating one sample (image) classified into 10 classes (assuming CIFAR-10).\n",
        "\n",
        "In summary, this code defines a ResNet architecture using PyTorch, specifically designed for image classification tasks. It demonstrates how to create a custom ResNet model using `BasicBlock` and implement its forward pass for inference on a sample input image. The output logits can be further processed using a softmax function to obtain class probabilities for classification."
      ],
      "metadata": {
        "id": "lHunk9Aa1GpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "dvd_3m2o2NAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet (Residual Network) is a deep learning architecture that has been widely used in various domains, including healthcare. One real-world example of ResNet in a healthcare setting is its application in medical image analysis, particularly in the diagnosis of diseases like cancer using histopathology images.\n",
        "\n",
        "In histopathology, pathologists examine tissue samples under a microscope to identify abnormalities and diagnose diseases. Digital pathology and deep learning have revolutionized this field by allowing automated analysis of histopathology images to assist pathologists in making more accurate and efficient diagnoses.\n",
        "\n",
        "ResNet's architecture, with its residual connections, helps address the problem of vanishing gradients and enables the training of very deep neural networks. The ability to create deeper networks has been beneficial in handling complex medical images with many features and details.\n",
        "\n",
        "Here's how ResNet can be used in a healthcare setting:\n",
        "\n",
        "1. Cancer Diagnosis: In cancer diagnosis, ResNet can be used to classify histopathology images into different categories like benign, malignant, or normal tissue. The model can be trained on a large dataset of annotated histopathology images, and its ability to learn intricate patterns in the data can help in accurate cancer detection.\n",
        "\n",
        "2. Tumor Segmentation: ResNet can also be used for semantic segmentation, where it can identify and segment tumor regions in medical images. This segmentation can help in measuring the tumor size, assessing its growth, and planning treatment options.\n",
        "\n",
        "3. Disease Progression Monitoring: By feeding a series of medical images over time, ResNet can be used to track disease progression and predict future outcomes. For instance, in the case of neurodegenerative diseases like Alzheimer's, the model can analyze brain scans to monitor changes in brain structures.\n",
        "\n",
        "4. Radiology Image Analysis: In radiology, ResNet can be applied to various imaging modalities, such as X-rays, CT scans, and MRI images. It can aid in detecting abnormalities like fractures, tumors, or other anomalies in these images.\n",
        "\n",
        "Using ResNet in these healthcare applications can significantly improve diagnostic accuracy, reduce human error, and speed up the analysis process. However, it is crucial to ensure that the models are carefully validated and integrated into the clinical workflow to ensure their safety and effectiveness in real-world medical scenarios."
      ],
      "metadata": {
        "id": "o1cmy8J5G5_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "pGhwE2IT30tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is ResNet, and why is it significant in deep learning?\n",
        "ResNet, short for Residual Network, is a deep learning architecture introduced by Kaiming He et al. in 2015. It was developed to address the vanishing gradient problem in very deep neural networks. ResNet introduced the concept of residual blocks, which allow gradients to flow directly through the network, enabling the training of very deep networks (e.g., 50, 100, or even 1000 layers) without degradation in performance.\n",
        "\n",
        "2. How does ResNet achieve its remarkable depth?\n",
        "ResNet's key innovation is the use of residual blocks. Instead of directly learning the mapping from one layer to another, ResNet learns the residual mapping. This is done by adding the input of a layer to its output, allowing the network to learn the residual (difference) between the input and output. This shortcut connection is known as a skip connection, and it helps the gradients propagate more effectively, allowing for very deep architectures.\n",
        "\n",
        "3. What are skip connections in ResNet, and why are they essential?\n",
        "Skip connections, also called identity shortcuts, are the additional connections that skip one or more layers in the neural network. In ResNet, these skip connections allow the gradient to bypass the layers within the residual blocks, ensuring that the gradients do not vanish as the network gets deeper. This enables the training of much deeper networks, leading to improved accuracy.\n",
        "\n",
        "4. Are there different versions of ResNet, and if so, what are their differences?\n",
        "Yes, there are several versions of ResNet, typically denoted by the number of layers. Some common variants include ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. The numbers in their names represent the total number of layers, including both convolutional and fully connected layers. The deeper versions generally tend to perform better but also require more computational resources.\n",
        "\n",
        "5. What applications benefit from using ResNet?\n",
        "ResNet has found applications in various computer vision tasks, including image classification, object detection, and segmentation. Due to its ability to handle very deep networks effectively, it has become a popular choice in many other domains, including natural language processing and speech recognition.\n",
        "\n",
        "6. How does ResNet compare to other architectures like VGG and Inception?\n",
        "ResNet has outperformed many earlier architectures, such as VGG and Inception, in various benchmark tasks. Its skip connections enable training of much deeper networks, which has led to better accuracy with fewer parameters. While VGG and Inception were also significant milestones, ResNet's impact on deep learning research and applications has been especially profound.\n",
        "\n",
        "7. Does using ResNet make training faster or slower?\n",
        "In general, training ResNet may take longer compared to shallower networks due to the increased depth and complexity. However, the use of skip connections helps accelerate the training process by avoiding the vanishing gradient problem. Additionally, techniques like batch normalization and improved optimization algorithms have contributed to faster convergence.\n",
        "\n",
        "8. Can ResNet be used for transfer learning?\n",
        "Yes, ResNet is often used for transfer learning. Pre-trained versions of ResNet on large image datasets like ImageNet are readily available. These pre-trained models can be fine-tuned or used as feature extractors for various downstream tasks, saving time and computational resources in training new models from scratch.\n",
        "\n",
        "9. Are there any variations of ResNet for other types of data, such as audio or text?\n",
        "While ResNet was initially developed for computer vision tasks, its underlying principles of using residual blocks and skip connections have inspired similar architectures for other types of data. For example, ResNet-like models have been adapted for audio processing tasks and natural language processing tasks, such as speech recognition and machine translation.\n",
        "\n",
        "10. Are there any limitations or drawbacks of using ResNet?\n",
        "Although ResNet is a powerful and widely used architecture, it may suffer from overfitting when applied to small datasets or insufficiently regularized. The deeper versions of ResNet may also require significant computational resources, which can limit their practicality on certain hardware or devices with limited capabilities. Proper hyperparameter tuning and regularization techniques are essential to address these challenges."
      ],
      "metadata": {
        "id": "POCRiv0D7orx"
      }
    }
  ]
}