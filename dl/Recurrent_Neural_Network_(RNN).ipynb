{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMN6RrEjXt1clNTjWk2uJer",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/dl/Recurrent_Neural_Network_(RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network (RNN) Model Background"
      ],
      "metadata": {
        "id": "KCPAdT8zBSKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recurrent Neural Network (RNN) is a type of artificial neural network designed to process sequential data, such as time series, text, speech, and more. Unlike traditional feedforward neural networks, RNNs have connections that allow information to flow in cycles, enabling them to maintain an internal state, or memory, which makes them well-suited for handling sequential data.\n",
        "\n",
        "The basic idea behind an RNN is to use the information from previous time steps to influence the current prediction, effectively capturing dependencies and patterns within sequential data.\n",
        "\n",
        "**Pros of Recurrent Neural Networks**:\n",
        "\n",
        "1. Sequential data processing: RNNs are specifically designed for sequential data, making them effective for tasks like time series prediction, natural language processing (NLP), speech recognition, and handwriting recognition.\n",
        "\n",
        "2. Variable-length inputs: RNNs can handle input sequences of variable lengths, which is useful when dealing with data that doesn't have a fixed length.\n",
        "\n",
        "3. Memory: RNNs can remember information from previous time steps due to their cyclic connections, making them capable of handling long-term dependencies in data.\n",
        "\n",
        "4. Parameter sharing: RNNs use the same set of weights across different time steps, which allows the network to be more compact and efficient in terms of parameters.\n",
        "\n",
        "**Cons of Recurrent Neural Networks**:\n",
        "\n",
        "1. Vanishing and exploding gradients: RNNs can suffer from the vanishing and exploding gradient problems, which occur when gradients become too small or too large during training, leading to difficulties in learning long-range dependencies.\n",
        "\n",
        "2. Computational complexity: Training RNNs can be computationally expensive, especially for long sequences and deep architectures.\n",
        "\n",
        "3. Lack of parallelism: Due to their sequential nature, RNNs are inherently difficult to parallelize, limiting their efficiency on certain hardware architectures.\n",
        "\n",
        "4. Short-term memory: Standard RNN architectures may have difficulty retaining information for very long periods, leading to the \"short-term memory\" problem.\n",
        "\n",
        "**When to use Recurrent Neural Networks**:\n",
        "\n",
        "You should consider using RNNs when dealing with sequential data and tasks that involve temporal dependencies or patterns. Some common use cases include:\n",
        "\n",
        "1. Natural Language Processing (NLP): RNNs are widely used for tasks like machine translation, sentiment analysis, text generation, and speech recognition.\n",
        "\n",
        "2. Time Series Prediction: RNNs are effective in predicting future values in time series data, such as stock prices, weather forecasts, or industrial sensor readings.\n",
        "\n",
        "3. Handwriting Recognition: RNNs have been used successfully in recognizing and generating handwritten text.\n",
        "\n",
        "4. Music Generation: RNNs can be used to create music by learning patterns in existing compositions.\n",
        "\n",
        "5. Video Analysis: RNNs can process video data by treating consecutive frames as sequential inputs, useful in tasks like action recognition or video captioning.\n",
        "\n",
        "However, it's important to note that while RNNs have been foundational for sequential data processing, newer models like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been introduced to address some of the issues related to vanishing gradients and short-term memory, and they often outperform traditional RNNs in practice. Additionally, for certain tasks, other models like Transformer-based architectures (e.g., BERT, GPT) have shown superior performance, especially in NLP-related tasks."
      ],
      "metadata": {
        "id": "2LMqMm9I71XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "8Z_SV9u1bOCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Generate synthetic time series data\n",
        "def generate_time_series(n_samples=100, time_steps=10):\n",
        "    X = np.random.rand(n_samples, time_steps)\n",
        "    y = X[:, 0] + np.sin(X[:, 1]) + 0.2 * np.random.rand(n_samples)\n",
        "    return X, y\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate the dataset\n",
        "X_train, y_train = generate_time_series(n_samples=1000)\n",
        "X_test, y_test = generate_time_series(n_samples=200)\n",
        "\n",
        "# Reshape the data for RNN input (batch_size, time_steps, input_features)\n",
        "X_train = X_train.reshape(-1, 10, 1)\n",
        "X_test = X_test.reshape(-1, 10, 1)\n",
        "\n",
        "# Create the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(20, input_shape=(None, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the RNN model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "\n",
        "# Make predictions with the trained model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Plot the first 20 predictions and ground truth values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test[:20], label='Ground Truth', marker='o')\n",
        "plt.plot(predictions[:20], label='Predictions', marker='x')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('RNN Time Series Prediction')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sG-FXA38Lj_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "6QU_kP_w0_qZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import Libraries:** The code starts by importing necessary libraries: `numpy` for numerical computations, `keras` for building the neural network model, and specific modules from `keras.models` and `keras.layers` required for constructing the RNN model.\n",
        "\n",
        "2. **Generate Synthetic Time Series Data:** The function `generate_time_series` is defined to create synthetic time series data. It generates `n_samples` time series sequences, each with `time_steps` data points. The input data `X` is a random array, and the target `y` is created based on a combination of the first element of `X`, the sine function of the second element of `X`, and some random noise.\n",
        "\n",
        "3. **Setting Random Seed:** The random seed is set to 42 using `np.random.seed(42)` to ensure reproducibility of the results.\n",
        "\n",
        "4. **Generate Dataset:** Using the `generate_time_series` function, the training and test datasets `X_train`, `y_train`, `X_test`, and `y_test` are generated.\n",
        "\n",
        "5. **Reshape Data for RNN Input:** The data is reshaped to have the shape `(batch_size, time_steps, input_features)`, which is required for RNN input. Here, the batch size is inferred automatically, and `time_steps` is set to 10 (as specified in the `generate_time_series` function), and `input_features` is set to 1 since there is only one feature in the input data.\n",
        "\n",
        "6. **Create the RNN Model:** A simple RNN model is created using the `Sequential` API from Keras. The RNN layer consists of 20 units (neurons), and the input shape is specified as `(None, 1)`, where `None` indicates variable-length input sequences, and `1` denotes the number of features at each time step. A `Dense` layer with one neuron is added as the output layer.\n",
        "\n",
        "7. **Compile the Model:** The model is compiled using the mean squared error (`mse`) loss function and the Adam optimizer.\n",
        "\n",
        "8. **Train the RNN Model:** The model is trained using the `fit` method on the training data `X_train` and `y_train`. It is trained for 50 epochs with a batch size of 32.\n",
        "\n",
        "9. **Evaluate the Model:** The model's performance is evaluated on the test data using the `evaluate` method, and the mean squared error loss on the test set is printed.\n",
        "\n",
        "10. **Make Predictions:** The trained model is used to make predictions on the test data (`X_test`), and the predictions are stored in the variable `predictions`.\n",
        "\n",
        "11. **Plot the Predictions and Ground Truth:** The first 20 predictions and ground truth values are plotted using `matplotlib.pyplot`. The ground truth values are represented with circular markers (`o`), and the predictions are represented with cross markers (`x`). The plot shows how well the RNN model predicts the time series data.\n",
        "\n",
        "Overall, this code demonstrates how to create a simple RNN model using Keras, train it on synthetic time series data, evaluate its performance, and make predictions on new data."
      ],
      "metadata": {
        "id": "w9-Fp0ETLq10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "JNObjIN72Rcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of a Recurrent Neural Network (RNN) being used in a healthcare setting is for predicting patient outcomes based on time series data.\n",
        "\n",
        "Let's consider a scenario where a hospital or healthcare provider wants to predict the likelihood of a patient developing a certain medical condition, such as sepsis, based on their vital signs and other health-related measurements over time.\n",
        "\n",
        "The data collected for each patient would typically include a sequence of vital signs and lab results, taken at regular intervals (e.g., every hour or every few hours). This data can be considered as a time series, where each time step corresponds to a different point in time, and the RNN can be employed to analyze this sequential data.\n",
        "\n",
        "The RNN would take in the patient's historical data (time series) as input and learn from the patterns and trends in the data to make predictions about their future health status. By leveraging the temporal relationships between data points, RNNs can capture long-term dependencies and complex patterns in the patient's vital signs, which might not be apparent from a single snapshot of data.\n",
        "\n",
        "The trained RNN model could then be used to monitor patients in real-time, continuously analyzing their incoming data and alerting medical staff if the risk of sepsis (or any other target condition) is detected to be increasing, enabling early intervention and potentially improving patient outcomes.\n",
        "\n",
        "Overall, RNNs in this healthcare context offer the advantage of considering both the temporal aspect of the data and the interdependencies between sequential measurements, making them valuable tools for predictive analytics and decision support in patient care."
      ],
      "metadata": {
        "id": "l5mZAXDiGrp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "__PLonA933hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Recurrent Neural Network (RNN)?\n",
        "   - A Recurrent Neural Network (RNN) is a type of artificial neural network designed to process sequential data by introducing loops within the network architecture. These loops allow information to persist and be passed from one time step to another, making RNNs well-suited for tasks involving sequential or time-series data.\n",
        "\n",
        "2. How does an RNN handle sequential data?\n",
        "   - RNNs use the same set of weights for each time step, allowing them to process sequential data of varying lengths. The output of a time step is fed back into the network as input for the next time step, creating a feedback loop that enables information retention.\n",
        "\n",
        "3. What are some applications of RNNs?\n",
        "   - RNNs find applications in various fields, including natural language processing (NLP) tasks such as language modeling, machine translation, speech recognition, sentiment analysis, and text generation. They are also used in time-series forecasting, music generation, video analysis, and more.\n",
        "\n",
        "4. What are the challenges with traditional RNNs?\n",
        "   - Traditional RNNs suffer from the vanishing gradient problem, which hinders their ability to capture long-term dependencies in sequences. When gradients become very small during training, it can be challenging for the model to learn from distant time steps in the sequence.\n",
        "\n",
        "5. How do Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) address RNN limitations?\n",
        "   - LSTM and GRU are advanced variants of RNNs that incorporate gating mechanisms. These gates allow the model to control the flow of information, mitigating the vanishing gradient problem. LSTM and GRU architectures have shown improved performance in capturing long-range dependencies in sequential data.\n",
        "\n",
        "6. Can RNNs handle variable-length sequences?\n",
        "   - Yes, RNNs are capable of handling variable-length sequences. Since they share weights across time steps, they can process sequences of different lengths during both training and inference.\n",
        "\n",
        "7. What is the role of time step truncation in training RNNs?\n",
        "   - Due to the vanishing gradient problem, training RNNs on very long sequences can be computationally expensive and time-consuming. Time step truncation involves breaking long sequences into smaller segments, allowing for more efficient training and better handling of long-range dependencies.\n",
        "\n",
        "8. Are there any alternatives to RNNs for sequential data?\n",
        "   - Yes, apart from RNNs, transformers have gained popularity for sequential tasks. Transformers utilize self-attention mechanisms to process sequences in parallel, making them more efficient for long-range dependencies. However, they may require more data and computational resources compared to RNNs.\n",
        "\n",
        "9. How can RNNs be used in generative tasks?\n",
        "   - RNNs can be used in generative tasks like text generation, music composition, and image captioning. By training the network to predict the next element in a sequence based on the previous ones, the model can generate new data points.\n",
        "\n",
        "10. What are some famous architectures that use RNNs?\n",
        "    - Some well-known architectures include Seq2Seq (Sequence-to-Sequence) models for machine translation, attention-based models, and bi-directional RNNs that process sequences in both forward and backward directions. Additionally, encoder-decoder architectures are often used for various NLP tasks."
      ],
      "metadata": {
        "id": "aYHDTUsW7cOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "4obq812tX9d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is the primary advantage of Recurrent Neural Networks (RNNs) over traditional feedforward neural networks?\n",
        "\n",
        "a) RNNs can handle variable-length sequences\n",
        "b) RNNs have more hidden layers\n",
        "c) RNNs have faster training times\n",
        "d) RNNs require fewer computational resources\n",
        "\n",
        "**Question 2:** In the context of RNNs, what is \"sequence padding\"?\n",
        "\n",
        "a) Adding extra layers to the RNN\n",
        "b) Truncating sequences to a fixed length\n",
        "c) Removing recurrent connections from the network\n",
        "d) Adding a constant value to all sequence elements\n",
        "\n",
        "**Question 3:** What is the vanishing gradient problem in RNNs?\n",
        "\n",
        "a) The RNN has too many layers, causing slow training\n",
        "b) The RNN has too few layers, leading to poor performance\n",
        "c) Gradients become extremely small during backpropagation, leading to slow or halted learning\n",
        "d) Gradients become too large, causing the model to diverge during training\n",
        "\n",
        "**Question 4:** Which type of RNN unit helps address the vanishing gradient problem by allowing gradients to flow more easily?\n",
        "\n",
        "a) Long Short-Term Memory (LSTM) units\n",
        "b) Gated Recurrent Units (GRUs)\n",
        "c) Simple RNN units\n",
        "d) Complex Recurrent Units (CRUs)\n",
        "\n",
        "**Question 5:** In an RNN architecture, how does information from previous time steps influence the current prediction?\n",
        "\n",
        "a) Only the immediately preceding time step's information is considered\n",
        "b) Information from all previous time steps is equally combined\n",
        "c) Information from earlier time steps has no influence on the current prediction\n",
        "d) Information from previous time steps decays exponentially before reaching the current prediction\n",
        "\n",
        "**Question 6:** What is the key idea behind \"teacher forcing\" in training RNNs?\n",
        "\n",
        "a) Using a teacher to guide the learning process\n",
        "b) Forcing the model to memorize training data\n",
        "c) Randomly shuffling training examples\n",
        "d) Feeding the true output from the previous time step as input to the current time step during training\n",
        "\n",
        "**Question 7:** In machine translation using RNNs, what is the purpose of the \"encoder-decoder\" architecture?\n",
        "\n",
        "a) To use separate RNNs for encoding and decoding information\n",
        "b) To create a more complex model architecture\n",
        "c) To handle multiple languages simultaneously\n",
        "d) To translate code into natural language\n",
        "\n",
        "**Question 8:** Which RNN architecture can be thought of as a simplified version of LSTM and is computationally more efficient?\n",
        "\n",
        "a) Bidirectional RNN\n",
        "b) Gated Recurrent Unit (GRU)\n",
        "c) Deep RNN\n",
        "d) Stacked RNN\n",
        "\n",
        "**Question 9:** What is the main limitation of traditional RNNs when dealing with very long sequences?\n",
        "\n",
        "a) They have trouble capturing short-term dependencies\n",
        "b) They tend to overfit the data\n",
        "c) They struggle with vanishing gradients\n",
        "d) They suffer from memory and computation issues due to vanishing gradients\n",
        "\n",
        "**Question 10:** Which of the following is NOT a common application of Recurrent Neural Networks?\n",
        "\n",
        "a) Text generation\n",
        "b) Image classification\n",
        "c) Speech recognition\n",
        "d) Time series prediction\n",
        "\n",
        "**Answers:**\n",
        "1. a) RNNs can handle variable-length sequences\n",
        "2. b) Truncating sequences to a fixed length\n",
        "3. c) Gradients become extremely small during backpropagation, leading to slow or halted learning\n",
        "4. a) Long Short-Term Memory (LSTM) units\n",
        "5. b) Information from all previous time steps is equally combined\n",
        "6. d) Feeding the true output from the previous time step as input to the current time step during training\n",
        "7. a) To use separate RNNs for encoding and decoding information\n",
        "8. b) Gated Recurrent Unit (GRU)\n",
        "9. d) They suffer from memory and computation issues due to vanishing gradients\n",
        "10. b) Image classification"
      ],
      "metadata": {
        "id": "Bvt5oGHiX_Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "dEEo0r3-3TWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Patient Readmission Prediction**\n",
        "   - Objective: Predict the likelihood of a patient being readmitted to the hospital within a certain time frame (e.g., 30 days) after being discharged.\n",
        "   - Data: Hospital discharge summaries, diagnosis, patient demographics, etc.\n",
        "  \n",
        "2. **Drug Prescription Analysis**\n",
        "   - Objective: Analyze doctor's prescription notes to predict the next drug that might be prescribed for a specific ailment.\n",
        "   - Data: Prescription notes, patient health records.\n",
        "\n",
        "3. **Disease Progression Modeling**\n",
        "   - Objective: Track the progression of a disease over time in a patient, predicting future states or complications.\n",
        "   - Data: Time-series data of disease markers or patient metrics.\n",
        "\n",
        "4. **Health Monitoring from Wearables**\n",
        "   - Objective: Analyze time-series data from wearable health devices to predict health anomalies or events like heart attacks.\n",
        "   - Data: Heart rate, oxygen levels, steps counted, sleep patterns, etc.\n",
        "\n",
        "5. **EHR Sequence Modeling**\n",
        "   - Objective: Predict future medical events (like visits, tests, or procedures) for patients based on their Electronic Health Record (EHR) sequence.\n",
        "   - Data: Electronic Health Records.\n",
        "\n",
        "6. **Medical Imaging Sequence Analysis**\n",
        "   - Objective: Analyze sequences of medical images (like MRI scans taken at different times) to monitor the progression or regression of tumors.\n",
        "   - Data: Sequenced medical images.\n",
        "\n",
        "7. **Clinical Note Generation**\n",
        "   - Objective: Based on the previous clinical notes and patient data, generate a possible next clinical note or fill in parts of a clinical note.\n",
        "   - Data: Past clinical notes, lab results, diagnosis.\n",
        "\n",
        "8. **Epidemic Outbreak Prediction**\n",
        "   - Objective: Use data from previous outbreaks or epidemic patterns to predict the next possible outbreak in a region.\n",
        "   - Data: Time-series data of reported cases, hospital admissions, etc.\n",
        "\n",
        "9. **Patient Sentiment Analysis over Time**\n",
        "   - Objective: Analyze the sentiments of patients over time based on their writings or feedback, which can be used for mental health monitoring.\n",
        "   - Data: Patient diaries, feedback forms, online writings.\n",
        "\n",
        "10. **Treatment Effectiveness Analysis**\n",
        "   - Objective: Based on a sequence of patient health metrics, predict how effective a treatment or medication is over time.\n",
        "   - Data: Time-series data on patient vitals, lab results, medication doses.\n",
        "\n",
        "11. **Genomic Sequence Analysis**\n",
        "   - Objective: Analyze sequences of genomic data to predict susceptibility to certain diseases.\n",
        "   - Data: Genomic sequences.\n",
        "\n",
        "12. **Speech Analysis for Neurological Diseases**\n",
        "   - Objective: Use RNNs to analyze speech patterns over time to diagnose or monitor the progression of neurological diseases like Parkinson’s or Alzheimer’s.\n",
        "   - Data: Audio recordings of patients speaking.\n",
        "\n",
        "13. **Activity Recognition for Elderly Care**\n",
        "   - Objective: Analyze time-series data from sensors placed in an elderly person's home to recognize their activities and predict anomalies like falls.\n",
        "   - Data: Sensor data from a smart home setting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "35x0_pwn3VVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "m4Tfh7qqI2dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simple working example of a Recurrent Neural Network (RNN) model using a real-world healthcare dataset. In this example, we'll use the \"Diabetes 130-US hospitals for years 1999-2008 Data Set\" from the UCI Machine Learning Repository, which contains information about diabetes patients' admissions to hospitals.\n",
        "\n",
        "We'll build an RNN model to predict whether a patient will be readmitted to the hospital within 30 days based on their medical history.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the diabetes dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip\"\n",
        "df = pd.read_csv(url, compression='zip', na_values='?', low_memory=False)\n",
        "\n",
        "# Preprocessing\n",
        "df = df[['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient',\n",
        "         'number_emergency', 'number_inpatient', 'number_diagnoses', 'diabetesMed', 'readmitted']]\n",
        "\n",
        "# Convert categorical variables to numerical\n",
        "df['diabetesMed'] = df['diabetesMed'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Splitting dataset\n",
        "X = df.drop('readmitted', axis=1)\n",
        "y = df['readmitted']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for RNN\n",
        "X_train_rnn = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_rnn = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Build RNN model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_rnn, y_test)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "Remember that this is a basic example, and there are many ways to improve the model's performance by tuning hyperparameters, adding more complex layers, or using more advanced techniques like attention mechanisms. Additionally, ensure that you understand the dataset and task thoroughly before making any decisions about model architecture and preprocessing."
      ],
      "metadata": {
        "id": "no6fYCVoI5Rk"
      }
    }
  ]
}