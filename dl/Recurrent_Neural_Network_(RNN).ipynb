{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa8814RvGSy93j9i7Rbeoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/Recurrent_Neural_Network_(RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "KCPAdT8zBSKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recurrent Neural Network (RNN) is a type of artificial neural network designed to process sequential data, such as time series, text, speech, and more. Unlike traditional feedforward neural networks, RNNs have connections that allow information to flow in cycles, enabling them to maintain an internal state, or memory, which makes them well-suited for handling sequential data.\n",
        "\n",
        "The basic idea behind an RNN is to use the information from previous time steps to influence the current prediction, effectively capturing dependencies and patterns within sequential data.\n",
        "\n",
        "**Pros of Recurrent Neural Networks**:\n",
        "\n",
        "1. Sequential data processing: RNNs are specifically designed for sequential data, making them effective for tasks like time series prediction, natural language processing (NLP), speech recognition, and handwriting recognition.\n",
        "\n",
        "2. Variable-length inputs: RNNs can handle input sequences of variable lengths, which is useful when dealing with data that doesn't have a fixed length.\n",
        "\n",
        "3. Memory: RNNs can remember information from previous time steps due to their cyclic connections, making them capable of handling long-term dependencies in data.\n",
        "\n",
        "4. Parameter sharing: RNNs use the same set of weights across different time steps, which allows the network to be more compact and efficient in terms of parameters.\n",
        "\n",
        "**Cons of Recurrent Neural Networks**:\n",
        "\n",
        "1. Vanishing and exploding gradients: RNNs can suffer from the vanishing and exploding gradient problems, which occur when gradients become too small or too large during training, leading to difficulties in learning long-range dependencies.\n",
        "\n",
        "2. Computational complexity: Training RNNs can be computationally expensive, especially for long sequences and deep architectures.\n",
        "\n",
        "3. Lack of parallelism: Due to their sequential nature, RNNs are inherently difficult to parallelize, limiting their efficiency on certain hardware architectures.\n",
        "\n",
        "4. Short-term memory: Standard RNN architectures may have difficulty retaining information for very long periods, leading to the \"short-term memory\" problem.\n",
        "\n",
        "**When to use Recurrent Neural Networks**:\n",
        "\n",
        "You should consider using RNNs when dealing with sequential data and tasks that involve temporal dependencies or patterns. Some common use cases include:\n",
        "\n",
        "1. Natural Language Processing (NLP): RNNs are widely used for tasks like machine translation, sentiment analysis, text generation, and speech recognition.\n",
        "\n",
        "2. Time Series Prediction: RNNs are effective in predicting future values in time series data, such as stock prices, weather forecasts, or industrial sensor readings.\n",
        "\n",
        "3. Handwriting Recognition: RNNs have been used successfully in recognizing and generating handwritten text.\n",
        "\n",
        "4. Music Generation: RNNs can be used to create music by learning patterns in existing compositions.\n",
        "\n",
        "5. Video Analysis: RNNs can process video data by treating consecutive frames as sequential inputs, useful in tasks like action recognition or video captioning.\n",
        "\n",
        "However, it's important to note that while RNNs have been foundational for sequential data processing, newer models like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been introduced to address some of the issues related to vanishing gradients and short-term memory, and they often outperform traditional RNNs in practice. Additionally, for certain tasks, other models like Transformer-based architectures (e.g., BERT, GPT) have shown superior performance, especially in NLP-related tasks."
      ],
      "metadata": {
        "id": "2LMqMm9I71XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "8Z_SV9u1bOCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Generate synthetic time series data\n",
        "def generate_time_series(n_samples=100, time_steps=10):\n",
        "    X = np.random.rand(n_samples, time_steps)\n",
        "    y = X[:, 0] + np.sin(X[:, 1]) + 0.2 * np.random.rand(n_samples)\n",
        "    return X, y\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate the dataset\n",
        "X_train, y_train = generate_time_series(n_samples=1000)\n",
        "X_test, y_test = generate_time_series(n_samples=200)\n",
        "\n",
        "# Reshape the data for RNN input (batch_size, time_steps, input_features)\n",
        "X_train = X_train.reshape(-1, 10, 1)\n",
        "X_test = X_test.reshape(-1, 10, 1)\n",
        "\n",
        "# Create the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(20, input_shape=(None, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the RNN model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "\n",
        "# Make predictions with the trained model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Plot the first 20 predictions and ground truth values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test[:20], label='Ground Truth', marker='o')\n",
        "plt.plot(predictions[:20], label='Predictions', marker='x')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('RNN Time Series Prediction')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sG-FXA38Lj_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "6QU_kP_w0_qZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import Libraries:** The code starts by importing necessary libraries: `numpy` for numerical computations, `keras` for building the neural network model, and specific modules from `keras.models` and `keras.layers` required for constructing the RNN model.\n",
        "\n",
        "2. **Generate Synthetic Time Series Data:** The function `generate_time_series` is defined to create synthetic time series data. It generates `n_samples` time series sequences, each with `time_steps` data points. The input data `X` is a random array, and the target `y` is created based on a combination of the first element of `X`, the sine function of the second element of `X`, and some random noise.\n",
        "\n",
        "3. **Setting Random Seed:** The random seed is set to 42 using `np.random.seed(42)` to ensure reproducibility of the results.\n",
        "\n",
        "4. **Generate Dataset:** Using the `generate_time_series` function, the training and test datasets `X_train`, `y_train`, `X_test`, and `y_test` are generated.\n",
        "\n",
        "5. **Reshape Data for RNN Input:** The data is reshaped to have the shape `(batch_size, time_steps, input_features)`, which is required for RNN input. Here, the batch size is inferred automatically, and `time_steps` is set to 10 (as specified in the `generate_time_series` function), and `input_features` is set to 1 since there is only one feature in the input data.\n",
        "\n",
        "6. **Create the RNN Model:** A simple RNN model is created using the `Sequential` API from Keras. The RNN layer consists of 20 units (neurons), and the input shape is specified as `(None, 1)`, where `None` indicates variable-length input sequences, and `1` denotes the number of features at each time step. A `Dense` layer with one neuron is added as the output layer.\n",
        "\n",
        "7. **Compile the Model:** The model is compiled using the mean squared error (`mse`) loss function and the Adam optimizer.\n",
        "\n",
        "8. **Train the RNN Model:** The model is trained using the `fit` method on the training data `X_train` and `y_train`. It is trained for 50 epochs with a batch size of 32.\n",
        "\n",
        "9. **Evaluate the Model:** The model's performance is evaluated on the test data using the `evaluate` method, and the mean squared error loss on the test set is printed.\n",
        "\n",
        "10. **Make Predictions:** The trained model is used to make predictions on the test data (`X_test`), and the predictions are stored in the variable `predictions`.\n",
        "\n",
        "11. **Plot the Predictions and Ground Truth:** The first 20 predictions and ground truth values are plotted using `matplotlib.pyplot`. The ground truth values are represented with circular markers (`o`), and the predictions are represented with cross markers (`x`). The plot shows how well the RNN model predicts the time series data.\n",
        "\n",
        "Overall, this code demonstrates how to create a simple RNN model using Keras, train it on synthetic time series data, evaluate its performance, and make predictions on new data."
      ],
      "metadata": {
        "id": "w9-Fp0ETLq10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "JNObjIN72Rcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of a Recurrent Neural Network (RNN) being used in a healthcare setting is for predicting patient outcomes based on time series data.\n",
        "\n",
        "Let's consider a scenario where a hospital or healthcare provider wants to predict the likelihood of a patient developing a certain medical condition, such as sepsis, based on their vital signs and other health-related measurements over time.\n",
        "\n",
        "The data collected for each patient would typically include a sequence of vital signs and lab results, taken at regular intervals (e.g., every hour or every few hours). This data can be considered as a time series, where each time step corresponds to a different point in time, and the RNN can be employed to analyze this sequential data.\n",
        "\n",
        "The RNN would take in the patient's historical data (time series) as input and learn from the patterns and trends in the data to make predictions about their future health status. By leveraging the temporal relationships between data points, RNNs can capture long-term dependencies and complex patterns in the patient's vital signs, which might not be apparent from a single snapshot of data.\n",
        "\n",
        "The trained RNN model could then be used to monitor patients in real-time, continuously analyzing their incoming data and alerting medical staff if the risk of sepsis (or any other target condition) is detected to be increasing, enabling early intervention and potentially improving patient outcomes.\n",
        "\n",
        "Overall, RNNs in this healthcare context offer the advantage of considering both the temporal aspect of the data and the interdependencies between sequential measurements, making them valuable tools for predictive analytics and decision support in patient care."
      ],
      "metadata": {
        "id": "l5mZAXDiGrp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "__PLonA933hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Recurrent Neural Network (RNN)?\n",
        "   - A Recurrent Neural Network (RNN) is a type of artificial neural network designed to process sequential data by introducing loops within the network architecture. These loops allow information to persist and be passed from one time step to another, making RNNs well-suited for tasks involving sequential or time-series data.\n",
        "\n",
        "2. How does an RNN handle sequential data?\n",
        "   - RNNs use the same set of weights for each time step, allowing them to process sequential data of varying lengths. The output of a time step is fed back into the network as input for the next time step, creating a feedback loop that enables information retention.\n",
        "\n",
        "3. What are some applications of RNNs?\n",
        "   - RNNs find applications in various fields, including natural language processing (NLP) tasks such as language modeling, machine translation, speech recognition, sentiment analysis, and text generation. They are also used in time-series forecasting, music generation, video analysis, and more.\n",
        "\n",
        "4. What are the challenges with traditional RNNs?\n",
        "   - Traditional RNNs suffer from the vanishing gradient problem, which hinders their ability to capture long-term dependencies in sequences. When gradients become very small during training, it can be challenging for the model to learn from distant time steps in the sequence.\n",
        "\n",
        "5. How do Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) address RNN limitations?\n",
        "   - LSTM and GRU are advanced variants of RNNs that incorporate gating mechanisms. These gates allow the model to control the flow of information, mitigating the vanishing gradient problem. LSTM and GRU architectures have shown improved performance in capturing long-range dependencies in sequential data.\n",
        "\n",
        "6. Can RNNs handle variable-length sequences?\n",
        "   - Yes, RNNs are capable of handling variable-length sequences. Since they share weights across time steps, they can process sequences of different lengths during both training and inference.\n",
        "\n",
        "7. What is the role of time step truncation in training RNNs?\n",
        "   - Due to the vanishing gradient problem, training RNNs on very long sequences can be computationally expensive and time-consuming. Time step truncation involves breaking long sequences into smaller segments, allowing for more efficient training and better handling of long-range dependencies.\n",
        "\n",
        "8. Are there any alternatives to RNNs for sequential data?\n",
        "   - Yes, apart from RNNs, transformers have gained popularity for sequential tasks. Transformers utilize self-attention mechanisms to process sequences in parallel, making them more efficient for long-range dependencies. However, they may require more data and computational resources compared to RNNs.\n",
        "\n",
        "9. How can RNNs be used in generative tasks?\n",
        "   - RNNs can be used in generative tasks like text generation, music composition, and image captioning. By training the network to predict the next element in a sequence based on the previous ones, the model can generate new data points.\n",
        "\n",
        "10. What are some famous architectures that use RNNs?\n",
        "    - Some well-known architectures include Seq2Seq (Sequence-to-Sequence) models for machine translation, attention-based models, and bi-directional RNNs that process sequences in both forward and backward directions. Additionally, encoder-decoder architectures are often used for various NLP tasks."
      ],
      "metadata": {
        "id": "aYHDTUsW7cOJ"
      }
    }
  ]
}