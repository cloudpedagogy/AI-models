{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCBmScXBJYOHKRD0ocy57+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model Background"
      ],
      "metadata": {
        "id": "ljldcaJEAqNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer is a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It was designed primarily for natural language processing (NLP) tasks, but its versatility has led to its application in various other domains, such as computer vision and speech recognition.\n",
        "\n",
        "The Transformer architecture relies heavily on the concept of self-attention mechanisms, allowing it to capture long-range dependencies in the input data, making it particularly effective for tasks involving sequential or contextual information.\n",
        "\n",
        "**Pros of the Transformer neural network**:\n",
        "\n",
        "1. Parallelism: Transformers can process input sequences in parallel, making them more efficient than traditional recurrent neural networks (RNNs) for longer sequences.\n",
        "\n",
        "2. Capturing Long-Range Dependencies: Due to self-attention mechanisms, Transformers can efficiently capture dependencies between distant words in a sentence, enabling better understanding of the context.\n",
        "\n",
        "3. Scalability: Transformers can be effectively trained on large datasets and can handle longer input sequences without memory or performance issues.\n",
        "\n",
        "4. Versatility: Although initially designed for NLP tasks, the Transformer architecture has been successfully adapted to various other domains, including computer vision and speech processing.\n",
        "\n",
        "5. Reduced Vanishing/Exploding Gradient Problem: Transformers' self-attention mechanism helps mitigate the vanishing and exploding gradient issues encountered in RNNs, making them easier to train.\n",
        "\n",
        "**Cons of the Transformer neural network**:\n",
        "\n",
        "1. High Computational Complexity: The self-attention mechanism results in quadratic complexity with respect to the sequence length, making it computationally more expensive for longer sequences.\n",
        "\n",
        "2. Memory Consumption: Transformers require more memory to store intermediate activations compared to RNNs, which can be a concern for very large models and sequences.\n",
        "\n",
        "3. Interpretability: Transformers can be complex, and interpreting their decisions might be challenging compared to simpler models like logistic regression.\n",
        "\n",
        "4. Data Dependency: Transformers rely heavily on large amounts of data to generalize well, which might be an issue for tasks with limited data availability.\n",
        "\n",
        "**When to use the Transformer neural network**:\n",
        "\n",
        "1. Sequential Data: Transformers are particularly well-suited for tasks involving sequential data, such as machine translation, sentiment analysis, question-answering systems, and language modeling.\n",
        "\n",
        "2. Long-Range Dependencies: When your task requires capturing long-range dependencies between elements in the input sequence, Transformers excel over traditional sequential models like RNNs.\n",
        "\n",
        "3. Large Datasets: Transformers can fully leverage the power of large datasets due to their scalability, making them a good choice for tasks with access to substantial amounts of training data.\n",
        "\n",
        "4. State-of-the-Art Results: If your primary goal is achieving state-of-the-art performance in NLP or other related tasks, Transformers have shown impressive results in various benchmarks.\n",
        "\n",
        "However, for smaller datasets or tasks with limited computational resources, simpler models like RNNs or convolutional neural networks (CNNs) might still be viable options. Additionally, if interpretability is crucial for your task, using simpler models might be more beneficial. It's essential to consider your specific use case and available resources before deciding to use the Transformer architecture."
      ],
      "metadata": {
        "id": "hRSRd-eV-hAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "vRDGGo8fbA90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (if not already installed)\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the Transformer model\n",
        "class CustomTransformerModel(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model_name, num_classes):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Define a custom dataset (you can replace this with your own dataset)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample data for demonstration\n",
        "    texts = [\n",
        "        \"This is a sample sentence.\",\n",
        "        \"The Transformer model works well for natural language processing tasks.\",\n",
        "        \"Hugging Face's Transformers library is great for implementing NLP models.\",\n",
        "    ]\n",
        "    labels = [0, 1, 1]\n",
        "\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load pre-trained model and tokenizer\n",
        "    pretrained_model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "    model = CustomTransformerModel(pretrained_model_name, num_classes=2).to(device)\n",
        "\n",
        "    # Prepare the data and create DataLoader\n",
        "    max_length = 64\n",
        "    dataset = CustomDataset(texts, labels, tokenizer, max_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Training loop (for demonstration purposes, adjust this for real training)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    num_epochs = 3\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Avg. Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "    # Example inference\n",
        "    model.eval()\n",
        "    test_sentence = \"The Transformer model is awesome!\"\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        test_sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "        print(f\"Inference: {test_sentence}\")\n",
        "        print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "foN5RrC43k5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "Joz623tz0fM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Install Required Libraries:**\n",
        "   - This code uses PyTorch and Hugging Face Transformers. If they are not installed, the code installs them using pip.\n",
        "\n",
        "2. **Import Libraries:**\n",
        "   - Import the necessary libraries: `torch`, `BertTokenizer`, `BertModel` from the `transformers` library, and `DataLoader`, `Dataset` from PyTorch.\n",
        "\n",
        "3. **Define the Custom Transformer Model:**\n",
        "   - Create a custom class `CustomTransformerModel` that inherits from `torch.nn.Module`.\n",
        "   - The constructor takes two arguments: `pretrained_model_name` (the name of the pre-trained BERT model) and `num_classes` (number of classes for classification).\n",
        "   - In the constructor, the BERT tokenizer and model are loaded using `BertTokenizer.from_pretrained` and `BertModel.from_pretrained`.\n",
        "   - The model includes a dropout layer (`torch.nn.Dropout`) and a linear layer (`torch.nn.Linear`) to produce the final logits for classification.\n",
        "   - The `forward` method performs the forward pass of the model, taking `input_ids` and `attention_mask` as inputs and returning the logits.\n",
        "\n",
        "4. **Define Custom Dataset:**\n",
        "   - Create a custom class `CustomDataset` that inherits from `Dataset`.\n",
        "   - The constructor takes `texts`, `labels`, `tokenizer`, and `max_length`.\n",
        "   - The `__len__` method returns the number of samples in the dataset.\n",
        "   - The `__getitem__` method retrieves an item at a given index and returns the input tensors (`input_ids` and `attention_mask`) and the label as a dictionary.\n",
        "\n",
        "5. **Example Usage:**\n",
        "   - The code demonstrates how to use the custom model and dataset.\n",
        "   - Sample texts and labels are defined for demonstration purposes.\n",
        "\n",
        "6. **Set Device and Load Pre-trained Model:**\n",
        "   - The device is set to GPU (`\"cuda\"`) if available, otherwise CPU (`\"cpu\"`).\n",
        "   - The BERT tokenizer and the custom model are loaded with the pre-trained weights.\n",
        "   - The model is moved to the selected device using `.to(device)`.\n",
        "\n",
        "7. **Prepare Data and Create DataLoader:**\n",
        "   - The dataset is created using the custom dataset class, passing in the texts, labels, tokenizer, and maximum sequence length (`max_length`).\n",
        "   - The DataLoader is created to efficiently handle the data during training, with a batch size of 2 and shuffling the data.\n",
        "\n",
        "8. **Training Loop (For Demonstration Purposes):**\n",
        "   - An optimizer (`AdamW`) is initialized with a learning rate of 1e-5 to update the model parameters during training.\n",
        "   - The training loop iterates over each epoch and each batch of data from the DataLoader.\n",
        "   - The input data, labels, and attention mask are moved to the selected device.\n",
        "   - The optimizer's gradients are reset with `optimizer.zero_grad()`.\n",
        "   - The model is called with `model(input_ids, attention_mask)` to get the logits.\n",
        "   - Cross-entropy loss is calculated and used for backpropagation and updating the model parameters.\n",
        "   - The average loss for the epoch is printed.\n",
        "\n",
        "9. **Example Inference:**\n",
        "   - The model is set to evaluation mode using `model.eval()`.\n",
        "   - An example test sentence is defined.\n",
        "   - The test sentence is tokenized using the tokenizer and converted to input tensors.\n",
        "   - The logits are obtained for the input tensors, and the predicted label is obtained by taking the argmax.\n",
        "   - The test sentence and the predicted label are printed.\n",
        "\n",
        "That's a detailed explanation of the code, covering model definition, data preparation, training loop, and example inference using a custom Transformer model for text classification."
      ],
      "metadata": {
        "id": "5V0s9tcU4fbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "Yhvc079I15vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A real-world example of a Transformer model in a healthcare setting is the application of natural language processing (NLP) models for clinical text processing. In healthcare, there is a vast amount of unstructured clinical data, including medical notes, electronic health records (EHRs), and research papers. These data sources contain valuable information about patients' conditions, treatments, and outcomes.\n",
        "\n",
        "The Transformer architecture, particularly models like BERT (Bidirectional Encoder Representations from Transformers) and its variants, have been leveraged to process and extract useful insights from clinical text data. Here's how it can be applied:\n",
        "\n",
        "1. Clinical Text Classification: Transformer models can be used to classify clinical notes or EHR entries into various categories. For example, identifying whether a patient's note describes a diagnosis, a symptom, a prescription, or a treatment. This helps in organizing and categorizing the vast amount of textual information available in EHRs, making it easier for clinicians to retrieve relevant data.\n",
        "\n",
        "2. Named Entity Recognition (NER): NER involves identifying and classifying entities like diseases, medications, procedures, or anatomical terms within clinical text. Transformers can be trained to perform NER on clinical notes, allowing for the automated extraction of critical information from patient records and supporting tasks like adverse event detection or pharmacovigilance.\n",
        "\n",
        "3. Clinical Question Answering: Transformer models can be fine-tuned to answer specific medical questions based on the context provided in clinical literature or research papers. These models can assist healthcare professionals in finding relevant evidence-based answers quickly, aiding in diagnosis and treatment decisions.\n",
        "\n",
        "4. Clinical Language Understanding: Transformers can be utilized to build powerful language models that can understand medical terms and jargon. These models can assist in interpreting free-text entries made by clinicians, helping to maintain accurate and up-to-date records.\n",
        "\n",
        "5. Drug-Drug Interaction Prediction: Transformers can analyze drug-related text data, including drug descriptions, medical literature, and pharmacological databases, to predict potential drug-drug interactions. This capability supports medication safety and decision-making by alerting clinicians to potential risks.\n",
        "\n",
        "The application of Transformer models in healthcare has the potential to improve patient care, optimize clinical workflows, and enhance medical research by efficiently processing and understanding vast amounts of unstructured clinical data. However, it's important to ensure data privacy and adhere to ethical guidelines while deploying such models in healthcare settings."
      ],
      "metadata": {
        "id": "4rc5nk4kI-hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "fgFm22k73Xrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Transformer model, and how does it differ from traditional neural networks?\n",
        "   - A Transformer is a type of neural network architecture designed to process sequential data, such as natural language. Unlike traditional sequential models like RNNs and LSTMs, Transformers use self-attention mechanisms to capture dependencies between different positions in the input sequence without relying on a sequential order.\n",
        "\n",
        "2. Who introduced the Transformer model?\n",
        "   - The Transformer model was introduced in the paper \"Attention Is All You Need\" by Vaswani et al., published in 2017.\n",
        "\n",
        "3. What is the key innovation of the Transformer model?\n",
        "   - The key innovation of the Transformer model is the self-attention mechanism, which allows it to weigh the importance of different input positions when processing a particular position, enabling it to learn long-range dependencies efficiently.\n",
        "\n",
        "4. What is self-attention, and how does it work in Transformers?\n",
        "   - Self-attention is a mechanism that allows each position in the input sequence to attend to all other positions to compute a weighted sum of their representations. In Transformers, this attention mechanism is computed through three linear transformations (query, key, and value) and scaled dot-product attention.\n",
        "\n",
        "5. What are some popular applications of the Transformer model?\n",
        "   - Transformers have achieved state-of-the-art results in a wide range of natural language processing tasks, including machine translation, language modeling, question answering, sentiment analysis, and text generation.\n",
        "\n",
        "6. What is BERT, and how is it related to the Transformer model?\n",
        "   - BERT (Bidirectional Encoder Representations from Transformers) is a popular pre-trained language model based on the Transformer architecture. It can be fine-tuned for various downstream tasks, achieving impressive performance without extensive task-specific data.\n",
        "\n",
        "7. Can Transformers handle tasks other than natural language processing?\n",
        "   - Yes, Transformers can be applied to various domains beyond natural language processing. They have been successfully used in computer vision tasks, such as image recognition and image captioning, as well as in speech recognition and music generation.\n",
        "\n",
        "8. Are there any limitations of the Transformer model?\n",
        "   - Transformers are computationally expensive, especially for long sequences, due to the self-attention mechanism's quadratic complexity. Researchers have been exploring various techniques, such as sparse attention and model parallelism, to mitigate this issue.\n",
        "\n",
        "9. How can I use pre-trained Transformer models for my own NLP tasks?\n",
        "   - Several pre-trained Transformer models, such as BERT and GPT, are available for public use. You can download pre-trained weights and fine-tune the model on your specific NLP task using transfer learning.\n",
        "\n",
        "10. What are some potential future developments for the Transformer model?\n",
        "   - Researchers are continuously exploring ways to improve Transformers by incorporating more efficient attention mechanisms, handling larger datasets, and designing better architectures to tackle more complex tasks. Additionally, Transformers could potentially be extended to handle graph-structured data and other sequential data types."
      ],
      "metadata": {
        "id": "e8tgQNue85jJ"
      }
    }
  ]
}