{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT5baNbsCtBiBWFk+D9+VM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "_NeJ56YP2W_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Attention Mechanism is a crucial component in many neural network architectures, particularly in sequence-to-sequence models and natural language processing tasks. Its primary function is to enable the network to focus on relevant parts of the input sequence when processing a specific element of the output sequence. This mechanism allows the network to \"pay attention\" to different parts of the input while generating the output, making it especially useful for tasks involving variable-length sequences and long-range dependencies.\n",
        "\n",
        "The Attention Mechanism can be understood as follows:\n",
        "\n",
        "1. Input and Output Sequences: Suppose you have an input sequence (e.g., a sentence) and want to generate an output sequence (e.g., translation into another language). Both sequences are divided into individual elements (e.g., words or tokens).\n",
        "\n",
        "2. Context Vector: When generating an element in the output sequence, the attention mechanism assigns weights to each element in the input sequence, indicating their importance or relevance for generating the current output element. These weights collectively form a context vector that is used to influence the output generation.\n",
        "\n",
        "3. Dynamic Focus: Unlike traditional sequence-to-sequence models that use a fixed-length context vector, the attention mechanism allows the model to dynamically focus on different parts of the input sequence for different elements in the output sequence.\n",
        "\n",
        "**Pros of the Attention Mechanism**:\n",
        "1. Improved Performance: Attention helps the model focus on relevant parts of the input, leading to improved performance in tasks involving long sequences or complex dependencies.\n",
        "\n",
        "2. Captures Global Context: Attention enables the model to consider the entire input sequence rather than being limited to fixed-size context windows, making it better suited for tasks requiring a broader context.\n",
        "\n",
        "3. Reduced Vanishing Gradient Problem: By allowing direct connections between input and output, attention mitigates the vanishing gradient problem, making it easier to train deeper models.\n",
        "\n",
        "4. Interpretable: Attention weights provide insights into which parts of the input are important for generating specific parts of the output, making the model more interpretable.\n",
        "\n",
        "**Cons of the Attention Mechanism**:\n",
        "1. Computational Overhead: Attention introduces additional computations, as the model has to compute the relevance of each input element for every output element. This can make training and inference slower compared to non-attention models.\n",
        "\n",
        "2. Model Complexity: Implementing attention in neural network architectures adds complexity to the model, making it harder to design, understand, and optimize.\n",
        "\n",
        "3. Overfitting: Attention can sometimes lead to overfitting, especially when there is noise in the training data or the attention weights are overemphasized during training.\n",
        "\n",
        "**When to use the Attention Mechanism**:\n",
        "The Attention Mechanism is particularly beneficial in the following scenarios:\n",
        "\n",
        "1. Sequence-to-Sequence Tasks: When dealing with tasks involving variable-length input and output sequences, such as machine translation, text summarization, and speech recognition.\n",
        "\n",
        "2. Long Dependencies: When the task requires capturing long-range dependencies between elements in the input and output sequences.\n",
        "\n",
        "3. Semantic Understanding: For tasks where a model needs to understand the meaning or context of different parts of the input to generate a relevant and coherent output.\n",
        "\n",
        "4. Visual Attention: In computer vision tasks, attention mechanisms have been used to process images and focus on specific regions, improving image captioning, object recognition, and visual question answering.\n",
        "\n",
        "In summary, the Attention Mechanism is a powerful tool for enhancing the performance of neural network models in tasks involving sequences with variable lengths and complex dependencies. However, its use comes with computational costs and increased model complexity, so it should be considered based on the specific requirements of the task and available resources."
      ],
      "metadata": {
        "id": "wyEOkbHmCpKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "OgNx7m-gCGNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dot, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "def create_attention_model(input_vocab_size, output_vocab_size, input_sequence_length, output_sequence_length, hidden_units):\n",
        "    encoder_inputs = Input(shape=(input_sequence_length,))\n",
        "    decoder_inputs = Input(shape=(output_sequence_length,))\n",
        "\n",
        "    encoder_embedding = Embedding(input_vocab_size, hidden_units, input_length=input_sequence_length)\n",
        "    decoder_embedding = Embedding(output_vocab_size, hidden_units, input_length=output_sequence_length)\n",
        "\n",
        "    encoder_inputs_embedded = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs_embedded = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs_embedded)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state=encoder_states)\n",
        "\n",
        "    attention_layer = Dot(axes=[2, 2])\n",
        "    attention, _ = scaled_dot_product_attention(decoder_outputs, encoder_outputs, encoder_outputs)\n",
        "    decoder_attention = tf.concat([attention, decoder_outputs], axis=-1)\n",
        "\n",
        "    decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
        "    output = decoder_dense(decoder_attention)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "input_vocab_size = 10000  # Replace with your input vocabulary size\n",
        "output_vocab_size = 8000  # Replace with your output vocabulary size\n",
        "input_sequence_length = 50  # Replace with your input sequence length\n",
        "output_sequence_length = 60  # Replace with your output sequence length\n",
        "hidden_units = 128  # Replace with the number of hidden units in the LSTM layer\n",
        "\n",
        "model = create_attention_model(input_vocab_size, output_vocab_size, input_sequence_length, output_sequence_length, hidden_units)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "1jXrQakRq_uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "06K5_tmJQlkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import necessary libraries:\n",
        "   - `tensorflow`: The main library used for building and training neural networks in TensorFlow.\n",
        "   - `tensorflow.keras.layers`: Contains various layers that can be used to construct neural network architectures.\n",
        "   - `tensorflow.keras.models`: Contains the `Model` class used to create Keras models.\n",
        "\n",
        "2. Define the `scaled_dot_product_attention` function:\n",
        "   - This function calculates the scaled dot product attention mechanism used in the Transformer model.\n",
        "   - It takes in three arguments, `q`, `k`, and `v`, representing the query, key, and value tensors, respectively.\n",
        "   - The function computes the scaled dot product between `q` and `k`, then scales the result by the square root of the dimension of `k`.\n",
        "   - If a `mask` is provided, it is applied to the attention logits to mask out certain positions in the output.\n",
        "\n",
        "3. Define the `create_attention_model` function:\n",
        "   - This function creates a custom attention-based sequence-to-sequence model using Keras.\n",
        "   - It takes in several arguments, such as `input_vocab_size`, `output_vocab_size`, `input_sequence_length`, `output_sequence_length`, and `hidden_units`.\n",
        "   - Inside the function, an encoder-decoder architecture is built using LSTM cells.\n",
        "   - The input tensors (`encoder_inputs` and `decoder_inputs`) are embedded using separate embedding layers for the encoder and decoder.\n",
        "   - The encoder LSTM processes the embedded input sequences and produces the `encoder_outputs` and the final hidden and cell states (`encoder_states`).\n",
        "   - The decoder LSTM processes the embedded target sequences (`decoder_inputs_embedded`) with the initial states set to the final states of the encoder.\n",
        "   - The attention mechanism is applied using the `Dot` layer. The attention is calculated between the decoder LSTM outputs and the encoder LSTM outputs.\n",
        "   - The attention context is concatenated with the decoder LSTM outputs and fed into a dense layer to produce the final output logits for the model.\n",
        "   - The model is defined using the `Model` class and returned.\n",
        "\n",
        "4. Example usage:\n",
        "   - The code provides an example of how to use the `create_attention_model` function.\n",
        "   - You need to specify the sizes of your input and output vocabularies (`input_vocab_size` and `output_vocab_size`).\n",
        "   - Also, you need to set the lengths of your input and output sequences (`input_sequence_length` and `output_sequence_length`).\n",
        "   - The number of hidden units in the LSTM layer is set to `hidden_units`.\n",
        "\n",
        "5. Create the model:\n",
        "   - The example usage creates the attention-based sequence-to-sequence model by calling the `create_attention_model` function with the specified arguments.\n",
        "   - The model is stored in the variable `model`.\n",
        "\n",
        "6. Display the model summary:\n",
        "   - The summary of the model, including the layers, output shape, and the number of parameters, is printed using the `model.summary()` method.\n",
        "\n",
        "Note: The model is set up as an example and may not be complete or optimized for any specific task. In practice, you need to adjust the architecture and hyperparameters based on your specific dataset and task requirements."
      ],
      "metadata": {
        "id": "zBAe8UzBUymx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "gkp31vfMYK3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of the Attention Mechanism in a healthcare setting is its application in Electronic Health Record (EHR) data processing. Electronic Health Records contain vast amounts of patient information, including clinical notes, lab results, radiology reports, and more. Extracting meaningful information from EHRs is crucial for clinical decision-making, but it can be challenging due to the unstructured nature and sheer volume of data.\n",
        "\n",
        "Attention Mechanism in healthcare helps to address this challenge by allowing models to focus on the most relevant parts of the patient's medical history or clinical notes while making predictions. Let's take a specific example of using the Attention Mechanism in healthcare for the task of clinical notes summarization:\n",
        "\n",
        "**Clinical Notes Summarization with Attention Mechanism:**\n",
        "The task is to summarize long and complex clinical notes written by healthcare providers into concise and informative summaries, capturing the essential details of a patient's medical condition.\n",
        "\n",
        "**How it works:**\n",
        "1. **Data Preprocessing:**\n",
        "   - The raw clinical notes are preprocessed to remove irrelevant information and anonymize patient identifiers. The text is tokenized into sentences or smaller units for further analysis.\n",
        "\n",
        "2. **Token Embedding:**\n",
        "   - Each token (word or subword) in the clinical notes is embedded into a numerical vector representation, allowing the model to process and understand the text.\n",
        "\n",
        "3. **Encoder-Decoder Architecture:**\n",
        "   - The Attention Mechanism is commonly used in an Encoder-Decoder architecture for this task.\n",
        "   - The encoder processes the input clinical notes and encodes them into a context vector representation.\n",
        "   - The decoder takes the context vector and generates the summary token by token.\n",
        "\n",
        "4. **Attention Mechanism:**\n",
        "   - During the decoding process, the Attention Mechanism is employed to dynamically weigh the importance of each token in the encoder's output when generating the summary.\n",
        "   - The attention scores are calculated based on the similarity between the current decoder state and each encoder token's representation.\n",
        "   - Tokens that are most relevant to the current decoding step receive higher attention scores, allowing the model to focus on essential information and neglect irrelevant details.\n",
        "\n",
        "5. **Training:**\n",
        "   - The model is trained using a dataset of clinical notes and their corresponding summaries.\n",
        "   - During training, the model learns to align the encoder and decoder representations effectively to generate accurate and informative summaries.\n",
        "\n",
        "6. **Inference:**\n",
        "   - In the inference phase, the trained model can take new and unseen clinical notes as input and generate summaries by paying attention to the most relevant information.\n",
        "\n",
        "**Benefits:**\n",
        "- The Attention Mechanism allows the model to automatically learn to focus on critical clinical details, such as diagnosis, treatment plans, and patient outcomes, while disregarding less relevant information.\n",
        "- The summarization process becomes more accurate and efficient, enabling healthcare providers to quickly extract essential information from lengthy clinical notes, leading to better patient care and decision-making.\n",
        "\n",
        "**Challenges:**\n",
        "- Developing accurate and robust Attention Mechanism models for clinical notes summarization requires large and high-quality annotated datasets, which can be challenging to obtain due to privacy concerns and data scarcity in healthcare settings.\n",
        "- Fine-tuning and optimizing the model architecture may require significant computational resources and expertise.\n",
        "\n",
        "Overall, the Attention Mechanism in healthcare has the potential to revolutionize information extraction and summarization from Electronic Health Records, improving healthcare providers' ability to access critical patient information efficiently. It can facilitate faster and more informed decision-making, ultimately leading to better patient outcomes and improved healthcare delivery."
      ],
      "metadata": {
        "id": "UFZSzRo461BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "rIkaYwy-Z6iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the Attention Mechanism in the context of neural networks?\n",
        "   - The Attention Mechanism is a concept in deep learning that allows a model to focus on the most relevant parts of the input data while making predictions. It was originally introduced for sequence-to-sequence tasks in natural language processing but has since been applied to various other tasks and domains.\n",
        "\n",
        "2. How does the Attention Mechanism work?\n",
        "   - The Attention Mechanism assigns weights to different parts of the input data based on their importance for the task at hand. These weights indicate how much attention the model should pay to each input element when making predictions. The model learns these weights during training, enabling it to focus on relevant information.\n",
        "\n",
        "3. What problems does the Attention Mechanism solve?\n",
        "   - The Attention Mechanism helps address issues with traditional sequence-to-sequence models, where the entire input sequence is treated equally, leading to information loss and inefficiencies. By allowing the model to attend to specific parts of the input, it improves the model's ability to handle long sequences and capture long-range dependencies.\n",
        "\n",
        "4. Can the Attention Mechanism be used in different types of neural networks?\n",
        "   - Yes, the Attention Mechanism is not limited to specific neural network architectures. It can be applied to various models, such as recurrent neural networks (RNNs), transformer-based models, and convolutional neural networks (CNNs), to enhance their performance and interpretability.\n",
        "\n",
        "5. How does the Attention Mechanism improve natural language processing tasks?\n",
        "   - In natural language processing, the Attention Mechanism helps the model focus on essential words or phrases in the input sentence while generating the output. This enables the model to produce more accurate and contextually relevant translations, summaries, or answers in question-answering tasks.\n",
        "\n",
        "6. Does the Attention Mechanism have applications beyond NLP?\n",
        "   - Yes, the Attention Mechanism has proven useful in other domains as well. It has been successfully applied in computer vision tasks, speech recognition, recommendation systems, and even in reinforcement learning.\n",
        "\n",
        "7. What are some variations of the Attention Mechanism?\n",
        "   - Several variations of the Attention Mechanism exist, including self-attention, scaled dot-product attention, multi-head attention, and global attention. Each variation introduces slight modifications to the original mechanism to suit different architectures and tasks.\n",
        "\n",
        "8. How does the Attention Mechanism handle long sequences in practice?\n",
        "   - The Attention Mechanism's ability to selectively focus on relevant parts of the input helps address the challenges of handling long sequences. By paying attention to only relevant elements, the model can maintain context and dependencies over long distances without being overwhelmed by irrelevant information.\n",
        "\n",
        "9. Does the Attention Mechanism have any limitations or challenges?\n",
        "   - While the Attention Mechanism is powerful, it can be computationally expensive, especially for very long sequences. Researchers are continuously working on optimizing the mechanism to make it more efficient for handling large-scale data.\n",
        "\n",
        "10. What future developments can we expect for the Attention Mechanism?\n",
        "   - As deep learning research progresses, we can expect more sophisticated and efficient variants of the Attention Mechanism to emerge. Additionally, attention-based models might be integrated into more diverse architectures to further improve performance across various tasks."
      ],
      "metadata": {
        "id": "jZdnXmLcUd3t"
      }
    }
  ]
}