{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo22dAnvxWrsev3DjKKslh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/dl/Deep_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simplified template of a Deep Learning model using Python's Keras library. This example constructs a basic feed-forward neural network for a binary classification task. You can easily extend this template for more complex models and tasks:"
      ],
      "metadata": {
        "id": "HGOwF2EDIYxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.iloc[:, 0:8]\n",
        "Y = data.iloc[:, 8]\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split your data into train and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Add an input layer\n",
        "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "# Add one hidden layer\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "# Add an output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "nM0RUePQINRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, this is a simple template. Real-world problems often require a more sophisticated approach, like fine-tuning model architecture, optimizing hyperparameters, using different types of layers (Convolutional, Recurrent, etc.), and applying regularization techniques. This code should serve as a good starting point, though.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CvB7uFAWIi5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "5zuRSqJBIkg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's break it down:\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "```\n",
        "These lines import necessary libraries. `Sequential`, `Dense` and `Adam` are classes from Keras, used to create a model, a fully connected layer and an optimizer, respectively. `pandas` is used for data manipulation and analysis, `train_test_split` is used for splitting the data into training and testing data, and `StandardScaler` is used for feature scaling.\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "```\n",
        "The dataset is being loaded from a URL. It is the Pima Indians Diabetes dataset. The column names are provided in the `names` list and are assigned to the columns of the loaded data using the `names` argument in `pd.read_csv()`.\n",
        "\n",
        "```python\n",
        "X = data.iloc[:, 0:8]\n",
        "Y = data.iloc[:, 8]\n",
        "```\n",
        "The feature variables (input) and target variable (output) are being separated. `X` consists of the first 8 columns of the dataset (indexed from 0 to 7) which represent the input features, and `Y` is the last column which represents the target.\n",
        "\n",
        "```python\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "```\n",
        "The `StandardScaler` standardizes the features by removing the mean and scaling to unit variance. It is fit to the data using `fit_transform()` which calculates the parameters needed for scaling and applies the transformation.\n",
        "\n",
        "```python\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "```\n",
        "The `train_test_split()` function splits the data into training data and testing data. 80% of the data will be used for training the model and 20% will be used for evaluating it.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "```\n",
        "This block defines the architecture of the model. A `Sequential` model is used, which means that the model is composed of a linear stack of layers. The model has one input layer with 64 units, a hidden layer with 32 units, and an output layer with 1 unit. The `relu` activation function is used for the input and hidden layer and the `sigmoid` activation function is used for the output layer, which is suitable for binary classification.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "In the `compile()` function, the optimizer, loss function, and metrics are being specified. The Adam optimizer is used with a learning rate of 0.001. The loss function is binary cross-entropy, which is suitable for binary classification. The metric used to evaluate the model is accuracy.\n",
        "\n",
        "```python\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test))\n",
        "```\n",
        "The model is trained using the `fit()` function, with 100 epochs and a batch size of 32. The `validation_data` parameter is also set, so the model will evaluate its performance on the testing data at the end of each epoch.\n",
        "\n",
        "```python\n",
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "```\n",
        "The `evaluate()` function is used to measure the performance of the model on the test data after training. It returns the loss value and the metrics value for the model in test mode. The accuracy is then printed.\n"
      ],
      "metadata": {
        "id": "o0l7y8o112Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU Information in Colab"
      ],
      "metadata": {
        "id": "94Ezh-Was3Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning models, particularly neural networks, require a lot of computational power. They involve many matrix operations (like multiplication and addition), which are carried out repeatedly during the process of training the model. CPUs (Central Processing Units) are not designed to perform these operations in a highly efficient manner.\n",
        "\n",
        "GPUs (Graphics Processing Units), on the other hand, are designed to perform matrix operations extremely quickly, as these operations are also common in graphics rendering (which GPUs were originally built for). A GPU has many more cores than a CPU, and while each individual core is slower than a CPU core, the sheer number of GPU cores and their ability to process tasks in parallel makes the GPU much better suited to performing matrix operations quickly.\n",
        "\n",
        "This ability to process tasks in parallel makes GPUs ideal for training deep learning models. Training time can be significantly reduced, from weeks or days (when using a CPU) to hours or minutes (when using a GPU).\n",
        "\n",
        "Google Colab provides free access to a GPU for a certain amount of time, which can be very beneficial when training large deep learning models. Therefore, switching to GPU runtime in Colab can greatly accelerate the training process of your deep learning models.\n",
        "\n",
        "Remember to change the runtime to GPU when you're running computation-intensive tasks. For simple tasks that do not require much computation, the difference in speed between GPU and CPU may not be noticeable, and using CPU may be more cost-effective if you are using paid resources."
      ],
      "metadata": {
        "id": "j35Xx5rts5H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps in switching CPU in Colab"
      ],
      "metadata": {
        "id": "W2vKuKjetZ_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " To switch to a GPU in Google Colaboratory, follow these steps:\n",
        "\n",
        "1. After opening your notebook, go to the toolbar at the top of the page.\n",
        "2. Click on `Runtime` or `Edit` depending on the interface changes.\n",
        "3. In the dropdown menu, click on `Change runtime type`.\n",
        "4. In the pop-up window, under `Hardware Accelerator`, select `GPU` from the dropdown list.\n",
        "5. Click on `Save`.\n",
        "\n",
        "\n",
        "Keep in mind that the availability of the GPU may depend on the current demand, as Google Colab provides a shared resource. If you have trouble getting a GPU, you can try again later or consider upgrading to Colab Pro for more reliable access."
      ],
      "metadata": {
        "id": "d4xKtqAntQy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting your Deep Learning Model"
      ],
      "metadata": {
        "id": "LvB34IH1xaGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why to Export a Model?**\n",
        "\n",
        "The primary reason to export or save a model is for reuse. Training a deep learning model can take a long time, and once that process is complete, you don't want to redo it every time you need to make a prediction. By saving your trained model, you can reuse it whenever you like, either in the same environment or in a different one. This also allows you to share your model with others without requiring them to retrain it.\n",
        "\n",
        "**How to Export Models to Google Drive in Colab?**\n",
        "\n",
        "First, you need to mount your Google Drive in your notebook:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "```\n",
        "\n",
        "This code will ask for your authorization code. Follow the provided link, sign in to your Google account, and then copy the authorization code and paste it into the input box. Now, you have your Google Drive mounted in your Colab environment and you can read and write files.\n",
        "\n",
        "**1. Scikit-learn**\n",
        "\n",
        "Scikit-learn models can be saved using the `pickle` or `joblib` modules, as follows:\n",
        "\n",
        "```python\n",
        "import joblib\n",
        "\n",
        "# Save the model to Google Drive\n",
        "joblib.dump(model, '/content/gdrive/My Drive/sklearn_model.pkl')\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('/content/gdrive/My Drive/sklearn_model.pkl')\n",
        "```\n",
        "\n",
        "**2. PyTorch**\n",
        "\n",
        "PyTorch models can be saved using `torch.save()`:\n",
        "\n",
        "```python\n",
        "# Save the model to Google Drive\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/pytorch_model.pth')\n",
        "\n",
        "# Load the model\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/pytorch_model.pth'))\n",
        "```\n",
        "\n",
        "**3. Keras**\n",
        "\n",
        "In Keras, you can use the `model.save()` function:\n",
        "\n",
        "```python\n",
        "# Save the entire model to Google Drive\n",
        "model.save('/content/gdrive/My Drive/keras_model.h5')\n",
        "\n",
        "# Later, load the model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/gdrive/My Drive/keras_model.h5')\n",
        "```\n",
        "\n",
        "**4. TensorFlow**\n",
        "\n",
        "In TensorFlow, you can save and load models with `tf.saved_model`:\n",
        "\n",
        "```python\n",
        "# Save the model\n",
        "tf.saved_model.save(model, \"/content/gdrive/My Drive/tensorflow_model\")\n",
        "\n",
        "# Load the model\n",
        "model = tf.saved_model.load(\"/content/gdrive/My Drive/tensorflow_model\")\n",
        "```\n",
        "\n",
        "Replace `\"My Drive/your_model_file.extension\"` with your preferred path and model file name in Google Drive.\n",
        "\n",
        "By saving the models directly to Google Drive, you ensure that your trained models are safe even if your Colab instance disconnects or is recycled, and you can access your models from any environment that can connect to Google Drive."
      ],
      "metadata": {
        "id": "o-2qHIz1xYdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "nf9-BgiCJXQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a comprehensive list of some common terms used in the field of deep learning:\n",
        "\n",
        "1. **Artificial Neural Network (ANN):** Computational models inspired by the human brain. These models are used to predict outputs from a set of inputs.\n",
        "\n",
        "2. **Deep Learning:** A subfield of machine learning that focuses on algorithms inspired by the structure and function of the brain's neural networks.\n",
        "\n",
        "3. **Convolutional Neural Network (CNN):** A type of neural network often used in image processing that applies a number of filters to the raw pixel data of an image to extract and learn higher-level features.\n",
        "\n",
        "4. **Recurrent Neural Network (RNN):** A type of neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or the spoken word.\n",
        "\n",
        "5. **Long Short-Term Memory (LSTM):** A type of RNN capable of learning long-term dependencies, often used in sequence prediction problems.\n",
        "\n",
        "6. **Gradient Descent:** An optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent.\n",
        "\n",
        "7. **Backpropagation:** A method used to calculate the gradient of the loss function with respect to the weights in the neural network.\n",
        "\n",
        "8. **Overfitting:** A modeling error that occurs when a function is too closely aligned to a limited set of data points. An overfitted model might have low training error but high testing error.\n",
        "\n",
        "9. **Underfitting:** Occurs when a model cannot capture the underlying pattern of the data. Underfitted models usually have high training and testing errors.\n",
        "\n",
        "10. **Dropout:** A regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
        "\n",
        "11. **Batch Normalization:** A technique for improving the speed, performance, and stability of deep neural networks.\n",
        "\n",
        "12. **Activation Function:** A function that defines the output of a neuron given a set of inputs. Common examples include sigmoid, tanh, ReLU, Leaky ReLU, and softmax.\n",
        "\n",
        "13. **Loss Function:** A method of evaluating how well the algorithm models the dataset. If predictions deviate too much from actual results, loss function would yield a large number.\n",
        "\n",
        "14. **Optimizer:** A method used to adjust the attributes of the neural network such as weights and learning rate in order to reduce the losses.\n",
        "\n",
        "15. **Epoch:** An entire pass over the entire dataset while training a machine learning model.\n",
        "\n",
        "16. **Batch:** The number of samples that are passed through the network before the weights are updated.\n",
        "\n",
        "17. **Learning Rate:** The step size taken for each iteration while moving towards a minimum of a loss function.\n",
        "\n",
        "18. **Data Augmentation:** A technique used to increase the diversity of your training set by applying random (but realistic) transformations such as image rotation.\n",
        "\n",
        "19. **Transfer Learning:** A machine learning method where a pre-trained model is used on a new problem. It is a popular method in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks.\n",
        "\n",
        "20. **Generative Adversarial Networks (GANs):** A class of AI algorithms used in unsupervised machine learning, consisting of two networks, a generator and a discriminator, that contest with each other in a zero-sum game framework.\n",
        "\n",
        "This list could go on, but these are some of the core terms you are likely to come across in the field of deep learning.\n"
      ],
      "metadata": {
        "id": "CHp9X5WcJZiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "atiVgMmLJy49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is the difference between machine learning and deep learning?**\n",
        "\n",
        "   Machine learning is a broader concept, encompassing all methods and techniques for teaching computers to perform tasks by learning patterns in data. Deep learning is a subset of machine learning that specifically deals with artificial neural networks with several layers (\"deep\" structures). These deep networks are capable of learning complex patterns from large amounts of data.\n",
        "\n",
        "2. **Why is deep learning gaining so much popularity?**\n",
        "\n",
        "   Deep learning has been successful in a variety of areas, including image and speech recognition, natural language processing, and recommendation systems, among others. The rise in computational power, the availability of large datasets, and advancements in neural network algorithms have contributed to its popularity.\n",
        "\n",
        "3. **What is the role of a GPU in deep learning?**\n",
        "\n",
        "   Training deep learning models involves a lot of matrix operations. GPUs are designed to handle such operations efficiently, leading to a significant speedup in training time. Hence, they are often preferred for training deep learning models, especially with large datasets.\n",
        "\n",
        "4. **Why do we need so many layers in a deep learning model?**\n",
        "\n",
        "   Each layer in a deep learning model learns to recognize different features in the input data. Lower layers might learn to recognize simple patterns, like edges in an image, while deeper layers combine these simple patterns to recognize more complex features, like shapes or objects. The more layers, the more complexity can be captured by the model.\n",
        "\n",
        "5. **What is the vanishing gradient problem in deep learning?**\n",
        "\n",
        "   The vanishing gradient problem is a difficulty encountered during the training of deep neural networks using gradient-based optimization methods. As the gradient of the loss function is backpropagated to earlier layers, repeated multiplication of small numbers (gradients) can make the gradient exceedingly small. Consequently, weights of the initial layers change very little, and the network fails to learn effectively.\n",
        "\n",
        "6. **What is the difference between a CNN and an RNN?**\n",
        "\n",
        "   Convolutional Neural Networks (CNNs) are mainly used for processing grid-like data such as images, making them effective for tasks like image and video recognition. Recurrent Neural Networks (RNNs), on the other hand, are designed to recognize patterns in sequential data, like time series or natural language, making them effective for tasks like speech recognition or language modeling.\n",
        "\n",
        "7. **What is transfer learning in deep learning?**\n",
        "\n",
        "   Transfer learning is a technique where a pre-trained model is adapted for a new, different task. Instead of starting the learning process from scratch, you start from patterns that have been learned when solving a different problem. This technique can lead to significant savings in computational resources and can also require less data.\n",
        "   \n",
        "8. **What is overfitting and underfitting in the context of deep learning?**\n",
        "\n",
        "   Overfitting occurs when a model learns the training data too well, to the extent that it performs poorly on unseen data because it has also learned the noise in the training data. Underfitting, on the other hand, occurs when a model is too simple to learn the underlying structure of the data, resulting in poor performance on both the training and unseen data."
      ],
      "metadata": {
        "id": "M2nbVdSSJ0Qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future trends"
      ],
      "metadata": {
        "id": "L7mjgnS2KpbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Improved Explainability and Transparency:** As deep learning models are often considered \"black boxes\", there's a growing demand for improved explainability and transparency. Techniques that shed light on the internal mechanisms of these models will become more advanced and widespread.\n",
        "\n",
        "2. **Efficient Deep Learning Models:** With the increasing complexity of deep learning models, there's a growing trend towards making these models more efficient, in terms of both computational requirements and power consumption. Techniques such as model pruning, quantization, and knowledge distillation are expected to become more sophisticated.\n",
        "\n",
        "3. **Transfer Learning and Few-shot Learning:** Transfer learning, which involves applying knowledge from one model to another, is expected to become more prominent. Few-shot learning, which allows models to understand information with a very small amount of data, will also continue to be a focus.\n",
        "\n",
        "4. **Self-supervised Learning:** This technique, which involves learning representations from the data itself without explicit labels, is expected to play a larger role, particularly in areas where labelled data is scarce.\n",
        "\n",
        "5. **Integration with Other AI Technologies:** Deep learning is expected to be integrated with other AI technologies like reinforcement learning, evolutionary algorithms, and swarm intelligence to create hybrid models with superior performance.\n",
        "\n",
        "6. **Application in Edge Devices:** As edge devices like smartphones and IoT devices become more powerful, we'll see more and more deep learning being deployed on these devices, leading to on-device AI capabilities.\n",
        "\n",
        "7. **Growth of Transformer Architectures:** Transformers, which have taken the NLP field by storm, are now also being used in other areas such as computer vision and reinforcement learning, a trend that is likely to continue.\n",
        "\n",
        "8. **AI Ethics and Regulation:** As deep learning applications become more widespread, issues related to ethics (like bias in AI models) and regulation of AI will come to the forefront.\n",
        "\n",
        "9. **Automated Machine Learning (AutoML):** This involves automating parts of the machine learning process, including hyperparameter tuning and feature selection. This trend could make deep learning more accessible to non-experts.\n",
        "\n",
        "10. **Privacy-Preserving Machine Learning:** Techniques such as differential privacy and federated learning can be used to train models without accessing raw data, which is crucial for handling sensitive data. This trend is expected to grow in the future.\n",
        "\n"
      ],
      "metadata": {
        "id": "EFmF1jUwKuUt"
      }
    }
  ]
}