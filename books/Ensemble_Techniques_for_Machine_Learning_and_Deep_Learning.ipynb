{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG5xfqCO7/igzzsBZ6u+Ms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/books/Ensemble_Techniques_for_Machine_Learning_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Techniques for Machine Learning and Deep Learning"
      ],
      "metadata": {
        "id": "kd1NMBJhDXE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "-Yj9ZO9ODYsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are powerful methods in machine learning and deep learning that aim to combine multiple models to improve predictive performance and robustness. By leveraging the strengths of diverse models, ensemble techniques can often outperform individual models and provide more reliable results. This course will cover various ensemble methods, both for traditional machine learning algorithms and deep learning models.\n"
      ],
      "metadata": {
        "id": "d_HB1Q6BDd8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 1: Introduction to Ensemble Techniques\n"
      ],
      "metadata": {
        "id": "MrnT7kbFDjFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 What are Ensemble Techniques?\n",
        "\n",
        "1.2 Advantages of Ensemble Methods\n",
        "\n",
        "1.3 Types of Ensemble Techniques\n"
      ],
      "metadata": {
        "id": "v-FNcI5KDnlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 2: Bagging Techniques\n"
      ],
      "metadata": {
        "id": "KtoZyZJrDyED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Bagging and Bootstrap Aggregating (Random Forest)\n",
        "\n",
        "2.2 Random Forest Algorithm\n",
        "\n",
        "2.3 Implementation of Random Forest in Scikit-learn\n",
        "\n",
        "2.4 Fine-tuning Random Forest Hyperparameters\n"
      ],
      "metadata": {
        "id": "i3wjWJ-VD14v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3: Boosting Techniques\n"
      ],
      "metadata": {
        "id": "DQb7N8_iD66v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Introduction to Boosting\n",
        "\n",
        "3.2 AdaBoost (Adaptive Boosting)\n",
        "\n",
        "3.3 Gradient Boosting Machines (GBM)\n",
        "\n",
        "3.4 XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "3.5 LightGBM and CatBoost\n"
      ],
      "metadata": {
        "id": "UI3hHGi7D-Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 4: Stacking and Blending\n"
      ],
      "metadata": {
        "id": "NjNEb62hECpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Stacking Ensemble Technique\n",
        "\n",
        "4.2 Blending Ensemble Technique\n",
        "\n",
        "4.3 Comparison between Stacking and Blending\n"
      ],
      "metadata": {
        "id": "FwVjCwO3F3TX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 5: Ensemble Techniques for Deep Learning\n"
      ],
      "metadata": {
        "id": "ywQb0abyF8y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1 Ensemble Methods for Neural Networks\n",
        "\n",
        "5.2 Bagging Neural Networks\n",
        "\n",
        "5.3 Boosting Neural Networks\n",
        "\n",
        "5.4 Stacking and Blending Neural Networks\n"
      ],
      "metadata": {
        "id": "sJ6o9sZYGBMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 6: Ensembling Convolutional Neural Networks (CNNs)\n"
      ],
      "metadata": {
        "id": "pCnoyaBRGF1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 Ensembling CNNs with Bagging\n",
        "\n",
        "6.2 Ensembling CNNs with Boosting\n",
        "\n",
        "6.3 Combining CNNs with Stacking and Blending\n"
      ],
      "metadata": {
        "id": "-e5wkkrAGHNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 7: Ensembling Recurrent Neural Networks (RNNs)\n"
      ],
      "metadata": {
        "id": "MXQfld4iGNPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1 Ensembling RNNs with Bagging\n",
        "\n",
        "7.2 Ensembling RNNs with Boosting\n",
        "\n",
        "7.3 Combining RNNs with Stacking and Blending\n"
      ],
      "metadata": {
        "id": "hs2Bi4x7GQp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 8: Ensemble Techniques for Transfer Learning\n"
      ],
      "metadata": {
        "id": "kjNDVtbIGURV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.1 Transfer Learning Basics\n",
        "\n",
        "8.2 Using Ensemble Techniques with Transfer Learning\n",
        "\n",
        "8.3 Fine-tuning Ensemble Models for Specific Tasks\n"
      ],
      "metadata": {
        "id": "HMX54tQUGXcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 9: Evaluation and Interpretation of Ensemble Models\n"
      ],
      "metadata": {
        "id": "6vwMw-bbGcbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.1 Performance Metrics for Ensemble Models\n",
        "\n",
        "9.2 Model Interpretability in Ensembles\n",
        "\n",
        "9.3 Interpreting Ensemble Predictions\n"
      ],
      "metadata": {
        "id": "UeuajpTQGe6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 10: Handling Imbalanced Data with Ensembles\n"
      ],
      "metadata": {
        "id": "n5qWw0RXGiph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.1 Imbalanced Data Problem Overview\n",
        "\n",
        "10.2 Using Ensembles to Handle Imbalanced Data\n",
        "\n",
        "10.3 Performance Metrics for Imbalanced Data\n"
      ],
      "metadata": {
        "id": "GDNwMW7aGjwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: Introduction to Ensemble Techniques\n"
      ],
      "metadata": {
        "id": "y50BbDPEGpi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 What are Ensemble Techniques?\n"
      ],
      "metadata": {
        "id": "AM1RmrpDEHaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are a class of machine learning methods that aim to improve the predictive performance and generalization capabilities of models by combining multiple individual models. The basic idea behind ensemble techniques is to leverage the wisdom of the crowd: by aggregating the predictions of several diverse models, the ensemble can often produce more accurate and robust predictions than any single model alone.\n",
        "\n",
        "There are several types of ensemble techniques, the most popular ones being Bagging, Boosting, and Stacking. Bagging, short for Bootstrap Aggregating, involves training multiple instances of the same base model on different subsets of the training data. These subsets are created by sampling with replacement, and the final prediction is usually obtained by averaging (in regression) or voting (in classification) over the predictions of each individual model.\n",
        "\n",
        "Boosting, on the other hand, is a sequential ensemble technique that focuses on iteratively improving the performance of weak learners. It trains a series of weak models, where each subsequent model gives more weight to the misclassified instances of the previous model. The final prediction is a weighted combination of the weak learners' outputs, with more weight given to those that perform better.\n",
        "\n",
        "Stacking, short for Stacked Generalization, is a more advanced ensemble technique that combines multiple base models with a meta-model (also called a blender or aggregator). Instead of simple averaging or voting, stacking involves using the predictions of the base models as input features for the meta-model. The meta-model then learns to make the final prediction based on these predictions from the base models, essentially learning from their collective strengths and weaknesses.\n",
        "\n",
        "Ensemble techniques are widely used in various machine learning applications, including classification, regression, and even in more complex tasks like anomaly detection and recommendation systems. By combining the abilities of different models, ensemble techniques can enhance predictive accuracy, reduce overfitting, and increase the model's ability to generalize to unseen data. However, building and training ensembles can be computationally expensive and require careful consideration of model diversity and complexity to achieve optimal performance. Nonetheless, they remain a valuable tool in the machine learning toolkit and continue to be actively researched and applied in the field.\n"
      ],
      "metadata": {
        "id": "fpIkU3gnEtrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Advantages of Ensemble Methods\n"
      ],
      "metadata": {
        "id": "s3Vpoz8oExEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods are powerful techniques in machine learning that involve combining multiple models to make more accurate predictions. The idea behind ensemble methods is that by combining the predictions of multiple models, their individual weaknesses can be mitigated, leading to improved overall performance. Here are some key advantages of ensemble methods:\n",
        "\n",
        "1. Improved Accuracy: One of the primary advantages of ensemble methods is their ability to enhance prediction accuracy. By combining the predictions of diverse models, the ensemble can capture a wider range of patterns and relationships in the data. This often results in more robust and accurate predictions, as the errors of individual models tend to cancel out when aggregated.\n",
        "\n",
        "2. Robustness and Stability: Ensemble methods are inherently more stable and less prone to overfitting compared to single models. When using diverse base models, the ensemble can generalize better to unseen data. This robustness is especially beneficial when dealing with noisy or incomplete datasets.\n",
        "\n",
        "3. Reduced Bias: Different machine learning models often exhibit different biases, which can lead to erroneous predictions in certain regions of the input space. Ensemble methods help to alleviate this issue by averaging out the biases of individual models, leading to more balanced and unbiased predictions.\n",
        "\n",
        "4. Flexibility with Model Types: Ensemble methods can be applied to a wide variety of model types, ranging from decision trees and random forests to neural networks and support vector machines. This flexibility allows practitioners to combine the strengths of different algorithms, tailoring the ensemble to suit the specific characteristics of the problem at hand.\n",
        "\n",
        "5. Interpretability: While individual complex models might be difficult to interpret, ensemble methods can provide more interpretable results. For instance, in the case of a voting ensemble, the majority vote among models' predictions can offer clear insights into the final decision-making process.\n",
        "\n",
        "6. Handling Imbalanced Data: Imbalanced datasets, where one class is underrepresented, can pose challenges to traditional machine learning models. Ensemble methods can help address this issue by giving more weight to the underrepresented class, thus improving the model's ability to recognize minority patterns.\n",
        "\n",
        "7. Easy Implementation: Implementing ensemble methods is relatively straightforward, especially when using popular libraries and frameworks that provide pre-built ensemble techniques. This ease of implementation makes ensemble methods accessible to a wide range of users, from beginners to experienced practitioners.\n",
        "\n",
        "In conclusion, ensemble methods offer numerous advantages that make them a valuable tool in the machine learning arsenal. Their ability to improve accuracy, reduce bias, handle complex data distributions, and provide more interpretable results make them a popular choice for various real-world applications. By harnessing the power of multiple models, ensemble methods consistently deliver enhanced predictive performance and contribute significantly to the advancement of machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "qZrUkQqNE2MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Types of Ensemble Techniques\n"
      ],
      "metadata": {
        "id": "YYmhKwMPE5jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are a set of machine learning methods that combine multiple models to improve prediction accuracy and robustness compared to using a single model. These methods work on the principle that a diverse set of models, when combined, can overcome individual model weaknesses and yield better overall performance. Here are some common types of ensemble techniques:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**:\n",
        "Bagging involves creating multiple copies of the same base model, training each copy on a randomly sampled subset of the training data with replacement. These subsets are known as bootstrap samples. The final prediction is then obtained by averaging (for regression) or voting (for classification) the predictions of individual models. The most well-known algorithm based on bagging is Random Forest.\n",
        "\n",
        "2. **Boosting**:\n",
        "Boosting is an iterative ensemble technique that sequentially builds a strong model by focusing on the examples that previous models have misclassified. In each iteration, the algorithm gives higher weight to the misclassified examples, forcing subsequent models to concentrate on these hard-to-predict cases. Boosting reduces both bias and variance and improves the overall model performance. Common boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), and XGBoost (Extreme Gradient Boosting).\n",
        "\n",
        "3. **Stacking (Stacked Generalization)**:\n",
        "Stacking involves training multiple diverse base models and then using a meta-model to combine their predictions. The base models make predictions on the same data, and these predictions become the input features for the meta-model. Stacking allows each base model to focus on different aspects of the data and can lead to more accurate and robust predictions. The meta-model can be a simple linear model or another machine learning model.\n",
        "\n",
        "4. **Voting**:\n",
        "Voting is a simple ensemble technique where multiple models make predictions on the same data, and the final prediction is determined by majority voting (for classification) or averaging (for regression). It works well when the base models have similar performance and are diverse enough to capture different patterns in the data.\n",
        "\n",
        "5. **Weighted Average**:\n",
        "In this method, each model is assigned a weight, and their predictions are combined by taking a weighted average. Models with better performance are given higher weights. This technique is often used when some models are more reliable or accurate than others.\n",
        "\n",
        "6. **Bayesian Model Averaging**:\n",
        "Bayesian model averaging is a probabilistic ensemble technique that considers the uncertainty associated with each model. Instead of just averaging predictions, it computes a weighted average based on the model's posterior probabilities. This approach is particularly useful when dealing with small datasets or when the models have different levels of complexity.\n",
        "\n",
        "7. **Bootstrapped Ensembles**:\n",
        "In this approach, ensembles are built using different bootstrapped samples of the training data, resulting in diverse subsets for training each model. Then, the final predictions are obtained by combining the predictions of all individual models. This method aims to increase diversity and improve ensemble performance.\n",
        "\n",
        "These are some of the commonly used ensemble techniques. The choice of ensemble method depends on the problem domain, data characteristics, and the algorithms used as base models. By employing ensemble techniques, you can often achieve more accurate and stable predictions in various machine learning tasks.\n"
      ],
      "metadata": {
        "id": "zDi4gnEbE-Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2: Bagging Techniques"
      ],
      "metadata": {
        "id": "gnla0sD7FECq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Random Forest Algorithm\n"
      ],
      "metadata": {
        "id": "77K05iUsFO04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest algorithm is an ensemble learning method that combines multiple decision trees to create a more accurate and robust predictive model. It is widely used for both classification and regression tasks. The basic idea behind Random Forest is to build multiple decision trees and then combine their predictions through voting (for classification) or averaging (for regression).\n",
        "\n",
        "The Pima Indians Diabetes dataset contains information about female patients of Pima Indian heritage. The goal is to predict whether a patient has diabetes based on various features such as glucose concentration, blood pressure, BMI, etc.\n",
        "\n",
        "Let's go through an example of implementing the Random Forest algorithm using the Pima Indians Diabetes dataset in Google Colab.\n",
        "\n",
        "Step 1: Import necessary libraries and load the dataset."
      ],
      "metadata": {
        "id": "jkUEyn-BFXlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset from the provided URL.\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Assign column names to the dataset.\n",
        "data.columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "\n",
        "# Separate features and target variable.\n",
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tGbKY32MFbzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create and train the Random Forest classifier.\n"
      ],
      "metadata": {
        "id": "G4prMH_wFk5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Random Forest classifier with 100 trees.\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier.\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "koTDsaUNFl_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Make predictions on the test set and evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "DPiR-Tk6Fqqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set.\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report for more detailed performance metrics.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix.\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BkWgmyPIF4rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Random Forest classifier is now trained and evaluated on the Pima Indians Diabetes dataset. The accuracy, classification report, and confusion matrix will provide a comprehensive evaluation of the model's performance in predicting diabetes outcomes for the patients.\n",
        "\n",
        "Note: In a real-world scenario, it's essential to perform hyperparameter tuning, cross-validation, and other performance improvement techniques to obtain the best possible model. However, this example serves as a basic demonstration of implementing Random Forest in Google Colab with the Pima Indians Diabetes dataset.\n"
      ],
      "metadata": {
        "id": "S08n57iqFP5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Fine-tuning Random Forest Hyperparameters\n"
      ],
      "metadata": {
        "id": "0iKVvdoDGFSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning hyperparameters in Random Forest involves finding the best combination of hyperparameters that optimize the model's performance. We can use techniques like grid search or random search to explore different hyperparameter values and choose the ones that yield the best results.\n",
        "\n",
        "For this example, we will use the Pima Indians Diabetes dataset, available from the URL: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv. The dataset contains information about female patients, and the goal is to predict whether a patient has diabetes or not based on various features.\n",
        "\n",
        "Here's how to fine-tune Random Forest hyperparameters using the Pima Indian Diabetes dataset in Colab:\n",
        "\n",
        "Step 1: Import necessary libraries and load the dataset."
      ],
      "metadata": {
        "id": "JbEnCraOGQ9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset.\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "data = pd.read_csv(url, header=None)"
      ],
      "metadata": {
        "id": "nPRDhGNnGXFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Prepare the data.\n"
      ],
      "metadata": {
        "id": "ETqrVVBkGcGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the last column is the target variable and the rest are features.\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bcK57PwSGdeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Fine-tune Random Forest hyperparameters using GridSearchCV.\n"
      ],
      "metadata": {
        "id": "v9g0kaE1Gi0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid to search.\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],      # Number of trees in the forest.\n",
        "    'max_depth': [None, 5, 10, 20],      # Maximum depth of the tree.\n",
        "    'min_samples_split': [2, 5, 10],     # Minimum number of samples required to split an internal node.\n",
        "    'min_samples_leaf': [1, 2, 4],       # Minimum number of samples required to be at a leaf node.\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier.\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create the GridSearchCV object.\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Perform the grid search to find the best hyperparameters.\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DxGQnBw5Gkpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate the model with the best hyperparameters on the test set.\n"
      ],
      "metadata": {
        "id": "m3zgx8FKGuUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best Random Forest model from the grid search.\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model.\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Random Forest Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "OHMNhPBgGyti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we used GridSearchCV from scikit-learn to perform an exhaustive search over a specified hyperparameter grid. The `GridSearchCV` object performs cross-validation and evaluates the model's performance using the specified hyperparameter combinations. The `best_estimator_` attribute of the `GridSearchCV` object gives us the best model found during the search, which we can then use to make predictions and evaluate its accuracy on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "UVCgBp0cGN0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3: Boosting Techniques"
      ],
      "metadata": {
        "id": "PgBc6o2PVHwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Introduction to Boosting"
      ],
      "metadata": {
        "id": "olxv1P9qVPsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is a powerful ensemble learning technique used in machine learning to improve the performance of weak learners or base models. The idea behind boosting is to combine multiple weak learners, such as simple decision trees or classifiers, to create a strong learner that exhibits better generalization and predictive accuracy.\n",
        "\n",
        "The boosting process works iteratively, and at each step, it focuses on the misclassified instances from the previous iteration. The main goal is to give more weight to those instances that were incorrectly predicted by the current ensemble, allowing subsequent weak learners to pay greater attention to these challenging cases.\n",
        "\n",
        "The general procedure for boosting can be summarized as follows:\n",
        "\n",
        "1. **Initialize weights**: Assign equal weights to all training instances. These weights determine the importance of each instance during the learning process.\n",
        "\n",
        "2. **Train a weak learner**: Apply a weak learner (e.g., decision tree, shallow neural network, etc.) on the training data, considering the instance weights assigned in the previous step.\n",
        "\n",
        "3. **Evaluate performance**: Assess the performance of the weak learner on the training set. The performance measure is typically used to adjust the weights of the misclassified instances.\n",
        "\n",
        "4. **Update instance weights**: Increase the weights of the misclassified instances, making them more influential in the next round of training.\n",
        "\n",
        "5. **Combine weak learners**: Repeat the process by training another weak learner on the updated data (with adjusted instance weights). Continue this process for a predefined number of iterations or until a specific stopping criterion is met.\n",
        "\n",
        "6. **Combine weak learners into a strong learner**: Combine the predictions of all weak learners with appropriate weights to create a powerful ensemble model.\n",
        "\n",
        "Boosting algorithms, such as AdaBoost (Adaptive Boosting) and Gradient Boosting Machines (GBM), are among the most popular implementations of the boosting technique. They differ in the way they assign weights, how they combine weak learners, and the specific loss functions they optimize.\n",
        "\n",
        "The key benefits of boosting include improved model accuracy, better generalization, and the ability to handle complex datasets effectively. However, boosting may be sensitive to noise and outliers, and it can also be computationally intensive due to the iterative nature of the process.\n",
        "\n",
        "Overall, boosting is a fundamental technique in the field of machine learning, widely used for various tasks, including classification, regression, and ranking problems.\n"
      ],
      "metadata": {
        "id": "bD3otMH_Vazm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 AdaBoost (Adaptive Boosting)"
      ],
      "metadata": {
        "id": "zHFgYOWIVd99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost (Adaptive Boosting) is an ensemble learning technique that aims to improve the performance of weak classifiers by combining them into a strong classifier. It does this by giving more weight to misclassified examples in each iteration, allowing subsequent weak classifiers to focus on the difficult-to-classify instances. The final prediction is obtained by taking a weighted majority vote of all weak classifiers. AdaBoost is particularly effective in handling imbalanced datasets and can be used for both classification and regression tasks.\n",
        "\n",
        "Now, let's demonstrate AdaBoost using the Pima Indian Diabetes dataset. This dataset contains features of female patients, and the task is to predict whether a patient has diabetes or not."
      ],
      "metadata": {
        "id": "WCEnh5KIVl8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load Pima Indian Diabetes dataset from the provided URL.\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
        "           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Separate features (X) and target (y).\n",
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree as the base classifier for AdaBoost.\n",
        "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Create an AdaBoost classifier with 50 weak learners (decision trees).\n",
        "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the AdaBoost classifier.\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set.\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report and confusion matrix.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "m0OfVAriVsP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we first load the Pima Indian Diabetes dataset from the provided URL and split it into training and testing sets. We then create a Decision Tree classifier (base classifier) with a shallow depth of 1. This weak classifier is used as the base estimator for AdaBoost. We then create an AdaBoost classifier with 50 weak learners (decision trees). Next, we train the AdaBoost classifier on the training data and make predictions on the test set. Finally, we calculate the accuracy of the model and print the classification report and confusion matrix for further evaluation.\n",
        "\n",
        "Please note that the provided dataset may contain missing values, and you may want to perform data preprocessing, feature scaling, and handle missing values appropriately before using it in a machine learning model.\n"
      ],
      "metadata": {
        "id": "iVSP_N5AViDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Gradient Boosting Machines (GBM)\n"
      ],
      "metadata": {
        "id": "79nv2fP9V8Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Machines (GBM) is a popular ensemble learning technique that builds multiple weak learners (typically decision trees) sequentially to correct the errors made by previous models. It combines the predictions of these weak learners to produce a final strong prediction with improved accuracy.\n",
        "\n",
        "Let's go through an example of using Gradient Boosting Machines with the Pima Indian Diabetes dataset. The dataset contains several features related to medical information of Pima Indian women, and the task is to predict whether a woman has diabetes (1) or not (0).\n",
        "\n",
        "1. Load the dataset and import the required libraries:"
      ],
      "metadata": {
        "id": "CpWQtpNRWFbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)"
      ],
      "metadata": {
        "id": "ZSzF38GCWGjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Prepare the data:\n"
      ],
      "metadata": {
        "id": "-tRtkgPUWN4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and the target variable (y)\n",
        "X = data.drop('class', axis=1)\n",
        "y = data['class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tFsY-TxnWSJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Build and train the Gradient Boosting Classifier:\n"
      ],
      "metadata": {
        "id": "NHrNUQPpWYzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradient Boosting Classifier\n",
        "gbm_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "gbm_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3y0g-espWZlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Make predictions and evaluate the model:\n"
      ],
      "metadata": {
        "id": "XgN8UWGVWhyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = gbm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "sPrrdFUbWmyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we used the Pima Indian Diabetes dataset to train a Gradient Boosting Classifier. We split the data into training and testing sets, built the classifier using 100 estimators (weak learners), trained the model on the training data, and evaluated its performance using accuracy, confusion matrix, and classification report.\n",
        "\n",
        "GBM is a powerful ensemble technique that can provide impressive results on various datasets. However, it's essential to tune hyperparameters and perform cross-validation for optimal performance and to prevent overfitting.\n"
      ],
      "metadata": {
        "id": "c5nu08M4WBCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 XGBoost (Extreme Gradient Boosting)\n"
      ],
      "metadata": {
        "id": "MqgssH_pWsQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is a powerful and efficient machine learning algorithm based on gradient boosting. It is designed to handle complex datasets and is particularly popular for structured/tabular data. XGBoost is an ensemble technique that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model.\n",
        "\n",
        "To demonstrate XGBoost with the Pima Indian Diabetes dataset, we'll follow these steps in a Google Colab environment:\n",
        "\n",
        "1. Import the required libraries.\n",
        "2. Load and preprocess the Pima Indian Diabetes dataset.\n",
        "3. Split the dataset into training and testing sets.\n",
        "4. Create and train the XGBoost classifier.\n",
        "5. Evaluate the model's performance.\n",
        "\n",
        "Let's go through the implementation:"
      ],
      "metadata": {
        "id": "fBQAWcuxW1Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 2: Load and preprocess the Pima Indian Diabetes dataset.\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
        "                'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Replace 0 values in numeric columns with NaN.\n",
        "data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = \\\n",
        "    data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)\n",
        "\n",
        "# Drop rows with missing values (NaN).\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y).\n",
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create and train the XGBoost classifier.\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',  # For binary classification problems.\n",
        "    eval_metric='logloss',        # Logarithmic loss for binary classification.\n",
        "    use_label_encoder=False      # To suppress deprecation warning.\n",
        ")\n",
        "\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model's performance.\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report and confusion matrix.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XUNCzn00W3P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this example, we use the Pima Indian Diabetes dataset, which contains various features related to diabetes and an outcome column indicating whether a person has diabetes (1) or not (0). We preprocess the data by replacing 0 values with NaN and dropping rows with missing values. Then, we split the dataset into training and testing sets.\n",
        "\n",
        "We create an XGBoost classifier with the `xgb.XGBClassifier` class, specifying the appropriate objective and evaluation metric for binary classification. We set `use_label_encoder=False` to suppress a deprecation warning. We then train the XGBoost classifier on the training data.\n",
        "\n",
        "After training, we evaluate the model's performance on the test set using accuracy, classification report, and confusion matrix.\n",
        "\n",
        "Please note that XGBoost has several hyperparameters that can be tuned to improve performance. In practice, you may want to perform hyperparameter tuning using techniques like grid search or random search to find the best combination of hyperparameters for your specific problem.\n"
      ],
      "metadata": {
        "id": "JbllFnEkWx_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 LightGBM and CatBoost\n"
      ],
      "metadata": {
        "id": "TK0fyTRCXEKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM and CatBoost are two popular gradient boosting frameworks known for their efficiency and performance in handling large datasets and achieving high accuracy. In this explanation, we'll use the Pima Indian Diabetes dataset and demonstrate how to use LightGBM and CatBoost in a Colab environment.\n",
        "\n",
        "**Pima Indian Diabetes Dataset:**\n",
        "The Pima Indian Diabetes dataset contains diagnostic measurements of Pima Indian women. The objective is to predict whether a woman has diabetes (1) or not (0) based on features such as glucose concentration, blood pressure, and others. The dataset can be found in the following URL:\n",
        "[https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
        "\n",
        "**LightGBM:**\n",
        "LightGBM is a fast and efficient gradient boosting framework that uses a histogram-based algorithm to speed up training and reduce memory usage. It is known for its speed and ability to handle large datasets. LightGBM splits data by histograms and uses a leaf-wise approach to grow trees, making it faster than traditional depth-wise approaches.\n",
        "\n",
        "**CatBoost:**\n",
        "CatBoost is another gradient boosting library that excels in handling categorical features and provides excellent out-of-the-box performance. It automatically handles categorical variables during the training process and requires minimal data preprocessing compared to other boosting frameworks.\n",
        "\n",
        "Below is an example code using the Pima Indian Diabetes dataset to demonstrate LightGBM and CatBoost in a Colab environment:"
      ],
      "metadata": {
        "id": "XHBz1B-5Qij7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LightGBM and CatBoost libraries (if not already installed)\n",
        "!pip install lightgbm\n",
        "!pip install catboost\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM Model\n",
        "lgb_classifier = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
        "lgb_classifier.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_classifier.predict(X_test)\n",
        "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
        "print(\"LightGBM Accuracy:\", accuracy_lgb)\n",
        "\n",
        "# CatBoost Model\n",
        "cat_classifier = CatBoostClassifier(iterations=100, random_state=42, verbose=False)\n",
        "cat_classifier.fit(X_train, y_train)\n",
        "y_pred_cat = cat_classifier.predict(X_test)\n",
        "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
        "print(\"CatBoost Accuracy:\", accuracy_cat)"
      ],
      "metadata": {
        "id": "7cpKvyqwQnri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we first install the LightGBM and CatBoost libraries if they are not already installed in the Colab environment. We then load the Pima Indian Diabetes dataset, split it into features and the target variable. Next, we split the data into training and testing sets.\n",
        "\n",
        "We then create a LightGBM model and a CatBoost model. We use the training data to train each model and evaluate their performance using the testing data. The accuracy of each model is printed as the final result.\n",
        "\n",
        "Both LightGBM and CatBoost are powerful gradient boosting frameworks, and the choice between them often depends on the specific dataset and requirements. Experimenting with different models and tuning hyperparameters can further improve the results.\n"
      ],
      "metadata": {
        "id": "WVLfYB5DQb7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Stacking and Blending\n"
      ],
      "metadata": {
        "id": "8fpqcYwEQ_dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Stacking Ensemble Technique\n"
      ],
      "metadata": {
        "id": "QEhm_9ooRFyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking is a meta-ensemble technique that combines the predictions of multiple base models by training a new model (meta-model) on their individual predictions. The base models are typically different algorithms or variations of the same algorithm trained on different subsets of the data. The meta-model takes the predictions of the base models as input features and learns to make the final prediction.\n",
        "\n",
        "The process of stacking can be summarized as follows:\n",
        "1. Split the dataset into multiple subsets (folds).\n",
        "2. Train multiple base models on different subsets of the data.\n",
        "3. Make predictions on the validation set for each base model.\n",
        "4. Combine the predictions of the base models and use them as new features.\n",
        "5. Train a meta-model (e.g., logistic regression, random forest) on the combined predictions.\n",
        "6. Use the meta-model to make the final prediction on new data.\n",
        "\n",
        "Example: Stacking with Pima Indian Diabetes Dataset\n",
        "\n",
        "In this example, we will use the Pima Indian Diabetes dataset to demonstrate the stacking ensemble technique. The dataset contains various features related to Pima Indian women, and the goal is to predict whether a woman has diabetes or not (binary classification).\n",
        "\n",
        "Step 1: Import the required libraries and load the dataset."
      ],
      "metadata": {
        "id": "PZY7-L1oRQ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset.\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "data = pd.read_csv(url, header=None)"
      ],
      "metadata": {
        "id": "dkccqMWJRhbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the data and split it into training and testing sets.\n"
      ],
      "metadata": {
        "id": "EjFn6z-pRkWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume the last column (index 8) is the target variable (diabetes).\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "06cykTNtRrp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the base models and train them on different subsets of the data.\n"
      ],
      "metadata": {
        "id": "kaa5J9JKRtEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base models.\n",
        "base_model1 = LogisticRegression(random_state=42)\n",
        "base_model2 = RandomForestClassifier(random_state=42)\n",
        "base_model3 = KNeighborsClassifier()\n",
        "\n",
        "# Train the base models.\n",
        "base_model1.fit(X_train, y_train)\n",
        "base_model2.fit(X_train, y_train)\n",
        "base_model3.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "FLdWOBiSRxSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Make predictions on the validation set for each base model.\n"
      ],
      "metadata": {
        "id": "QehFFxDXR1kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set for each base model.\n",
        "pred_base_model1 = base_model1.predict(X_test)\n",
        "pred_base_model2 = base_model2.predict(X_test)\n",
        "pred_base_model3 = base_model3.predict(X_test)"
      ],
      "metadata": {
        "id": "otcQQpEqR4IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Combine the predictions of the base models and use them as new features.\n"
      ],
      "metadata": {
        "id": "PnQ5rqcAR80_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new feature matrix by stacking the predictions of the base models.\n",
        "stacked_features = np.column_stack((pred_base_model1, pred_base_model2, pred_base_model3))"
      ],
      "metadata": {
        "id": "ghvm77FtR_26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Train the meta-model (logistic regression) on the combined predictions.\n"
      ],
      "metadata": {
        "id": "-96RI6YpSEFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the meta-model (logistic regression) on the stacked features.\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "meta_model.fit(stacked_features, y_test)"
      ],
      "metadata": {
        "id": "B9lNCpNuSGic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Use the meta-model to make the final prediction on new data.\n"
      ],
      "metadata": {
        "id": "zSQVNqGUSJuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the meta-model on new data.\n",
        "pred_meta_model = meta_model.predict(stacked_features)\n",
        "\n",
        "# Calculate the accuracy of the stacked model.\n",
        "accuracy = accuracy_score(y_test, pred_meta_model)\n",
        "print(\"Stacking Ensemble Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "tuAbwkroSMQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this example, we used three different base models (Logistic Regression, Random Forest, and K-Nearest Neighbors) to make predictions on the validation set. We then stacked these predictions together and trained a meta-model (Logistic Regression) on the combined predictions. The final accuracy of the stacking ensemble model is printed as the result.\n",
        "\n",
        "Please note that this example is for demonstration purposes, and in practice, you would typically use more diverse and well-tuned models as base models and fine-tune the meta-model for better performance. Additionally, cross-validation should be used for more robust evaluation.\n"
      ],
      "metadata": {
        "id": "HjAe2RqCRMhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Blending Ensemble Technique\n"
      ],
      "metadata": {
        "id": "uIOU54m1S0KE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blending is a meta-ensemble technique that combines the predictions of multiple base models using a higher-level model, often referred to as the \"meta-model.\" Unlike stacking, where the predictions of base models are used as additional features for the meta-model, blending involves training the base models on the original training data and then using their predictions on a separate validation set to train the meta-model.\n",
        "\n",
        "The general steps involved in the blending ensemble technique are as follows:\n",
        "\n",
        "1. Split the original training data into two parts: a training set and a validation set.\n",
        "2. Train multiple base models on the training set.\n",
        "3. Generate predictions using these base models on the validation set (also called \"out-of-fold\" predictions).\n",
        "4. Use the out-of-fold predictions as features for the meta-model and train the meta-model on the true target values from the validation set.\n",
        "5. Optionally, repeat steps 1 to 4 for cross-validation to get more robust blending results.\n",
        "\n",
        "Now, let's demonstrate the blending ensemble technique using the Pima Indian Diabetes dataset. We'll use three base models: Logistic Regression, Random Forest, and XGBoost, and then use a simple logistic regression as the meta-model.\n",
        "\n",
        "Note: Before running the example in Colab, ensure that you have the Pima Indian Diabetes dataset available. You can download it and upload it to Colab or use the provided URL directly in the code.\n",
        "\n",
        "Example: Blending Ensemble Technique with Pima Indian Diabetes Dataset"
      ],
      "metadata": {
        "id": "QaoSEWysTHBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset (replace 'path_to_pima_dataset.csv' with the actual path).\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "column_names = ['pregnancies', 'glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'diabetes_pedigree', 'age', 'class']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Assume that 'class' is the target variable.\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 1: Split the training data into two parts (training set and validation set).\n",
        "X_train_base, X_valid, y_train_base, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train multiple base models.\n",
        "# Base Model 1: Logistic Regression\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Base Model 2: Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Base Model 3: XGBoost\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Step 3: Generate out-of-fold predictions for the meta-model.\n",
        "lr_preds_valid = lr_model.predict_proba(X_valid)[:, 1]\n",
        "rf_preds_valid = rf_model.predict_proba(X_valid)[:, 1]\n",
        "xgb_preds_valid = xgb_model.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "# Step 4: Use the out-of-fold predictions as features for the meta-model (simple logistic regression).\n",
        "meta_X = np.column_stack((lr_preds_valid, rf_preds_valid, xgb_preds_valid))\n",
        "\n",
        "# Train the meta-model (logistic regression) on the out-of-fold predictions.\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "meta_model.fit(meta_X, y_valid)\n",
        "\n",
        "# Step 5: Evaluate the blending ensemble model on the test set.\n",
        "lr_preds_test = lr_model.predict_proba(X_test)[:, 1]\n",
        "rf_preds_test = rf_model.predict_proba(X_test)[:, 1]\n",
        "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
        "meta_X_test = np.column_stack((lr_preds_test, rf_preds_test, xgb_preds_test))\n",
        "\n",
        "blending_preds = meta_model.predict(meta_X_test)\n",
        "\n",
        "# Calculate the accuracy of the blending ensemble model.\n",
        "accuracy = accuracy_score(y_test, blending_preds)\n",
        "print(\"Blending Ensemble Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "hI0UjV58TLRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this example, we used logistic regression, random forest, and XGBoost as our base models and then combined them using a simple logistic regression as the meta-model for blending. The blending ensemble technique can be further refined by using more sophisticated meta-models or by performing cross-validation for model selection and hyperparameter tuning.\n"
      ],
      "metadata": {
        "id": "G7YOR65OTEFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Comparison between Stacking and Blending\n"
      ],
      "metadata": {
        "id": "PWocqiUOTWET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking and blending are both techniques used in ensemble learning, where multiple machine learning models are combined to improve predictive performance. However, they have some differences in their approach and implementation.\n",
        "\n",
        "1. Definition:\n",
        "   - Stacking: Stacking, also known as stacked generalization, is a model ensemble method that involves training multiple base models and then using a meta-model (also called a blender) to combine their predictions.\n",
        "   - Blending: Blending is a simplified version of stacking where the predictions of multiple base models are combined directly using a simple averaging or weighted averaging approach, without using a meta-model.\n",
        "\n",
        "2. Architecture:\n",
        "   - Stacking: Stacking typically consists of two or more layers. In the first layer, multiple base models are trained on the same dataset. In the second layer, a meta-model is trained on the predictions of the base models from the first layer.\n",
        "   - Blending: Blending has a single layer, where the predictions of multiple base models are combined directly to form the ensemble prediction.\n",
        "\n",
        "3. Training process:\n",
        "   - Stacking: In stacking, the base models are trained on the training dataset, and then their predictions are collected and used as input features for the meta-model. The meta-model is trained on the training dataset with the true labels to learn how to combine the base models' predictions effectively.\n",
        "   - Blending: In blending, the base models are trained on the training dataset, and their predictions are combined using a simple averaging or weighted averaging approach. No additional meta-model is trained in blending.\n",
        "\n",
        "4. Complexity:\n",
        "   - Stacking: Stacking is generally more complex than blending due to the addition of the meta-model, which requires more computational resources and hyperparameter tuning.\n",
        "   - Blending: Blending is simpler since it does not involve training an additional meta-model.\n",
        "\n",
        "5. Performance:\n",
        "   - Stacking: Stacking can potentially lead to better performance compared to blending because the meta-model learns to weigh the predictions of the base models effectively, exploiting their individual strengths.\n",
        "   - Blending: Blending can still be effective, especially when the base models are diverse and complementary, but it might not perform as well as stacking in certain scenarios.\n",
        "\n",
        "6. Overfitting:\n",
        "   - Stacking: Stacking could be more prone to overfitting, especially if not properly regularized, as it involves an additional layer of complexity.\n",
        "   - Blending: Blending is less prone to overfitting since it is a simpler approach.\n",
        "\n",
        "In summary, both stacking and blending are effective ensemble methods, and their performance depends on the specific problem, the diversity of base models, and the amount of data available for training. Stacking is generally more powerful and flexible, but it comes with added complexity and risk of overfitting. Blending is simpler but may not fully capture the potential benefits of model combinations that stacking provides.\n"
      ],
      "metadata": {
        "id": "__d-x3bMTzz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Ensemble Techniques for Deep Learning\n"
      ],
      "metadata": {
        "id": "SwJnGqiYUFFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Ensemble Methods for Neural Networks\n"
      ],
      "metadata": {
        "id": "ZJcOICW7UIUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods for neural networks involve combining predictions from multiple neural networks to improve overall model performance. These ensemble techniques can help mitigate issues like overfitting and improve generalization. In this example, we'll use the Pima Indian Diabetes dataset to demonstrate ensemble methods for neural networks. The dataset contains features related to health measurements of Pima Indian women, and the target variable indicates whether they have diabetes or not.\n",
        "\n",
        "Step 1: Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "69494ePnUTS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset (replace 'path_to_dataset.csv' with the actual path).\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "F0VzuykQUXw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create and Train Individual Neural Networks\n"
      ],
      "metadata": {
        "id": "Y2UomXqmUddL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to create a single neural network model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and train multiple neural networks\n",
        "num_models = 5\n",
        "models = []\n",
        "for _ in range(num_models):\n",
        "    model = create_model()\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "41rYB82QU1-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Make Predictions and Aggregate Results\n"
      ],
      "metadata": {
        "id": "sdifXB8LU6kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make predictions using all models and aggregate the results\n",
        "def ensemble_predictions(models, X):\n",
        "    y_preds = [model.predict(X) for model in models]\n",
        "    y_preds = np.array(y_preds)\n",
        "    y_preds_mean = np.mean(y_preds, axis=0)\n",
        "    y_preds_mean = (y_preds_mean > 0.5).astype(int)\n",
        "    return y_preds_mean\n",
        "\n",
        "# Make predictions using the ensemble of models on the test set\n",
        "y_pred_ensemble = ensemble_predictions(models, X_test)"
      ],
      "metadata": {
        "id": "yO6aGsDeU_Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate the Ensemble Model's Performance\n"
      ],
      "metadata": {
        "id": "NK-SaxIcVBlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_ensemble)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred_ensemble)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "myeCUXvLVE4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we used the Pima Indian Diabetes dataset to demonstrate ensemble methods for neural networks. We created multiple neural network models, trained them on the training data, and then used the ensemble of these models to make predictions on the test set. By aggregating the predictions of individual models, we obtain an ensemble model with improved performance compared to any single model. The final evaluation metrics, including accuracy, confusion matrix, and classification report, help assess the ensemble model's performance on the test data.\n"
      ],
      "metadata": {
        "id": "eYtAcqJPULyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Bagging Neural Networks\n"
      ],
      "metadata": {
        "id": "qJY9Er4yVabu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging, short for Bootstrap Aggregating, is an ensemble technique that involves training multiple models independently on different subsets of the training data and combining their predictions to make the final decision. In the context of neural networks, Bagging involves training multiple neural networks with different random subsets of the training data and averaging their predictions to improve the model's performance and reduce overfitting.\n",
        "\n",
        "In this example, we will demonstrate how to apply Bagging to train multiple neural networks on the Pima Indian Diabetes dataset. The dataset contains various features related to Pima Indian women, and the target variable indicates whether they have diabetes or not. The goal is to use Bagging to build a more robust and accurate neural network for diabetes prediction.\n",
        "\n",
        "Step 1: Import the required libraries and load the dataset."
      ],
      "metadata": {
        "id": "1iKaFP9GVoSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset.\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)"
      ],
      "metadata": {
        "id": "aOqkEL0IVpSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the data.\n"
      ],
      "metadata": {
        "id": "nnATYKrnVxSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and the target variable (y).\n",
        "X = data.drop('class', axis=1)\n",
        "y = data['class']\n",
        "\n",
        "# Normalize the features to have values between 0 and 1.\n",
        "X = (X - X.min()) / (X.max() - X.min())\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Y1bh0AcQVzjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the function to create the base neural network.\n"
      ],
      "metadata": {
        "id": "g9-vVA80V5B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tFzaf5F4V5Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train multiple neural networks with Bagging.\n"
      ],
      "metadata": {
        "id": "l8WfkQfEV_pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_models = 10\n",
        "bagged_models = []\n",
        "for i in range(num_models):\n",
        "    # Create a new base model for each iteration.\n",
        "    model = create_base_model()\n",
        "\n",
        "    # Randomly sample a subset of the training data with replacement (bootstrap).\n",
        "    indices = np.random.choice(X_train.shape[0], X_train.shape[0], replace=True)\n",
        "    X_bagged = X_train.iloc[indices]\n",
        "    y_bagged = y_train.iloc[indices]\n",
        "\n",
        "    # Train the model on the bootstrap sample.\n",
        "    model.fit(X_bagged, y_bagged, epochs=100, batch_size=16, verbose=0)\n",
        "\n",
        "    # Append the trained model to the list.\n",
        "    bagged_models.append(model)"
      ],
      "metadata": {
        "id": "0oN6mVRfWAhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Make predictions and combine results from all models.\n"
      ],
      "metadata": {
        "id": "Np4Uhs_kWKSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set using each model.\n",
        "predictions = np.zeros((num_models, X_test.shape[0]))\n",
        "for i, model in enumerate(bagged_models):\n",
        "    predictions[i] = model.predict(X_test).flatten()\n",
        "\n",
        "# Average the predictions across all models to get the final prediction.\n",
        "final_predictions = np.mean(predictions, axis=0)\n",
        "rounded_predictions = np.round(final_predictions)\n",
        "\n",
        "# Convert the probabilities to binary predictions (0 or 1).\n",
        "rounded_predictions = np.array(rounded_predictions, dtype=int)\n",
        "\n",
        "# Calculate the accuracy of the Bagging ensemble model.\n",
        "accuracy = accuracy_score(y_test, rounded_predictions)\n",
        "print(\"Bagging Neural Networks Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "BAYoliXPWTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we trained 10 neural networks using Bagging with bootstrap sampling. Each base neural network is trained on a different subset of the training data, and their predictions are averaged to make the final decision. By doing so, we create an ensemble model that is more robust and performs better on unseen data than an individual neural network.\n",
        "\n",
        "Note: Since neural networks are computationally expensive, the number of models and the number of epochs for training each model can be adjusted based on the available computing resources. The provided code is a basic example to demonstrate the concept of Bagging with neural networks. For better performance, hyperparameter tuning and cross-validation can be applied.\n"
      ],
      "metadata": {
        "id": "JXLHBMcwVhJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Boosting Neural Networks\n"
      ],
      "metadata": {
        "id": "V3S4f3zGWu-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting Neural Networks, also known as AdaBoost with Neural Networks, is a technique that combines multiple neural networks to improve predictive performance. The basic idea behind boosting is to train multiple weak learners (neural networks with limited predictive power) sequentially, giving more weight to misclassified examples in each iteration to correct errors made by previous models.\n",
        "\n",
        "In this example, we'll use the Pima Indians Diabetes dataset, which contains features like glucose levels, blood pressure, and BMI to predict whether a person has diabetes or not. We'll implement AdaBoost with Neural Networks using the `sklearn.neural_network.MLPClassifier` class, which represents a multi-layer perceptron (MLP) neural network.\n",
        "\n",
        "Before running the code in Google Colab, make sure to upload the 'pima-indians-diabetes.data.csv' file to your Colab workspace or use the raw URL to load the data."
      ],
      "metadata": {
        "id": "T8xZrEPYW0dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Pima Indians Diabetes dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Prepare the data\n",
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base neural network classifier\n",
        "base_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "\n",
        "# Create an AdaBoost classifier with the base neural network\n",
        "n_estimators = 10\n",
        "ada_boost_classifier = AdaBoostClassifier(base_classifier, n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "# Train the AdaBoost classifier\n",
        "ada_boost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_boost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Neural Network Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "FX9yex8WW4Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code example, we first load the Pima Indians Diabetes dataset and split it into training and testing sets. We then create a base neural network classifier using `MLPClassifier` with a single hidden layer containing 100 units. Next, we create an AdaBoost classifier using `AdaBoostClassifier` and pass the base classifier along with the number of estimators (neural networks) to train sequentially. The AdaBoost classifier will combine the predictions of the neural networks and give more weight to misclassified examples in each iteration.\n",
        "\n",
        "Finally, we train the AdaBoost classifier, make predictions on the test set, and calculate the accuracy of the model's predictions. This example demonstrates how to implement Boosting with Neural Networks using the Pima Indians Diabetes dataset in Google Colab. Note that you can experiment with different hyperparameters and architectures to improve the model's performance.\n"
      ],
      "metadata": {
        "id": "JqZOAXLpWzIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Stacking and Blending Neural Networks\n"
      ],
      "metadata": {
        "id": "Ulva4nI5XBln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking and Blending are ensemble techniques that involve combining the predictions of multiple models to improve performance. In the context of neural networks, Stacking and Blending are meta-ensemble methods. Instead of combining the predictions of different models like in Bagging or Boosting, they combine the outputs of multiple neural networks to make final predictions.\n",
        "\n",
        "Pima Indian Diabetes Dataset:\n",
        "The Pima Indian Diabetes dataset is a well-known dataset used for binary classification tasks. It contains various features (such as glucose levels, blood pressure, and body mass index) of female patients of Pima Indian heritage, along with a binary target variable indicating the presence or absence of diabetes.\n",
        "\n",
        "The goal of our example is to demonstrate Stacking and Blending with neural networks using the Pima Indian Diabetes dataset.\n",
        "\n",
        "Let's start with the code example:"
      ],
      "metadata": {
        "id": "eRl_BpPuXJYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Prepare the data\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base neural network models\n",
        "def create_base_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(4, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize base models\n",
        "num_base_models = 3\n",
        "base_models = [create_base_model() for _ in range(num_base_models)]\n",
        "\n",
        "# Train the base models\n",
        "for i, model in enumerate(base_models):\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "# Create a new model for stacking\n",
        "stacked_model = Sequential()\n",
        "for model in base_models:\n",
        "    model.trainable = False\n",
        "    stacked_model.add(model)\n",
        "stacked_model.add(Dense(1, activation='sigmoid'))\n",
        "stacked_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the stacked model on predictions of base models\n",
        "predictions = np.array([model.predict(X_train) for model in base_models])\n",
        "predictions = np.mean(predictions, axis=0)\n",
        "stacked_model.fit(predictions, y_train, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Make predictions using the stacked model\n",
        "test_predictions = np.array([model.predict(X_test) for model in base_models])\n",
        "test_predictions = np.mean(test_predictions, axis=0)\n",
        "y_pred = stacked_model.predict(test_predictions)\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate the accuracy of the stacked model\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Stacked Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Q3oWrXAxXNfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we first load the Pima Indian Diabetes dataset and split it into training and testing sets. We define three base neural network models with the `create_base_model` function and train them on the training data. Next, we create a new neural network model, `stacked_model`, and initialize it with the trained base models. We then train this stacked model using the predictions of the base models on the training data.\n",
        "\n",
        "For Blending, the process is similar, but instead of training a new model on the predictions of the base models, we use the predictions as additional features in the original dataset and train a separate model on the new dataset.\n",
        "\n",
        "Please note that the example provided is a simplified version of Stacking and Blending for illustration purposes. In practice, you may need to perform hyperparameter tuning and cross-validation to obtain better results. Also, consider using more diverse and complex base models for real-world applications.\n"
      ],
      "metadata": {
        "id": "t-oA1LtzXH9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6: Ensembling Convolutional Neural Networks (CNNs)\n"
      ],
      "metadata": {
        "id": "ysmDg7kTXem6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Ensembling CNNs with Bagging\n"
      ],
      "metadata": {
        "id": "b-8vdFqQXkdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembling CNNs with Bagging is a powerful technique used to improve the performance and robustness of convolutional neural networks (CNNs). The idea is to train multiple CNN models independently on different subsets of the training data and then combine their predictions to obtain the final output. Bagging helps to reduce overfitting and can lead to better generalization. In this example, we'll use the popular Kaggle dataset \"CIFAR-10\" and implement bagging with CNNs.\n",
        "\n",
        "Before we begin, make sure you have the necessary libraries installed. You can install them using pip:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow numpy keras\n",
        "```\n",
        "\n",
        "Now, let's proceed with the code example:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define the CNN model architecture\n",
        "def create_cnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Set the number of models to create (bagging ensemble size)\n",
        "num_models = 5\n",
        "models = []\n",
        "\n",
        "# Train individual CNN models and store them in the 'models' list\n",
        "for i in range(num_models):\n",
        "    print(f\"Training Model {i+1}\")\n",
        "    model = create_cnn_model()\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1)\n",
        "    models.append(model)\n",
        "\n",
        "# Make predictions using each individual model\n",
        "predictions = np.zeros_like(y_test)\n",
        "for model in models:\n",
        "    predictions += model.predict(x_test)\n",
        "\n",
        "# Get the final prediction by averaging the outputs of all models\n",
        "final_prediction = np.argmax(predictions, axis=1)\n",
        "accuracy = np.mean(final_prediction == np.argmax(y_test, axis=1))\n",
        "\n",
        "print(f\"Ensemble accuracy: {accuracy}\")\n",
        "```\n",
        "\n",
        "In this example, we define a simple CNN architecture using Keras to classify images from the CIFAR-10 dataset. We train multiple CNN models with slightly different initializations using the `create_cnn_model()` function. Each model is then trained on the training data using the `fit()` function. After training, we make predictions on the test data using each individual model and accumulate the predictions in the `predictions` array.\n",
        "\n",
        "Finally, we obtain the final prediction by averaging the outputs of all models and calculate the accuracy on the test set. This ensemble of models helps to improve the overall accuracy compared to training a single model.\n",
        "\n",
        "Note: The number of models in the ensemble (`num_models`) and the number of epochs can be adjusted based on the available computational resources and desired performance. Additionally, other data augmentation techniques or hyperparameter tuning can be applied to further improve the performance of the individual models in the ensemble.\n"
      ],
      "metadata": {
        "id": "DabjzDxOXoxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Ensembling CNNs with Boosting\n"
      ],
      "metadata": {
        "id": "gHFX1rw-Y48h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembling CNNs with boosting is a powerful technique used to improve the performance of Convolutional Neural Networks (CNNs) by combining multiple models in a weighted or adaptive manner. One common way to implement this approach is by using AdaBoost, a popular boosting algorithm. In this example, I'll demonstrate how to ensemble CNNs using AdaBoost on a real-world dataset from Kaggle.\n",
        "\n",
        "For this example, we'll use the \"CIFAR-10\" dataset, which is a popular dataset for image classification tasks. The goal is to classify images into one of the ten categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck.\n",
        "\n",
        "Please note that Kaggle kernels may vary over time, so I won't provide a specific Kaggle kernel link. Instead, I'll outline the steps to follow, and you can create a Kaggle kernel using these steps.\n",
        "\n",
        "Step 1: Import the required libraries"
      ],
      "metadata": {
        "id": "t-1_E4_iZxwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "zPOCKQOeZ2-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load the CIFAR-10 dataset\n"
      ],
      "metadata": {
        "id": "jEtNseHDZ4_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "DoN9S60BZ7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the CNN architecture\n"
      ],
      "metadata": {
        "id": "8KBCQef5aAQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a list of individual CNN models\n",
        "n_models = 5\n",
        "cnn_models = [create_cnn_model() for _ in range(n_models)]"
      ],
      "metadata": {
        "id": "kFj14jpKaBJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train the individual CNN models\n"
      ],
      "metadata": {
        "id": "J3bCqLylaGJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(cnn_models):\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "    cnn_models[i] = model  # Update the trained model in the list"
      ],
      "metadata": {
        "id": "fQ3SDCCSaIhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Create AdaBoost and fit the ensemble\n"
      ],
      "metadata": {
        "id": "NzWbJKHcal-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an AdaBoost classifier with DecisionTree as the base estimator\n",
        "adaboost_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=n_models)\n",
        "\n",
        "# Flatten individual model predictions to be used as features for AdaBoost\n",
        "x_train_ensemble = np.array([model.predict(x_train) for model in cnn_models]).transpose(1, 0, 2).reshape(-1, n_models * 10)\n",
        "x_test_ensemble = np.array([model.predict(x_test) for model in cnn_models]).transpose(1, 0, 2).reshape(-1, n_models * 10)\n",
        "\n",
        "# Fit the AdaBoost classifier on the individual model predictions\n",
        "adaboost_clf.fit(x_train_ensemble, np.argmax(y_train, axis=1))"
      ],
      "metadata": {
        "id": "AxS_AvQeaprh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Evaluate the ensemble model\n"
      ],
      "metadata": {
        "id": "7Wsnoot3avFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = adaboost_clf.score(x_test_ensemble, np.argmax(y_test, axis=1))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)"
      ],
      "metadata": {
        "id": "gydp-Itcaz5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this example, we created five individual CNN models, trained them on the CIFAR-10 dataset, and then used AdaBoost to combine their predictions. The ensemble model's accuracy is printed as the final result.\n",
        "\n",
        "Please note that the above code provides a basic outline of how to implement ensemble CNNs with boosting on the CIFAR-10 dataset. The actual performance may vary depending on the specific CNN architectures, hyperparameters, and the number of models used in the ensemble. You can further optimize and fine-tune the models to achieve better results.\n"
      ],
      "metadata": {
        "id": "PuycPOoPZEDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Ensembling Recurrent Neural Networks (RNNs)\n"
      ],
      "metadata": {
        "id": "7y1Xl7GNa5YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Ensembling RNNs with Bagging\n"
      ],
      "metadata": {
        "id": "pV2E5K1-a-c0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembling is a technique used to combine the predictions of multiple machine learning models to improve overall performance and reduce overfitting. Bagging is a specific ensemble method where multiple models are trained independently on different subsets of the training data and then their predictions are combined using majority voting for classification tasks or averaging for regression tasks.\n",
        "\n",
        "In this example, we will use Bagging to ensemble multiple Recurrent Neural Networks (RNNs) on the Pima Indian Diabetes dataset to predict whether a person has diabetes or not.\n",
        "\n",
        "Let's start by preparing the environment in Google Colab and loading the necessary libraries and dataset:"
      ],
      "metadata": {
        "id": "HHHaxcAKbGB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install tensorflow==2.5.0\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data'\n",
        "column_names = ['pregnancies', 'glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'diabetes_pedigree', 'age', 'class']\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split features and labels\n",
        "X = dataset.drop('class', axis=1).values\n",
        "y = dataset['class'].values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features to improve model training\n",
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ],
      "metadata": {
        "id": "iCFvgAX8bJzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a function to build and train the individual RNN models:\n"
      ],
      "metadata": {
        "id": "pYgK23QObRsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_rnn_model(X_train, y_train, epochs=10, batch_size=32):\n",
        "    input_shape = (X_train.shape[1], 1)  # Add a time step for LSTM\n",
        "    model = build_rnn_model(input_shape)\n",
        "    model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    return model"
      ],
      "metadata": {
        "id": "OKnEDg6ybYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create the ensemble by training multiple RNN models and storing them in a list:\n"
      ],
      "metadata": {
        "id": "ML5dWTbAbZqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_models = 5\n",
        "rnn_models = []\n",
        "\n",
        "for i in range(num_models):\n",
        "    model = train_rnn_model(X_train, y_train, epochs=10, batch_size=32)\n",
        "    rnn_models.append(model)"
      ],
      "metadata": {
        "id": "nQMyF61LbcXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's make predictions using each RNN model on the test set and combine their predictions using majority voting:\n"
      ],
      "metadata": {
        "id": "4CtI8Wj9bhnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predictions(models, X):\n",
        "    y_pred = np.zeros((X.shape[0], len(models)))\n",
        "    for i, model in enumerate(models):\n",
        "        y_pred[:, i] = model.predict(X.reshape(X.shape[0], X.shape[1], 1)).flatten()\n",
        "    return np.round(np.mean(y_pred, axis=1))\n",
        "\n",
        "# Ensemble predictions\n",
        "y_pred_ensemble = ensemble_predictions(rnn_models, X_test)\n",
        "\n",
        "# Convert the probabilities to binary predictions (0 or 1)\n",
        "y_pred_ensemble = (y_pred_ensemble > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "Bsbn5FeDblw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's evaluate the performance of the ensemble by calculating the accuracy:\n"
      ],
      "metadata": {
        "id": "wqZBCjjxbngK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(\"Ensemble Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "jZVGXY7Lbq0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code demonstrates how to use Bagging with Recurrent Neural Networks to create an ensemble and make predictions on the Pima Indian Diabetes dataset. The accuracy of the ensemble will likely be better than individual models, and it helps in reducing overfitting.\n"
      ],
      "metadata": {
        "id": "uxfiQImDbEKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Ensembling RNNs with Boosting\n"
      ],
      "metadata": {
        "id": "hmKRlVGJbyPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembling RNNs with Boosting involves combining multiple Recurrent Neural Networks (RNNs) to create a more robust and accurate model. The idea is to train several RNNs independently and then combine their predictions to make the final decision. In this example, we will use a boosting algorithm called AdaBoost to combine the predictions of multiple RNNs on the Pima Indians Diabetes dataset.\n",
        "\n",
        "The Pima Indians Diabetes dataset contains features like glucose level, blood pressure, and other health-related measurements of Pima Indian women, along with a target variable indicating whether they have diabetes or not.\n",
        "\n",
        "Here's how we can do it step-by-step:\n",
        "\n",
        "1. Load the necessary libraries and the dataset.\n",
        "2. Preprocess the dataset (normalize and split into training and testing sets).\n",
        "3. Create multiple RNN models.\n",
        "4. Train the RNN models using AdaBoost.\n",
        "5. Evaluate the ensemble model."
      ],
      "metadata": {
        "id": "tS00Sirmb6X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's proceed with the code:\n"
      ],
      "metadata": {
        "id": "kzkkKzZ9cG-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Step 2: Preprocess the dataset\n",
        "X = dataset.drop('Outcome', axis=1).values\n",
        "y = dataset['Outcome'].values\n",
        "\n",
        "# Normalize the features\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create multiple RNN models\n",
        "def create_rnn_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 4: Train the RNN models using AdaBoost\n",
        "num_models = 5  # You can adjust the number of RNN models here\n",
        "models = []\n",
        "for _ in range(num_models):\n",
        "    model = create_rnn_model()\n",
        "    model.fit(X_train.reshape(-1, X_train.shape[1], X_train.shape[2]), y_train, epochs=50, batch_size=64, verbose=0)\n",
        "    models.append(model)\n",
        "\n",
        "# Step 5: Evaluate the ensemble model\n",
        "def ensemble_predictions(models, X):\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    for model in models:\n",
        "        y_pred += model.predict(X).flatten()\n",
        "    y_pred /= len(models)\n",
        "    return np.round(y_pred)\n",
        "\n",
        "# Evaluate the ensemble model on the test set\n",
        "y_pred_ensemble = ensemble_predictions(models, X_test.reshape(-1, X_test.shape[1], X_test.shape[2]))\n",
        "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "\n",
        "print(f\"Ensemble Accuracy: {accuracy_ensemble:.4f}\")"
      ],
      "metadata": {
        "id": "yYX6ldzscIJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code creates and trains multiple RNN models using the Pima Indians Diabetes dataset. It then uses AdaBoost to combine the predictions of these models and calculates the accuracy of the ensemble model on the test set. The final accuracy of the ensemble model will be printed at the end.\n",
        "\n",
        "Please note that this implementation treats the RNNs as black-box models, and you may further optimize the architecture and hyperparameters for better performance. Additionally, since AdaBoost is mainly used for classification tasks, the example assumes a binary classification problem for diabetes prediction.\n"
      ],
      "metadata": {
        "id": "c1MvErr_b47p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Combining RNNs with Stacking and Blending\n"
      ],
      "metadata": {
        "id": "PXdubPCjc1lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining RNNs (Recurrent Neural Networks) with stacking and blending is a technique used to improve the performance of predictive models. Stacking and blending are ensemble learning techniques that involve combining multiple models to create a more robust and accurate prediction.\n",
        "\n",
        "In this example, we'll use the Pima Indian Diabetes dataset and demonstrate how to combine RNNs with stacking and blending using Python and TensorFlow in Google Colab. We'll follow these steps:\n",
        "\n",
        "1. Data preparation: Load and preprocess the dataset.\n",
        "2. Define the individual RNN models.\n",
        "3. Implement stacking: Train multiple RNN models and combine their predictions.\n",
        "4. Implement blending: Blend the predictions from different models to make the final prediction.\n",
        "\n",
        "Please ensure you have TensorFlow and other necessary libraries installed in your Colab environment.\n",
        "\n",
        "Here's the code:"
      ],
      "metadata": {
        "id": "In-hzNdWdAxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Data preparation\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Define the individual RNN models\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(32, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 3: Implement stacking\n",
        "def stacking_rnn(n_models):\n",
        "    models = []\n",
        "    for _ in range(n_models):\n",
        "        model = create_rnn_model()\n",
        "        model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=32, verbose=0)\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "n_models = 3\n",
        "stacked_models = stacking_rnn(n_models)\n",
        "\n",
        "# Step 4: Implement blending\n",
        "def blending_predictions(models, X):\n",
        "    predictions = np.zeros((len(X), len(models)))\n",
        "    for i, model in enumerate(models):\n",
        "        predictions[:, i] = model.predict(X.reshape(X.shape[0], X.shape[1], 1)).flatten()\n",
        "    return np.mean(predictions, axis=1)\n",
        "\n",
        "# Combine the predictions from different models\n",
        "blended_predictions_train = blending_predictions(stacked_models, X_train)\n",
        "blended_predictions_test = blending_predictions(stacked_models, X_test)\n",
        "\n",
        "# Make final predictions and evaluate accuracy\n",
        "final_predictions_train = (blended_predictions_train > 0.5).astype(int)\n",
        "final_predictions_test = (blended_predictions_test > 0.5).astype(int)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, final_predictions_train)\n",
        "test_accuracy = accuracy_score(y_test, final_predictions_test)\n",
        "\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "XG6FhAxPdEwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we first load and preprocess the dataset, splitting it into training and testing sets. Then, we define the individual RNN models, stacking them together to create the `stacking_rnn` function. Finally, we blend the predictions from the stacked models using the `blending_predictions` function.\n",
        "\n",
        "Note that in this example, we used a simple RNN model with only one LSTM layer and two dense layers for demonstration purposes. In practice, you can customize the architecture and hyperparameters of the RNN models to achieve better performance. Additionally, you can explore other ensemble techniques like bagging, boosting, and more to further improve the model's accuracy.\n"
      ],
      "metadata": {
        "id": "Nq0ZDIFKc6XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Ensemble Techniques for Transfer Learning\n"
      ],
      "metadata": {
        "id": "DEoWxKJgdOJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Transfer Learning Basics\n"
      ],
      "metadata": {
        "id": "TW1nNVG7dTTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a machine learning technique that involves leveraging knowledge learned from one task to improve the performance of a related but different task. In the context of deep learning, transfer learning refers to using pre-trained neural network models on one dataset to improve the performance on a different, but related, dataset or task.\n",
        "\n",
        "The idea behind transfer learning is that neural networks learn hierarchical feature representations during training, where lower layers capture low-level features (e.g., edges, textures) and higher layers capture more abstract features (e.g., shapes, objects). These learned representations are transferable across tasks, especially when the tasks share some underlying patterns.\n",
        "\n",
        "Transfer learning consists of two main steps:\n",
        "\n",
        "1. Pre-training: The first step is to train a deep neural network model on a large-scale dataset for a related task. This pre-training step is computationally expensive and time-consuming but is typically done on powerful hardware or cloud-based resources. The pre-trained model captures valuable knowledge about the dataset it was trained on and learns to extract useful feature representations.\n",
        "\n",
        "2. Fine-tuning: After pre-training, the second step is fine-tuning. In this step, the pre-trained model is used as a starting point for the new task. The model is further trained on the target dataset, but with a much smaller learning rate to avoid destroying the previously learned representations. The idea is to adapt the model's knowledge to the new task while preserving the learned features from the pre-training.\n",
        "\n",
        "Advantages of Transfer Learning:\n",
        "\n",
        "1. Reduced Training Time: Pre-training a neural network on a large dataset can take days or weeks. By leveraging a pre-trained model, the fine-tuning process requires much less time and data.\n",
        "\n",
        "2. Improved Performance: Transfer learning can significantly boost the performance of models on the target task, especially when the target dataset is small or lacks sufficient labeled data.\n",
        "\n",
        "3. Generalization: Pre-trained models have learned general feature representations from diverse datasets, making them more adaptable to different tasks and datasets.\n",
        "\n",
        "4. Addressing Data Scarcity: In scenarios where obtaining a large labeled dataset is challenging, transfer learning can provide a viable solution.\n",
        "\n",
        "Common Transfer Learning Strategies:\n",
        "\n",
        "1. Feature Extraction: In this approach, the pre-trained model is used as a fixed feature extractor. The last fully connected layers of the model are removed, and the output of the remaining layers is used as features for a new classifier, which is trained on the target task.\n",
        "\n",
        "2. Fine-Tuning: Fine-tuning involves training the entire pre-trained model on the new task, but with a lower learning rate. This approach allows the model to learn task-specific features while preserving the previously learned representations.\n",
        "\n",
        "Transfer learning has been widely successful in various domains, such as computer vision (e.g., ImageNet pre-training for object recognition tasks), natural language processing (e.g., using pre-trained language models for sentiment analysis), and healthcare (e.g., using pre-trained models for medical image analysis).\n",
        "\n",
        "Overall, transfer learning is a powerful tool that allows data scientists and machine learning practitioners to build more effective and efficient models for a wide range of tasks. It enables us to leverage the knowledge gained from one task to solve related problems and overcome challenges posed by limited data availability and computational resources.\n"
      ],
      "metadata": {
        "id": "ajYN9KOSdYiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Using Ensemble Techniques with Transfer Learning\n"
      ],
      "metadata": {
        "id": "DJTwAJFXdbXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques with transfer learning can be a powerful approach to improve the performance of machine learning models. In this context, ensemble techniques involve combining multiple models to make predictions, and transfer learning refers to leveraging knowledge learned from one task or domain to improve the performance of another task or domain. In this example, we will use a popular ensemble technique called \"Voting Classifier\" along with transfer learning using pre-trained models to classify the Pima Indian diabetes dataset.\n",
        "\n",
        "We will use the following steps:\n",
        "\n",
        "1. Load the Pima Indian diabetes dataset.\n",
        "2. Prepare the data by splitting it into features (X) and the target variable (y).\n",
        "3. Preprocess the data.\n",
        "4. Implement transfer learning using pre-trained models.\n",
        "5. Create a Voting Classifier that combines multiple models.\n",
        "6. Evaluate the ensemble's performance.\n",
        "\n",
        "Let's start by implementing the code in Google Colab:\n"
      ],
      "metadata": {
        "id": "GvAXuvk4dl5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Pima Indian diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "# Assign column names for better understanding\n",
        "df.columns = [\n",
        "    \"pregnancies\", \"glucose\", \"blood_pressure\", \"skin_thickness\",\n",
        "    \"insulin\", \"bmi\", \"diabetes_pedigree_function\", \"age\", \"outcome\"\n",
        "]\n",
        "\n",
        "# Step 2: Prepare the data\n",
        "X = df.drop(columns=[\"outcome\"])\n",
        "y = df[\"outcome\"]\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Implement transfer learning using pre-trained models\n",
        "# (In this example, we'll not use any pre-trained models since we don't have a specific transfer learning scenario)\n",
        "\n",
        "# Step 5: Create a Voting Classifier that combines multiple models\n",
        "# We will use Decision Tree, Random Forest, and Logistic Regression as base classifiers\n",
        "base_classifiers = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('random_forest', RandomForestClassifier(random_state=42)),\n",
        "    ('logistic_regression', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "voting_classifier = VotingClassifier(estimators=base_classifiers, voting='hard')\n",
        "\n",
        "# Step 6: Evaluate the ensemble's performance\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the ensemble model\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Hv9gV1VRdtxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: In this example, we didn't use any pre-trained models for transfer learning as the Pima Indian diabetes dataset is relatively small and doesn't have any specific domain-related transfer learning scenarios. Transfer learning is more commonly used with deep learning models on large-scale datasets.\n",
        "\n",
        "You can run this code in Google Colab or any other Python environment with the required libraries installed. It uses a Voting Classifier to combine predictions from Decision Trees, Random Forest, and Logistic Regression models. You can further experiment with different base classifiers and tuning hyperparameters to improve the ensemble's performance.\n"
      ],
      "metadata": {
        "id": "DYZHnav8diQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Fine-tuning Ensemble Models for Specific Tasks\n"
      ],
      "metadata": {
        "id": "5AJCG74od5DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning ensemble models involves combining multiple pre-trained models and fine-tuning them on a specific task or dataset to improve performance. In this example, we'll use the Pima Indian Diabetes dataset to demonstrate how to fine-tune an ensemble of pre-trained classifiers using the `scikit-learn` library in Python.\n",
        "\n",
        "Here's the step-by-step code to fine-tune ensemble models using the Pima Indian Diabetes dataset in Google Colab:\n",
        "\n",
        "1. Import necessary libraries:"
      ],
      "metadata": {
        "id": "8BMOM5I8eAMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "HfGmUx9MeDgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the dataset from the provided URL:\n"
      ],
      "metadata": {
        "id": "ue4K1275eIjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)"
      ],
      "metadata": {
        "id": "UEknmbdVeewp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess the data:\n"
      ],
      "metadata": {
        "id": "VbBC0cDKelzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vN9iDXvgepMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Initialize and fine-tune the pre-trained classifiers:\n",
        "\n",
        "In this example, we'll use three pre-trained classifiers (Random Forest, Gradient Boosting, and SVM) and create an ensemble by combining them."
      ],
      "metadata": {
        "id": "jKaHttpwevw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pre-trained classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf), ('svm', svm_clf)], voting='soft')\n",
        "\n",
        "# Fit the ensemble on the training data\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "OAaJctYle00d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Remember to run the code cells in the same order in Google Colab to execute the entire process. The code creates an ensemble of three pre-trained classifiers (Random Forest, Gradient Boosting, and SVM) and fine-tunes them using the Pima Indian Diabetes dataset. The accuracy of the ensemble model is then printed as the final result.\n"
      ],
      "metadata": {
        "id": "FV5fB9APd-i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9: Evaluation and Interpretation of Ensemble Models\n"
      ],
      "metadata": {
        "id": "fvgDs3Cfe_7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Performance Metrics for Ensemble Models\n"
      ],
      "metadata": {
        "id": "53lW2sMVfGNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the performance of ensemble models, the most commonly used performance metrics are accuracy, precision, recall, F1-score, and ROC-AUC (Receiver Operating Characteristic - Area Under the Curve). Here's an example of how to calculate these performance metrics for an ensemble model using the Pima Indian Diabetes dataset in Python:"
      ],
      "metadata": {
        "id": "M0XX-5pjfM6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Load the dataset from the provided URL\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Preprocess the data\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the pre-trained classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf), ('svm', svm_clf)], voting='soft')\n",
        "\n",
        "# Fit the ensemble on the training data\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "y_prob = ensemble_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Ensemble Model Performance Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"ROC-AUC:\", roc_auc)"
      ],
      "metadata": {
        "id": "Wo_DFIShfPhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we calculated the accuracy, precision, recall, F1 score, and ROC-AUC for the ensemble model. Note that the SVM classifier in this example is used with `probability=True` to enable probability estimates, which is required for calculating the ROC-AUC.\n"
      ],
      "metadata": {
        "id": "9jVDC_aAfLbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Model Interpretability in Ensembles\n"
      ],
      "metadata": {
        "id": "JgCdzZFBfkap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model interpretability in ensembles is essential for understanding how the ensemble makes predictions and which features contribute the most to the decision-making process. In this code example, we will demonstrate how to interpret an ensemble model using the SHAP (SHapley Additive exPlanations) library in Python.\n",
        "\n",
        "SHAP values provide a unified measure of feature importance for any model, including ensemble models. We will use the same Pima Indian Diabetes dataset and ensemble model created in the previous example.\n",
        "\n",
        "Here's the step-by-step code to interpret the ensemble model using SHAP:\n",
        "\n",
        "1. Install the required libraries:"
      ],
      "metadata": {
        "id": "JkSqVunMfqIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n"
      ],
      "metadata": {
        "id": "A4x83jrafvq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import the necessary libraries:\n"
      ],
      "metadata": {
        "id": "1lDKdiFhfyNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "304C1jwjf1EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load the dataset from the provided URL:\n"
      ],
      "metadata": {
        "id": "go2aJhGUf4q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)"
      ],
      "metadata": {
        "id": "dlwPcN5mf-dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Preprocess the data:\n"
      ],
      "metadata": {
        "id": "oof5L2b0f_pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2vIXqZG0gDWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Initialize and fine-tune the ensemble model (same as in the previous example).\n",
        "\n",
        "6. Interpret the model using SHAP:"
      ],
      "metadata": {
        "id": "vwiYnD0xgJRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SHAP explainer\n",
        "explainer = shap.Explainer(ensemble_clf)\n",
        "\n",
        "# Calculate SHAP values for the test data\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary plot to show feature importance\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.columns[:-1])\n",
        "```"
      ],
      "metadata": {
        "id": "SgnfF3NqgPZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Display the SHAP values summary plot:\n"
      ],
      "metadata": {
        "id": "V7oYJM8igQiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.columns[:-1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kYZ0y54MgTze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code will generate a summary plot showing the feature importance values for the ensemble model using SHAP values. Each feature's contribution to each prediction will be visualized, helping to understand which features are driving the model's predictions.\n",
        "\n",
        "Make sure to run the code cells in the same order in Google Colab to execute the entire process and visualize the SHAP summary plot.\n"
      ],
      "metadata": {
        "id": "thpA-zjOfoyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Interpreting Ensemble Predictions\n"
      ],
      "metadata": {
        "id": "CFgu5lqWgZnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting ensemble predictions involves understanding how the individual classifiers in the ensemble contribute to the final prediction. One common way to interpret ensemble predictions is by examining the class probabilities assigned by each individual classifier. We can also analyze the importance of features in the ensemble's decision-making process.\n",
        "\n",
        "Let's use the previously loaded Pima Indian Diabetes dataset and the ensemble model created in the previous example to demonstrate how to interpret ensemble predictions using class probabilities and feature importance:"
      ],
      "metadata": {
        "id": "uKefPSy3gft9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the pre-trained classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf), ('svm', svm_clf)], voting='soft')\n",
        "\n",
        "# Fit the ensemble on the training data\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy)\n",
        "\n",
        "# Interpreting ensemble predictions using class probabilities\n",
        "class_probs = ensemble_clf.predict_proba(X_test)\n",
        "for i, class_prob in enumerate(class_probs[:5]):\n",
        "    print(f\"Sample {i+1}: Class 0 Prob={class_prob[0]:.4f}, Class 1 Prob={class_prob[1]:.4f}\")\n",
        "\n",
        "# Feature Importance in the ensemble model (Random Forest)\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "sorted_indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "for i in sorted_indices:\n",
        "    print(f\"Feature {i+1}: {feature_importance[i]:.4f}\")"
      ],
      "metadata": {
        "id": "9n2cF9jOgjNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this code, we calculate the class probabilities for each sample in the test set using the ensemble model. We then print the probabilities for the first five samples. Additionally, we calculate the feature importance using the Random Forest classifier, which is one of the individual classifiers in the ensemble, and print the importance scores for each feature.\n",
        "\n",
        "Keep in mind that the interpretation of ensemble predictions can vary depending on the specific algorithms used in the ensemble and the dataset characteristics. Also, some ensemble techniques, such as majority voting, might not provide direct probabilities for class assignments, making their interpretation different from models with probability outputs.\n"
      ],
      "metadata": {
        "id": "zsqB6IAqgeJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10: Handling Imbalanced Data with Ensembles\n"
      ],
      "metadata": {
        "id": "lQpi8S4-gqud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Imbalanced Data Problem Overview\n"
      ],
      "metadata": {
        "id": "xnm2AjqGgvcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The imbalanced data problem occurs when the distribution of classes in a dataset is highly skewed, meaning that one class is significantly more prevalent than the others. In such cases, the minority class (the class with fewer instances) is considered the \"positive\" class, while the majority class (the class with more instances) is considered the \"negative\" class. This imbalance can pose challenges when building machine learning models, as most standard algorithms are designed to work well with balanced datasets.\n",
        "\n",
        "Imbalanced data is a common issue in various real-world applications, such as fraud detection, medical diagnosis, anomaly detection, and customer churn prediction, where the positive class events are relatively rare compared to the negative class events.\n",
        "\n",
        "Challenges of Imbalanced Data:\n",
        "1. Bias: Models trained on imbalanced data tend to be biased towards the majority class. As a result, they may not adequately represent or accurately predict the minority class.\n",
        "\n",
        "2. Poor Generalization: Imbalanced datasets can lead to poor generalization, as the model may perform well on the majority class but poorly on the minority class in unseen data.\n",
        "\n",
        "3. Low Recall: In situations where the positive class is the one of interest (e.g., detecting fraudulent transactions), low recall can be a problem. Low recall means that the model fails to identify a significant number of positive instances.\n",
        "\n",
        "4. Overfitting: Models can be prone to overfitting the majority class, especially if the dataset is heavily imbalanced.\n",
        "\n",
        "Strategies to Handle Imbalanced Data:\n",
        "1. Resampling: This involves either oversampling the minority class by duplicating instances or undersampling the majority class by removing instances. Both methods aim to balance the class distribution.\n",
        "\n",
        "2. Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic samples for the minority class, which helps in creating a balanced dataset.\n",
        "\n",
        "3. Class Weighting: Many algorithms allow assigning higher weights to the minority class during training to give it more importance.\n",
        "\n",
        "4. Ensemble Methods: Ensemble models, like Bagging and Boosting, can be effective for imbalanced data, as they can reduce the impact of the class imbalance.\n",
        "\n",
        "5. Anomaly Detection: Consider treating the problem as an anomaly detection task if the minority class represents abnormal or rare events.\n",
        "\n",
        "6. Evaluation Metrics: Use appropriate evaluation metrics like precision, recall, F1-score, and area under the Receiver Operating Characteristic (ROC) curve (AUC-ROC) to assess model performance, rather than accuracy, which can be misleading on imbalanced data.\n",
        "\n",
        "Handling imbalanced data is an essential step in building robust and fair machine learning models. The choice of strategy depends on the specific problem and dataset characteristics, and experimentation with different techniques is often necessary to find the most effective solution.\n"
      ],
      "metadata": {
        "id": "9Chmtw4-g3MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Using Ensembles to Handle Imbalanced Data\n"
      ],
      "metadata": {
        "id": "iPfmzt3bhDY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods can be effective in handling imbalanced data by combining multiple models to improve the overall classification performance. Here, we'll use the Pima Indian Diabetes dataset and demonstrate how to use an ensemble of classifiers to address the class imbalance issue. We'll use the Random Forest and Gradient Boosting classifiers as the base models in the ensemble."
      ],
      "metadata": {
        "id": "SnNYkLFhhKC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the base classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
        "\n",
        "# Fit the ensemble on the training data\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "6hsYrk60hMy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By using an ensemble of classifiers, we can benefit from the combination of different model strengths and mitigate the impact of imbalanced data. Moreover, ensemble methods like voting can provide more balanced predictions as they take into account the collective decisions of multiple classifiers. When dealing with imbalanced data, it is essential to evaluate the model's performance using relevant metrics like precision, recall, and F1-score, in addition to accuracy, to get a comprehensive understanding of its effectiveness in handling the class imbalance.\n"
      ],
      "metadata": {
        "id": "tI9x681QhIi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Performance Metrics for Imbalanced Data\n"
      ],
      "metadata": {
        "id": "1Bo0ofdxhWUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with imbalanced data in ensemble models, it's essential to use performance metrics that are robust to imbalanced classes. Accuracy alone may not be a reliable measure when the classes are imbalanced because the model could achieve high accuracy by simply predicting the majority class. Instead, we can use metrics such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) to assess the ensemble model's performance on the Pima Indian Diabetes dataset.\n",
        "\n",
        "Here's how you can evaluate the ensemble model using these performance metrics:"
      ],
      "metadata": {
        "id": "ZllP1yeXhc5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the pre-trained classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf), ('svm', svm_clf)], voting='soft')\n",
        "\n",
        "# Fit the ensemble on the training data\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Calculate AUC-ROC score\n",
        "y_probs = ensemble_clf.predict_proba(X_test)[:, 1]\n",
        "auc_roc = roc_auc_score(y_test, y_probs)\n",
        "print(\"AUC-ROC Score:\", auc_roc)\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, color='b', label='Ensemble Model')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lo6814kmhgrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we calculate precision, recall, F1-score, and AUC-ROC for the ensemble model. The ROC curve is also plotted to visualize the trade-off between true positive rate (recall) and false positive rate. These metrics provide a more comprehensive evaluation of the ensemble's performance, especially when dealing with imbalanced data.\n"
      ],
      "metadata": {
        "id": "TxyzbTTphb28"
      }
    }
  ]
}