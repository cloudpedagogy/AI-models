{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgBoWEIwd5S6fPJvRz2sOu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Gaussian_Mixture_Models_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models (GMM) Background"
      ],
      "metadata": {
        "id": "WnNcx8dp9N5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Mixture Model (GMM) is a statistical model used for probability density estimation and unsupervised clustering tasks. It represents a probability distribution as a weighted sum of multiple Gaussian distributions, each referred to as a component. The idea behind GMM is that data points are generated from one of the Gaussian components with certain probabilities, and the model's goal is to estimate these underlying distributions and their associated weights.\n",
        "\n",
        "Here are some pros and cons of Gaussian Mixture Models:\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "1. **Flexibility:** GMM is a flexible model that can approximate complex data distributions by combining multiple Gaussian components with different means and covariances.\n",
        "\n",
        "2. **Soft Clustering:** Unlike hard clustering algorithms (e.g., K-means), GMM provides soft clustering. It assigns probabilities of data points belonging to each cluster, which can be useful when data points may belong to multiple clusters.\n",
        "\n",
        "3. **Representation of Uncertainty:** GMM naturally incorporates uncertainty in its clustering results due to the probability assignments. This can be beneficial in scenarios where data points don't clearly belong to a single cluster.\n",
        "\n",
        "4. **Robustness to Noise:** GMM can handle data with noise and outliers since it models data as a mixture of Gaussians, which are less sensitive to individual data points.\n",
        "\n",
        "5. **Richness of Information:** Apart from clustering, GMM can also be used for density estimation, generating synthetic data, and imputing missing data, among other applications.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "1. **Initialization Sensitivity:** The performance of GMM can be sensitive to the initialization of the model parameters, including the number of components and their initial positions.\n",
        "\n",
        "2. **Computationally Intensive:** GMM can be computationally expensive, especially when the dataset is large or the number of components is high.\n",
        "\n",
        "3. **Number of Components:** Determining the appropriate number of components (clusters) in the model can be challenging and often requires the use of model selection techniques.\n",
        "\n",
        "4. **Singular Covariance Issues:** GMM might encounter numerical instability when dealing with clusters with very small variance or singular covariance matrices.\n",
        "\n",
        "5. **Biased Representations:** GMM assumes that data follows Gaussian distributions, which may not be suitable for all types of data distributions.\n",
        "\n",
        "**When to Use GMM:**\n",
        "\n",
        "Gaussian Mixture Models can be useful in several scenarios:\n",
        "\n",
        "1. **Clustering with Uncertainty:** When you need to cluster data and want to consider uncertainty in the assignment of data points to clusters, GMM is a good choice.\n",
        "\n",
        "2. **Density Estimation:** If you want to estimate the underlying probability density of your data, GMM can provide a smooth approximation.\n",
        "\n",
        "3. **Outlier Detection:** GMM's soft clustering nature allows it to identify outliers that do not fit well into any of the Gaussian components.\n",
        "\n",
        "4. **Missing Data Imputation:** GMM can be used to impute missing data by estimating the missing values based on the probabilities of data belonging to different components.\n",
        "\n",
        "5. **Generating Synthetic Data:** GMM can be used to generate synthetic data that follows a distribution similar to the observed data.\n",
        "\n",
        "Remember that the effectiveness of GMM depends on the nature of your data and the specific task you want to accomplish. It is always a good idea to compare GMM with other clustering and density estimation algorithms to see which one best suits your needs."
      ],
      "metadata": {
        "id": "HGQwwSz5DWBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "O0KBQtGkB3qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 300\n",
        "X = np.concatenate([np.random.normal(0, 1, int(0.3 * n_samples)),\n",
        "                    np.random.normal(5, 1, int(0.7 * n_samples))])[:, np.newaxis]\n",
        "\n",
        "# Fit a Gaussian Mixture Model to the data\n",
        "n_components = 2\n",
        "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
        "gmm.fit(X)\n",
        "\n",
        "# Predict the cluster assignments for each data point\n",
        "labels = gmm.predict(X)\n",
        "\n",
        "# Generate new data points from the GMM model\n",
        "n_new_samples = 1000\n",
        "X_new, _ = gmm.sample(n_samples=n_new_samples)\n",
        "\n",
        "# Plot the original data and the GMM-generated data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[:, 0], np.zeros_like(X), alpha=0.6, label='Original Data')\n",
        "plt.scatter(X_new[:, 0], np.ones_like(X_new), alpha=0.6, label='GMM-Generated Data')\n",
        "plt.legend()\n",
        "plt.title('Original Data vs. GMM-Generated Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3Cet7MOyJyFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "IduBZmBnMpgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "```\n",
        "\n",
        "2. Generate some sample data:\n",
        "```python\n",
        "np.random.seed(42)\n",
        "n_samples = 300\n",
        "X = np.concatenate([np.random.normal(0, 1, int(0.3 * n_samples)),\n",
        "                    np.random.normal(5, 1, int(0.7 * n_samples))])[:, np.newaxis]\n",
        "```\n",
        "Here, we generate 300 data points from two normal distributions. The first distribution has a mean of 0 and standard deviation of 1, and the second distribution has a mean of 5 and standard deviation of 1. We then concatenate the two sets of data points to create a one-dimensional NumPy array called `X`.\n",
        "\n",
        "3. Fit a Gaussian Mixture Model to the data:\n",
        "```python\n",
        "n_components = 2\n",
        "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
        "gmm.fit(X)\n",
        "```\n",
        "We specify that we want to fit a Gaussian Mixture Model with 2 components to the data (`n_components=2`). The `GaussianMixture` class is imported from scikit-learn. We initialize the model and then use the `fit` method to learn the parameters of the model from the data.\n",
        "\n",
        "4. Predict the cluster assignments for each data point:\n",
        "```python\n",
        "labels = gmm.predict(X)\n",
        "```\n",
        "We use the trained GMM model to predict the cluster assignments for each data point in `X`. The cluster assignments are stored in the `labels` variable.\n",
        "\n",
        "5. Generate new data points from the GMM model:\n",
        "```python\n",
        "n_new_samples = 1000\n",
        "X_new, _ = gmm.sample(n_samples=n_new_samples)\n",
        "```\n",
        "We generate 1000 new data points (`n_new_samples=1000`) from the trained GMM model using the `sample` method. The new data points are stored in `X_new`, and we ignore the second return value (which would be the corresponding cluster assignments).\n",
        "\n",
        "6. Plot the original data and the GMM-generated data:\n",
        "```python\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[:, 0], np.zeros_like(X), alpha=0.6, label='Original Data')\n",
        "plt.scatter(X_new[:, 0], np.ones_like(X_new), alpha=0.6, label='GMM-Generated Data')\n",
        "plt.legend()\n",
        "plt.title('Original Data vs. GMM-Generated Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.show()\n",
        "```\n",
        "Finally, we create a plot using `matplotlib` to visualize the original data (`X`) and the data generated by the GMM model (`X_new`). We use `plt.scatter` to plot the points on the graph. The first call to `plt.scatter` plots the original data with zero as the y-coordinate, and the second call plots the generated data with one as the y-coordinate. The `alpha` parameter controls the transparency of the points. We also add a legend, title, and labels to the plot for better understanding.\n",
        "\n",
        "Overall, this code demonstrates how to use a Gaussian Mixture Model (GMM) to model a dataset and then generate new data points from the learned model. The plot shows the original data points and the GMM-generated data points on the same graph, giving a visual comparison of how well the GMM captures the underlying data distribution."
      ],
      "metadata": {
        "id": "5ae4pjycskfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "kyLh-O5ISjfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of using Gaussian Mixture Models (GMM) in a healthcare setting for medical image segmentation. Medical image segmentation is a critical task in medical imaging, where the goal is to partition an image into different regions representing different anatomical structures or pathologies.\n",
        "\n",
        "**Example: Brain Tumor Segmentation using Gaussian Mixture Models**\n",
        "\n",
        "In this example, we'll focus on segmenting brain tumor regions in Magnetic Resonance (MR) images using GMM. The GMM will be used to model the intensity distribution of the brain MRI, and it will help us identify different regions in the image corresponding to tumor and non-tumor areas.\n",
        "\n",
        "**Step-by-step explanation:**\n",
        "\n",
        "1. **Data Collection:** Collect a dataset of brain MR images that contains images with and without brain tumors. Each image in the dataset should be annotated with ground truth tumor segmentations.\n",
        "\n",
        "2. **Preprocessing:** Preprocess the MR images, which may include steps like intensity normalization, skull stripping, and resampling to a consistent voxel size.\n",
        "\n",
        "3. **Feature Extraction:** Extract relevant features from the preprocessed images that can help distinguish between tumor and non-tumor regions. For GMM, we can use the image intensity values as the feature vector.\n",
        "\n",
        "4. **Gaussian Mixture Model:** Fit a GMM to the feature vectors (image intensity values) extracted from the MR images. The GMM will learn the underlying distribution of intensity values and identify different clusters of intensities, which correspond to different tissues in the brain.\n",
        "\n",
        "5. **Segmentation:** Once the GMM is trained, use it to segment the brain MR images. Assign each voxel in the image to a cluster based on the highest probability from the GMM. For example, one cluster may represent healthy brain tissue, and another cluster may represent tumor regions.\n",
        "\n",
        "6. **Post-processing:** Perform post-processing on the segmentation results to refine the segmentation boundaries and remove any potential noise.\n",
        "\n",
        "7. **Evaluation:** Compare the GMM-based segmentation with the ground truth segmentations to evaluate the performance of the model. Common evaluation metrics in medical image segmentation include Dice Similarity Coefficient (DSC), Jaccard Index (IoU), and sensitivity/specificity.\n",
        "\n",
        "8. **Clinical Applications:** Once the GMM model is trained and validated, it can be used for various clinical applications, such as tumor volume estimation, treatment planning, and monitoring disease progression. Accurate tumor segmentation is essential for guiding treatment decisions and assessing treatment response.\n",
        "\n",
        "Gaussian Mixture Models offer a probabilistic approach to image segmentation, allowing us to model the complex intensity distributions in medical images effectively. In the healthcare setting, accurate tumor segmentation can aid radiologists and clinicians in making informed decisions, leading to improved patient care and better treatment outcomes."
      ],
      "metadata": {
        "id": "KJ1o6As29s46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "RjuBTJ6HZY9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is a Gaussian Mixture Model (GMM)?**\n",
        "   A GMM is a probabilistic model that represents data as a mixture of multiple Gaussian distributions. It assumes that the data is generated by a combination of several Gaussian distributions, each representing a cluster in the data.\n",
        "\n",
        "2. **What is the main advantage of using GMMs over k-means clustering?**\n",
        "   Unlike k-means clustering, which assigns each data point to a single cluster, GMMs allow soft clustering. This means that data points can belong to multiple clusters with varying degrees of membership, providing a more flexible representation of the data.\n",
        "\n",
        "3. **How is the number of components (clusters) determined in a GMM?**\n",
        "   Determining the optimal number of components is a challenging task in GMM. Various methods, such as the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC), can be used to estimate the appropriate number of clusters based on model complexity and data fit.\n",
        "\n",
        "4. **What are the applications of Gaussian Mixture Models?**\n",
        "   GMMs have numerous applications, including image segmentation, data clustering, anomaly detection, speech recognition, natural language processing, and even in modeling financial data and human activity recognition.\n",
        "\n",
        "5. **How are GMMs trained?**\n",
        "   GMMs are typically trained using the Expectation-Maximization (EM) algorithm. The EM algorithm is an iterative process that estimates the model parameters by alternating between the E-step (expectation) and M-step (maximization) until convergence.\n",
        "\n",
        "6. **Can GMMs handle high-dimensional data?**\n",
        "   GMMs can face challenges with high-dimensional data due to the \"curse of dimensionality.\" As the number of dimensions increases, the number of model parameters also increases, which can lead to overfitting and computational complexity. Dimensionality reduction techniques like Principal Component Analysis (PCA) are often used to mitigate these issues.\n",
        "\n",
        "7. **What happens when two Gaussian components in a GMM overlap significantly?**\n",
        "   When two Gaussian components overlap significantly, the model might struggle to distinguish between the clusters accurately. This can lead to ambiguous cluster assignments and might require fine-tuning of the model or using other clustering techniques to handle such cases.\n",
        "\n",
        "8. **How sensitive are GMMs to initialization?**\n",
        "   GMMs can be sensitive to initialization, especially when the number of components is not known a priori. Initialization with different seeds can lead to different solutions, which may not always be the global optimum. Multiple initializations and selection based on the best log-likelihood value are commonly used practices.\n",
        "\n",
        "9. **Can GMMs be used for anomaly detection?**\n",
        "   Yes, GMMs are commonly used for anomaly detection. By modeling normal data with a GMM, points that have a low likelihood under the model can be identified as anomalies.\n",
        "\n",
        "10. **What are some extensions of GMMs?**\n",
        "    Some extensions of GMMs include Diagonal Gaussian Mixture Models (DGMMs), which assume diagonal covariance matrices for computational efficiency, and Variational Gaussian Mixture Models (VGMMs), which leverage variational inference techniques for parameter estimation.\n",
        "\n",
        "Remember that Gaussian Mixture Models are just one of many powerful models used in machine learning and statistics, and their effectiveness depends on the nature of the data and the specific problem at hand."
      ],
      "metadata": {
        "id": "QYFNeLNZkSXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "4X455gL9cp-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is the primary goal of a Gaussian Mixture Model?\n",
        "\n",
        "a) Supervised classification  \n",
        "b) Dimensionality reduction  \n",
        "c) Clustering  \n",
        "d) Time series forecasting  \n",
        "\n",
        "**Question 2:** In a Gaussian Mixture Model, what is a \"mixture component\"?\n",
        "\n",
        "a) A single data point  \n",
        "b) A parameter in a Gaussian distribution  \n",
        "c) A cluster with a Gaussian distribution  \n",
        "d) A weight assigned to a data point  \n",
        "\n",
        "**Question 3:** How are the parameters of a Gaussian Mixture Model typically estimated?\n",
        "\n",
        "a) Using the mean and variance of the entire dataset  \n",
        "b) Using gradient descent on a likelihood function  \n",
        "c) Using principal component analysis  \n",
        "d) Using linear regression  \n",
        "\n",
        "**Question 4:** What does the \"Expectation\" step in the EM algorithm for GMMs involve?\n",
        "\n",
        "a) Estimating the parameters of the Gaussian distributions  \n",
        "b) Assigning data points to the most likely mixture component  \n",
        "c) Calculating the log-likelihood of the model  \n",
        "d) Initializing the parameters of the model  \n",
        "\n",
        "**Question 5:** Which statement is true about the covariance matrices in a GMM?\n",
        "\n",
        "a) All components must have the same covariance matrix.  \n",
        "b) Covariance matrices must always be diagonal.  \n",
        "c) Each component can have a different covariance matrix.  \n",
        "d) Covariance matrices are not used in GMMs.  \n",
        "\n",
        "**Question 6:** When is the Kullback-Leibler (KL) divergence used in the context of GMMs?\n",
        "\n",
        "a) To calculate the likelihood of the data given the model  \n",
        "b) To measure the difference between two probability distributions  \n",
        "c) To determine the optimal number of mixture components  \n",
        "d) To initialize the means and variances of the Gaussian components  \n",
        "\n",
        "**Question 7:** What problem is the \"singularity\" issue in GMMs referring to?\n",
        "\n",
        "a) The model converges too slowly during training.  \n",
        "b) The covariance matrix of a component becomes close to singular.  \n",
        "c) The model gets stuck in a local minimum.  \n",
        "d) The model has too few mixture components.  \n",
        "\n",
        "**Question 8:** Which algorithm is commonly used to find the optimal parameters of a GMM?\n",
        "\n",
        "a) K-Means clustering  \n",
        "b) Hierarchical clustering  \n",
        "c) Expectation-Maximization (EM)  \n",
        "d) Principal Component Analysis (PCA)  \n",
        "\n",
        "**Question 9:** In the context of GMMs, what does the term \"log-likelihood\" represent?\n",
        "\n",
        "a) The likelihood of the model parameters given the data  \n",
        "b) The likelihood of the data given the model parameters  \n",
        "c) The difference between the model and actual data  \n",
        "d) The number of iterations needed for the model to converge  \n",
        "\n",
        "**Question 10:** When selecting the number of components in a GMM, what could be a potential strategy?\n",
        "\n",
        "a) Always use a high number of components for accuracy.  \n",
        "b) Use domain knowledge or techniques like the elbow method.  \n",
        "c) Choose a small number of components to save computation time.  \n",
        "d) The number of components is fixed and cannot be changed.  \n",
        "\n",
        "**Answers:**\n",
        "1. c) Clustering\n",
        "2. c) A cluster with a Gaussian distribution\n",
        "3. b) Using gradient descent on a likelihood function\n",
        "4. b) Assigning data points to the most likely mixture component\n",
        "5. c) Each component can have a different covariance matrix.\n",
        "6. b) To measure the difference between two probability distributions.\n",
        "7. b) The covariance matrix of a component becomes close to singular.\n",
        "8. c) Expectation-Maximization (EM)\n",
        "9. b) The likelihood of the data given the model parameters.\n",
        "10. b) Use domain knowledge or techniques like the elbow method."
      ],
      "metadata": {
        "id": "jcXrDByLcrzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "ogJq7fnFuMH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Patient Segmentation**:\n",
        "    - **Objective**: Use GMM to segment patients based on their medical records or lab results into different risk categories.\n",
        "    - **Data**: Electronic health records, Lab test results.\n",
        "\n",
        "2. **Disease Progression Study**:\n",
        "    - **Objective**: Track the progression of a disease in a patient over time and categorize the stages using GMM.\n",
        "    - **Data**: Time-series medical data for chronic diseases like diabetes, hypertension, etc.\n",
        "\n",
        "3. **Medical Image Segmentation**:\n",
        "    - **Objective**: Use GMM to segment medical images, such as MRI or CT scans, into different tissue types or to identify tumors.\n",
        "    - **Data**: Medical imaging datasets, MRI, CT scans.\n",
        "\n",
        "4. **Genomic Data Clustering**:\n",
        "    - **Objective**: Cluster patients based on genomic or proteomic profiles to identify potential disease subtypes or treatment responses.\n",
        "    - **Data**: Genomic sequencing data, Proteomic data.\n",
        "\n",
        "5. **Anomaly Detection for ICU Patients**:\n",
        "    - **Objective**: Use GMM to detect unusual patterns in ICU patient vital signs which might be indicative of an impending medical crisis.\n",
        "    - **Data**: Time-series vital sign data from ICU patients.\n",
        "\n",
        "6. **Pharmacovigilance**:\n",
        "    - **Objective**: Cluster patients based on their reactions to specific drugs and try to identify potential adverse drug reactions.\n",
        "    - **Data**: Patient drug administration records and adverse event reports.\n",
        "\n",
        "7. **Treatment Efficacy Study**:\n",
        "    - **Objective**: Group patients based on how they respond to a particular treatment, which could help in personalizing future treatments.\n",
        "    - **Data**: Treatment records, post-treatment medical assessments.\n",
        "\n",
        "8. **Mental Health Monitoring**:\n",
        "    - **Objective**: Use GMM to cluster patient mood or cognitive scores over time to detect patterns related to mental health conditions like depression.\n",
        "    - **Data**: Patient surveys, cognitive test scores.\n",
        "\n",
        "9. **Predictive Maintenance of Medical Equipment**:\n",
        "    - **Objective**: Based on operational data, cluster medical equipment (like MRI machines) using GMM to identify potential failure or the need for maintenance.\n",
        "    - **Data**: Time-series operational data from medical devices.\n",
        "\n",
        "10. **Biosignal Analysis**:\n",
        "    - **Objective**: Cluster biosignals, like EEG or ECG, to identify patterns related to specific health conditions.\n",
        "    - **Data**: EEG, ECG, or other biosignal recordings.\n",
        "\n"
      ],
      "metadata": {
        "id": "1J_Md5Y9uOUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "LTsYjYd3gEp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through an example of how to implement Gaussian Mixture Models (GMM) using a real-world health dataset. In this example, we'll use the \"Heart Disease UCI\" dataset, which contains various features related to heart health and a target variable indicating the presence of heart disease."
      ],
      "metadata": {
        "id": "vEs9LUvUe9T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the Heart Disease UCI dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "column_names = [\n",
        "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\",\n",
        "    \"fbs\", \"restecg\", \"thalach\", \"exang\",\n",
        "    \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
        "]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "data = data.replace(\"?\", np.nan).dropna()  # Handle missing values\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
        "X = data[features].values\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette score\n",
        "silhouette_scores = []\n",
        "for n_clusters in range(2, 11):\n",
        "    gmm = GaussianMixture(n_components=n_clusters, random_state=0)\n",
        "    cluster_labels = gmm.fit_predict(X_scaled)\n",
        "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "optimal_n_clusters = np.argmax(silhouette_scores) + 2  # Adding 2 to start from n=2\n",
        "\n",
        "# Fit GMM with the optimal number of clusters\n",
        "gmm = GaussianMixture(n_components=optimal_n_clusters, random_state=0)\n",
        "cluster_labels = gmm.fit_predict(X_scaled)\n",
        "\n",
        "# Add the cluster labels to the dataset\n",
        "data[\"cluster\"] = cluster_labels\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster in range(optimal_n_clusters):\n",
        "    cluster_data = data[data[\"cluster\"] == cluster]\n",
        "    plt.scatter(cluster_data[\"age\"], cluster_data[\"thalach\"], label=f'Cluster {cluster}')\n",
        "\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Max Heart Rate (thalach)\")\n",
        "plt.title(\"GMM Clustering of Heart Disease Data\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j-NEwG5Ye1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the \"Heart Disease UCI\" dataset, select relevant features, standardize the data, determine the optimal number of clusters using the silhouette score, fit a GMM with the optimal number of clusters, and visualize the clusters using a scatter plot. The clusters are visualized based on the \"age\" and \"thalach\" (maximum heart rate achieved) features.\n",
        "\n",
        "Remember that choosing the right number of clusters is a crucial step in GMM and clustering in general. In this example, we used the silhouette score to determine the optimal number of clusters, but other methods like the Elbow Method can also be considered."
      ],
      "metadata": {
        "id": "MtYyT32HfEr4"
      }
    }
  ]
}