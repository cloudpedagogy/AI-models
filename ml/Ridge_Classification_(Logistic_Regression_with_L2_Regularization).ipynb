{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORw7LzjTc+l9ZyS84Y3WAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/Ridge_Classification_(Logistic_Regression_with_L2_Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Classification (Logistic Regression with L2 Regularization) Model Background"
      ],
      "metadata": {
        "id": "ZF7Lo6CjBF1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Classification, also known as Logistic Regression with L2 regularization, is a variant of logistic regression that incorporates L2 regularization to prevent overfitting and improve the generalization ability of the model. Regularization is a technique used to add a penalty term to the loss function, discouraging the model from assigning excessively large weights to the features. In Ridge Classification, this penalty is based on the L2 norm of the coefficients.\n",
        "\n",
        "The standard logistic regression model tries to find the optimal coefficients that maximize the likelihood of the training data. However, in situations where the number of features is large relative to the number of training samples or when there is multicollinearity among features, the model might become highly sensitive to noise in the data, leading to overfitting. L2 regularization addresses this issue by adding a penalty term to the loss function that is proportional to the sum of squares of the coefficient values. By doing so, Ridge Classification encourages the model to use smaller coefficient values, reducing the risk of overfitting.\n",
        "\n",
        "**Pros of Ridge Classification**:\n",
        "\n",
        "1. **Reduced Overfitting:** L2 regularization helps prevent overfitting, which is particularly beneficial when dealing with high-dimensional datasets or datasets with multicollinearity.\n",
        "\n",
        "2. **Stability and Robustness:** By shrinking the coefficients towards zero, Ridge Classification provides more stable and robust estimates of the feature importance compared to standard logistic regression.\n",
        "\n",
        "3. **Automatic Feature Selection:** The regularization process effectively reduces the impact of less important features, leading to automatic feature selection. Features that contribute little to the target prediction tend to have smaller coefficients or be effectively ignored.\n",
        "\n",
        "4. **Efficient and Widely Supported:** Ridge Classification is computationally efficient and relatively simple to implement. Many machine learning libraries support it, making it easily accessible.\n",
        "\n",
        "**Cons of Ridge Classification**:\n",
        "\n",
        "1. **Bias Towards Small Coefficients:** The penalty term encourages small coefficient values, which might result in an underestimation of the true effects of some important features.\n",
        "\n",
        "2. **Selection of Regularization Strength:** The regularization parameter (often denoted by 'λ') needs to be chosen carefully. Selecting an inappropriate value could lead to suboptimal performance.\n",
        "\n",
        "3. **Not Suitable for Sparse Feature Selection:** Ridge Classification does not perform explicit feature selection; it shrinks coefficients towards zero but does not set them exactly to zero. For tasks that require explicit feature selection, other methods like LASSO (L1 regularization) are more appropriate.\n",
        "\n",
        "**When to use Ridge Classification**:\n",
        "\n",
        "Ridge Classification is suitable in the following scenarios:\n",
        "\n",
        "1. **High-dimensional Data:** When dealing with datasets that have many features relative to the number of samples, Ridge Classification can help mitigate overfitting.\n",
        "\n",
        "2. **Multicollinearity:** If your features are highly correlated, Ridge Classification can be beneficial in stabilizing the model's estimates.\n",
        "\n",
        "3. **Generalization Improvement:** When your standard logistic regression model exhibits overfitting and you want to improve its generalization ability, Ridge Classification can be a good option.\n",
        "\n",
        "4. **Efficiency and Simplicity:** If you prefer a simple yet effective regularization technique, Ridge Classification is a good choice.\n",
        "\n",
        "It's important to note that Ridge Classification might not be the best choice when you specifically need a sparse model or when you suspect that only a small subset of features is relevant for the classification task. In such cases, LASSO (L1 regularization) or Elastic Net (a combination of L1 and L2 regularization) could be more appropriate options."
      ],
      "metadata": {
        "id": "828_rMW0FU8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "zFwR9MR6bJR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate some random data for classification\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Ridge Logistic Regression model\n",
        "ridge_classifier = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000, random_state=42)\n",
        "ridge_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ridge_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot the decision boundary\n",
        "def plot_decision_boundary(classifier, X, y):\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Ridge Logistic Regression Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(ridge_classifier, X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "wWVPlj5sSFbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "BwNVCtoY0zmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import the required libraries:\n",
        "   - `numpy`: A library for numerical computations in Python.\n",
        "   - `matplotlib.pyplot`: A library for creating visualizations in Python.\n",
        "   - `make_classification` from `sklearn.datasets`: A function to generate synthetic classification datasets.\n",
        "   - `train_test_split` from `sklearn.model_selection`: A function to split data into training and testing sets.\n",
        "   - `LogisticRegression` from `sklearn.linear_model`: A class to create and train a logistic regression model.\n",
        "   - `StandardScaler` from `sklearn.preprocessing`: A class to standardize the features of the dataset.\n",
        "   - `accuracy_score` from `sklearn.metrics`: A function to calculate the accuracy of the model's predictions.\n",
        "\n",
        "2. Generate some random data for classification:\n",
        "   - `make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)`: Generates a synthetic classification dataset with 1000 samples, 2 informative features, and 0 redundant features. The `random_state` ensures reproducibility.\n",
        "\n",
        "3. Split the data into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the dataset (`X` features and `y` labels) into training and testing sets. The test set size is set to 20% of the data, and `random_state` ensures reproducibility.\n",
        "\n",
        "4. Standardize the features for better convergence:\n",
        "   - `StandardScaler()`: Initializes a `StandardScaler` object, which will be used to standardize the features.\n",
        "   - `scaler.fit_transform(X_train)`: Fits the scaler to the training data and transforms the training features. It scales the features such that they have zero mean and unit variance.\n",
        "   - `scaler.transform(X_test)`: Uses the scaler fitted on the training data to transform the test features.\n",
        "\n",
        "5. Create and train the Ridge Logistic Regression model:\n",
        "   - `LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000, random_state=42)`: Initializes a logistic regression model with Ridge regularization (`penalty='l2'`). The `C` parameter controls the regularization strength (smaller values for stronger regularization). The `solver` specifies the optimization algorithm, and `max_iter` sets the maximum number of iterations for convergence.\n",
        "   - `ridge_classifier.fit(X_train_scaled, y_train)`: Trains the logistic regression model using the scaled training data and corresponding labels.\n",
        "\n",
        "6. Make predictions on the test set:\n",
        "   - `ridge_classifier.predict(X_test_scaled)`: Uses the trained model to predict the labels of the scaled test data.\n",
        "\n",
        "7. Calculate accuracy:\n",
        "   - `accuracy_score(y_test, y_pred)`: Compares the predicted labels with the true labels (test set) and calculates the accuracy of the model.\n",
        "\n",
        "8. Plot the decision boundary:\n",
        "   - The `plot_decision_boundary` function takes the trained classifier and the scaled training data (`X_train_scaled` and `y_train`) as arguments and plots the decision boundary of the classifier.\n",
        "   - It uses `numpy.meshgrid` to create a grid of points spanning the feature space, makes predictions for each point, and plots the decision boundary as a filled contour plot.\n",
        "   - The scatter plot shows the original training data points, color-coded according to their labels.\n",
        "\n",
        "9. Display the plot:\n",
        "   - The `plot_decision_boundary` function is called with the trained `ridge_classifier` and the scaled training data, visualizing the decision boundary of the classifier. The decision boundary separates the two classes in the feature space.\n",
        "\n",
        "Overall, this code generates a synthetic classification dataset, splits it into training and testing sets, trains a logistic regression model with Ridge regularization on the scaled training data, evaluates the model's performance on the test set, and plots the decision boundary of the trained classifier."
      ],
      "metadata": {
        "id": "WxzAceq21hqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "sd-Bt0X42IDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of Ridge Classification (Logistic Regression with L2 regularization) in a healthcare setting is predicting the likelihood of a patient having a certain medical condition based on various clinical features or risk factors.\n",
        "\n",
        "Let's consider the example of predicting whether a patient is at risk of developing diabetes. The dataset for this task may include features such as age, body mass index (BMI), blood pressure, cholesterol levels, family history of diabetes, and other relevant health indicators.\n",
        "\n",
        "The steps involved in applying Ridge Classification in this scenario would be as follows:\n",
        "\n",
        "1. Data Collection: Gather data from patients, including their clinical features and whether they have been diagnosed with diabetes or not.\n",
        "\n",
        "2. Data Preprocessing: Clean the data, handle missing values, and perform feature scaling or normalization if required.\n",
        "\n",
        "3. Feature Selection: Choose the most relevant features that are likely to contribute to the prediction of diabetes risk.\n",
        "\n",
        "4. Splitting Data: Divide the dataset into training and testing sets to evaluate the model's performance.\n",
        "\n",
        "5. Ridge Logistic Regression: Train the Ridge Classification model using the training data. Ridge Logistic Regression is similar to standard Logistic Regression but with an added L2 regularization term to penalize large coefficients. This helps prevent overfitting and improves generalization.\n",
        "\n",
        "6. Hyperparameter Tuning: Select the appropriate regularization strength (lambda or alpha) for the Ridge classifier through cross-validation or other techniques.\n",
        "\n",
        "7. Model Evaluation: Evaluate the model's performance on the test dataset using appropriate metrics such as accuracy, precision, recall, F1 score, or ROC-AUC.\n",
        "\n",
        "8. Predictions: Use the trained Ridge Classification model to predict the likelihood of diabetes risk for new, unseen patients.\n",
        "\n",
        "The outcome of this analysis could provide valuable insights to healthcare professionals. For instance, it could help identify patients who are at a higher risk of developing diabetes, allowing doctors to take proactive measures such as recommending lifestyle changes, suggesting regular screenings, or prescribing preventive medications to mitigate the risk.\n",
        "\n",
        "By incorporating L2 regularization, Ridge Classification helps to prevent overfitting, making the model more robust and reliable in real-world healthcare applications where the dataset might be limited or noisy."
      ],
      "metadata": {
        "id": "4bM0kIAMHmHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "IqI1iLGD3kB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Ridge Classification?\n",
        "   Ridge Classification, also known as Logistic Regression with L2 Regularization, is a machine learning algorithm used for binary classification tasks. It combines the logistic regression model with L2 regularization to prevent overfitting and improve generalization.\n",
        "\n",
        "2. How does Ridge Classification differ from standard Logistic Regression?\n",
        "   Standard logistic regression only minimizes the logistic loss function, whereas Ridge Classification adds an L2 regularization term to the loss function. This regularization term penalizes large coefficients, making the model less prone to overfitting.\n",
        "\n",
        "3. What is L2 Regularization in Ridge Classification?\n",
        "   L2 regularization, also known as ridge regularization, adds a penalty term to the logistic regression loss function based on the square of the coefficients. The regularization term controls the model complexity and helps to avoid overfitting.\n",
        "\n",
        "4. Why is Ridge Classification used?\n",
        "   Ridge Classification is used to prevent overfitting in logistic regression models. It's particularly helpful when dealing with high-dimensional data or when there is multicollinearity among the features.\n",
        "\n",
        "5. How is the regularization strength determined in Ridge Classification?\n",
        "   The regularization strength in Ridge Classification is controlled by a hyperparameter called lambda (λ) or alpha (α). Higher values of lambda lead to stronger regularization, while smaller values allow the model to fit the data more closely.\n",
        "\n",
        "6. Can Ridge Classification handle multiclass classification tasks?\n",
        "   Ridge Classification is primarily designed for binary classification problems, but it can be extended to handle multiclass classification using techniques like one-vs-all (OvA) or one-vs-one (OvO) strategies.\n",
        "\n",
        "7. What advantages does Ridge Classification offer over standard Logistic Regression?\n",
        "   Ridge Classification tends to provide better generalization performance on new, unseen data compared to standard logistic regression, especially when the dataset has high dimensionality or multicollinearity.\n",
        "\n",
        "8. Are there any disadvantages to using Ridge Classification?\n",
        "   One potential disadvantage of Ridge Classification is that it might not perform as well as more advanced classifiers, such as support vector machines or neural networks, in complex datasets with intricate patterns.\n",
        "\n",
        "9. Is there a relationship between Ridge Regression and Ridge Classification?\n",
        "   Yes, Ridge Classification is closely related to Ridge Regression. Both methods use L2 regularization, but Ridge Regression is used for linear regression tasks, while Ridge Classification is used for logistic regression and binary classification tasks.\n",
        "\n",
        "10. Can Ridge Classification handle non-linear relationships between features and the target variable?\n",
        "    Ridge Classification is a linear model, so it can only capture linear relationships between features and the target variable. To handle non-linear relationships, one can consider using kernel methods or other non-linear classifiers."
      ],
      "metadata": {
        "id": "HVGBD6Gw79Xw"
      }
    }
  ]
}