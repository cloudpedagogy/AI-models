{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwC9n5U9fP15gMxVroE2dN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/XGBoost_(Extreme_Gradient_Boosting).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost (Extreme Gradient Boosting) Model Background"
      ],
      "metadata": {
        "id": "Tfzm65CcAMqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that belongs to the family of gradient boosting methods. It is widely used for regression, classification, and ranking problems. XGBoost is known for its high performance, scalability, and the ability to handle various types of data efficiently. The algorithm was developed by Tianqi Chen and has gained popularity in both academic research and real-world applications.\n",
        "\n",
        "Here are some of the pros and cons of XGBoost:\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "1. **High Performance**: XGBoost is optimized for speed and performance. It can handle large datasets with millions of samples and features efficiently.\n",
        "\n",
        "2. **Regularization**: The algorithm includes L1 and L2 regularization terms, which help prevent overfitting and improve generalization.\n",
        "\n",
        "3. **Handling Missing Data**: XGBoost can handle missing data by automatically learning the best imputation during the boosting process.\n",
        "\n",
        "4. **Flexibility**: It supports both regression and classification problems and can be used for structured (tabular) and unstructured (e.g., text, images) data.\n",
        "\n",
        "5. **Feature Importance**: XGBoost provides a built-in feature importance mechanism, helping you understand which features are most influential in making predictions.\n",
        "\n",
        "6. **Parallel Processing**: The algorithm can efficiently utilize multi-core CPUs, making it faster during training.\n",
        "\n",
        "7. **Tree Pruning**: XGBoost employs a depth-first approach for growing trees and prunes them if the split is not favorable, leading to a more efficient and accurate model.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "1. **Tuning Complexity**: XGBoost has several hyperparameters that require tuning. Finding the optimal set of hyperparameters can be time-consuming.\n",
        "\n",
        "2. **Memory Usage**: While XGBoost is efficient, it can consume a significant amount of memory, especially for large datasets.\n",
        "\n",
        "3. **Black Box Model**: Like other ensemble methods, XGBoost is a complex model, making it difficult to interpret the internal workings.\n",
        "\n",
        "4. **Data Preprocessing**: Feature engineering and data preprocessing are essential to get the best results with XGBoost. Improper data preparation may lead to suboptimal performance.\n",
        "\n",
        "**When to use XGBoost:**\n",
        "\n",
        "You should consider using XGBoost in the following situations:\n",
        "\n",
        "1. **Large Datasets**: When you have a large dataset with a substantial number of samples and features, XGBoost's efficiency and speed make it a suitable choice.\n",
        "\n",
        "2. **Tabular Data**: XGBoost is well-suited for structured/tabular data, where each row represents an individual sample, and columns are features.\n",
        "\n",
        "3. **High-Dimensional Data**: If you have high-dimensional data (many features), XGBoost's ability to handle such scenarios can be advantageous.\n",
        "\n",
        "4. **Classification and Regression Tasks**: XGBoost is equally effective for classification and regression problems.\n",
        "\n",
        "5. **Ensemble Learning**: When you want to combine multiple weak learners (decision trees) to create a stronger and more accurate model, XGBoost's boosting approach can be highly beneficial.\n",
        "\n",
        "6. **Winning Competitions**: XGBoost has been widely used in machine learning competitions like Kaggle due to its high performance and effectiveness.\n",
        "\n",
        "Remember that while XGBoost can be an excellent choice for many scenarios, it is essential to try different algorithms and compare their performance to ensure the best model for your specific problem. Additionally, hyperparameter tuning is critical for getting the most out of XGBoost, so invest time in optimization when using this algorithm."
      ],
      "metadata": {
        "id": "HnX4pKyB-Nnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "rpZ5zj9sa7o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "BfLDw-4vDVbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset from sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data into DMatrix format (optimized data structure for XGBoost)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define the XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # Multi-class classification\n",
        "    'num_class': len(np.unique(y)),  # Number of classes in the dataset\n",
        "    'eval_metric': 'mlogloss',  # Logarithmic loss for multi-class\n",
        "    'eta': 0.1,  # Learning rate\n",
        "    'max_depth': 3,  # Maximum depth of a tree\n",
        "    'subsample': 0.8,  # Fraction of samples used for fitting the trees\n",
        "    'colsample_bytree': 0.8,  # Fraction of features used for fitting the trees\n",
        "    'seed': 42  # Random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "num_rounds = 100  # Number of boosting rounds (trees)\n",
        "model = xgb.train(params, dtrain, num_rounds)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "# Convert the predicted labels to integers\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display the classification report\n",
        "class_names = data.target_names\n",
        "report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "S2IOqk-xEPb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "SSZiTJhg0Iar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Importing Libraries**: The code starts by importing the required libraries: `pandas` for data handling, `numpy` for numerical operations, `xgboost` for building the XGBoost model, and various modules from `sklearn` for data splitting, evaluation metrics, and classification report.\n",
        "\n",
        "2. **Loading and Preparing Data**: The Iris dataset is loaded using `load_iris()` from `sklearn.datasets`. The features (X) and corresponding labels (y) are extracted from the dataset.\n",
        "\n",
        "3. **Data Splitting**: The data is split into training and testing sets using `train_test_split()` from `sklearn.model_selection`. The test set is set to 20% of the total data, and `random_state=42` ensures reproducibility of the split.\n",
        "\n",
        "4. **Data Preparation for XGBoost**: The training and testing data are converted into the DMatrix format, which is an optimized data structure for XGBoost. The `xgb.DMatrix()` function is used for this purpose, and the DMatrix format is assigned to `dtrain` and `dtest` for training and testing data, respectively.\n",
        "\n",
        "5. **Defining XGBoost Parameters**: The XGBoost model's hyperparameters are defined in the `params` dictionary. Key hyperparameters include `objective` (the objective function for multi-class classification), `num_class` (number of classes in the dataset), `eval_metric` (evaluation metric for multi-class, which is logarithmic loss in this case), `eta` (learning rate), `max_depth` (maximum depth of a tree), `subsample` (fraction of samples used for fitting the trees), `colsample_bytree` (fraction of features used for fitting the trees), and `seed` (random seed for reproducibility).\n",
        "\n",
        "6. **Training the XGBoost Model**: The XGBoost model is trained using the `xgb.train()` function. The training data (`dtrain`), the hyperparameters (`params`), and the number of boosting rounds (`num_rounds`) are passed as inputs to the function. The trained model is stored in the `model` variable.\n",
        "\n",
        "7. **Making Predictions**: The trained XGBoost model is used to make predictions on the test set using the `model.predict()` function. The predicted labels are stored in the `y_pred` variable.\n",
        "\n",
        "8. **Calculating Accuracy**: The accuracy of the model is calculated by comparing the predicted labels (`y_pred`) with the true labels (`y_test`) using the `accuracy_score()` function from `sklearn.metrics`.\n",
        "\n",
        "9. **Displaying Classification Report**: The classification report, including precision, recall, F1-score, and support for each class, is generated using the `classification_report()` function from `sklearn.metrics`. The `target_names` argument is used to specify the names of the classes.\n",
        "\n",
        "10. **Printing Results**: Finally, the accuracy of the model and the classification report are printed to the console.\n",
        "\n",
        "This code demonstrates how to train a simple XGBoost model for multi-class classification using the Iris dataset and evaluate its performance on the test set. It showcases the basic steps involved in training and evaluating an XGBoost model using Python."
      ],
      "metadata": {
        "id": "EVffYqyV56mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "T3-Hfu_11xK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of using XGBoost (Extreme Gradient Boosting) in the healthcare setting is for predicting readmissions of patients with chronic conditions.\n",
        "\n",
        "Chronic diseases like diabetes, heart failure, and chronic obstructive pulmonary disease (COPD) can lead to frequent hospital readmissions, which not only affect the patient's well-being but also result in increased healthcare costs. Hospitals and healthcare providers are keen on finding ways to reduce readmission rates by identifying high-risk patients and providing targeted interventions.\n",
        "\n",
        "XGBoost can be used to develop a predictive model based on historical patient data to determine the likelihood of a patient being readmitted within a specified time frame after discharge. The model can take into account various features, such as:\n",
        "\n",
        "1. Patient demographics (age, gender, etc.)\n",
        "2. Medical history (pre-existing conditions, comorbidities)\n",
        "3. Length of hospital stay during previous admissions\n",
        "4. Medications and treatment plans\n",
        "5. Lab test results\n",
        "6. Socioeconomic factors\n",
        "7. Access to follow-up care and support services\n",
        "\n",
        "The XGBoost model is trained on a labeled dataset that includes past patient records, where the label indicates whether the patient was readmitted within a certain period or not. The algorithm learns to identify patterns and relationships in the data to make accurate predictions on new, unseen patient data.\n",
        "\n",
        "Once the model is developed, healthcare providers can use it to prioritize high-risk patients who might benefit from additional care, personalized treatment plans, or increased follow-up visits to prevent readmissions. This approach helps healthcare institutions allocate their resources more effectively and potentially reduce the burden on emergency departments.\n",
        "\n",
        "It's important to note that such models need to be validated rigorously and carefully before deploying them in real-world clinical settings to ensure their safety, reliability, and compliance with ethical considerations related to patient privacy and consent. Nonetheless, XGBoost and other machine learning techniques hold great promise in improving healthcare outcomes by leveraging data-driven approaches for patient care."
      ],
      "metadata": {
        "id": "xAkXxkHWJ4t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "I3OMsIB-3ONh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is XGBoost, and how does it differ from traditional Gradient Boosting Machines (GBMs)?\n",
        "   XGBoost is an advanced implementation of gradient boosting machines designed for better efficiency and performance. It utilizes a combination of both gradient boosting and regularization techniques, making it more accurate and less prone to overfitting compared to traditional GBMs.\n",
        "\n",
        "2. How does XGBoost handle missing data?\n",
        "   XGBoost has a built-in mechanism to handle missing data during the training process. It automatically learns the best direction to take for missing values, resulting in robustness against missing data.\n",
        "\n",
        "3. What is the difference between XGBoost's \"gbtree\" and \"gblinear\" booster types?\n",
        "   XGBoost supports two booster types: \"gbtree\" and \"gblinear.\" \"gbtree\" uses decision trees as base learners, whereas \"gblinear\" uses linear models as base learners. \"gbtree\" is generally more powerful and suited for most tasks, while \"gblinear\" is useful when dealing with linear relationships in data.\n",
        "\n",
        "4. How does XGBoost handle feature importance?\n",
        "   XGBoost provides a feature importance score based on the number of times each feature is used in the boosting process. The higher the score, the more important the feature is for the model's predictions.\n",
        "\n",
        "5. What is \"early stopping\" in XGBoost, and how does it prevent overfitting?\n",
        "   Early stopping is a technique in XGBoost that halts the training process when the model's performance on the validation dataset stops improving. This helps prevent overfitting by finding the optimal number of boosting rounds, reducing the risk of the model memorizing noise in the data.\n",
        "\n",
        "6. Can XGBoost handle multi-class classification problems?\n",
        "   Yes, XGBoost can handle multi-class classification problems by extending its boosting framework to handle multiple classes.\n",
        "\n",
        "7. What is the role of regularization in XGBoost, and how does it prevent overfitting?\n",
        "   XGBoost incorporates L1 and L2 regularization terms in its objective function to penalize complex models. Regularization helps in preventing overfitting by discouraging the model from being too dependent on any single feature.\n",
        "\n",
        "8. Can XGBoost handle large-scale datasets efficiently?\n",
        "   Yes, XGBoost is designed to handle large-scale datasets efficiently. It has several optimizations, such as approximate tree learning and column block compressed data representation, which make it faster and more memory-efficient than traditional GBMs.\n",
        "\n",
        "9. Is XGBoost suitable for handling structured and unstructured data?\n",
        "   Yes, XGBoost is versatile and can handle both structured (tabular) and unstructured (e.g., text, images) data. For unstructured data, appropriate feature engineering is required before feeding it to XGBoost.\n",
        "\n",
        "10. What are some popular applications of XGBoost in real-world scenarios?\n",
        "    XGBoost is widely used in various domains, including Kaggle competitions, financial modeling, fraud detection, recommendation systems, and natural language processing tasks, among others. Its ability to handle complex datasets and produce accurate predictions makes it a popular choice across diverse applications.\n",
        "\n",
        "Remember that XGBoost's popularity comes from its ability to deliver excellent results across various problem domains, but it is crucial to tune its hyperparameters carefully to achieve the best performance."
      ],
      "metadata": {
        "id": "Es3e9ki39gvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "tS4oApP-jO7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What type of machine learning algorithm is XGBoost?\n",
        "\n",
        "a) Neural Network  \n",
        "b) Support Vector Machine  \n",
        "c) Decision Tree Ensemble  \n",
        "d) K-Means Clustering  \n",
        "\n",
        "**Question 2:** What is the primary objective of XGBoost?\n",
        "\n",
        "a) Minimize the bias of the model  \n",
        "b) Minimize the variance of the model  \n",
        "c) Minimize the loss function by adding weak learners  \n",
        "d) Maximize the accuracy of the model  \n",
        "\n",
        "**Question 3:** Which of the following is NOT a regularization technique used in XGBoost?\n",
        "\n",
        "a) L1 Regularization (Lasso)  \n",
        "b) Dropout  \n",
        "c) L2 Regularization (Ridge)  \n",
        "d) Tree Pruning  \n",
        "\n",
        "**Question 4:** In XGBoost, what is the term used to describe individual decision trees that make up the ensemble?\n",
        "\n",
        "a) Leaf Nodes  \n",
        "b) Child Trees  \n",
        "c) Boosting Units  \n",
        "d) Weak Learners  \n",
        "\n",
        "**Question 5:** How does XGBoost handle missing values?\n",
        "\n",
        "a) It ignores the rows with missing values  \n",
        "b) It fills missing values with the median of the feature  \n",
        "c) It uses a separate branch to handle missing values  \n",
        "d) It replaces missing values with 0  \n",
        "\n",
        "**Question 6:** What is \"Gradient Boosting\" in the context of XGBoost?\n",
        "\n",
        "a) A method to boost the learning rate of the model  \n",
        "b) A technique to enhance the color contrast in visualizations  \n",
        "c) An optimization algorithm that updates the weights of features  \n",
        "d) A boosting algorithm that combines weak learners sequentially  \n",
        "\n",
        "**Question 7:** What is the purpose of the \"XG\" in XGBoost?\n",
        "\n",
        "a) It stands for \"Extreme Gradient,\" emphasizing its boosted nature  \n",
        "b) It is named after the developer's initials  \n",
        "c) It refers to the fact that it works exceptionally well with large datasets  \n",
        "d) It's an acronym for \"eXtensible Gradient\"  \n",
        "\n",
        "**Question 8:** Which evaluation metric is commonly used with XGBoost for regression problems?\n",
        "\n",
        "a) Accuracy  \n",
        "b) F1 Score  \n",
        "c) R-squared (Coefficient of Determination)  \n",
        "d) Precision  \n",
        "\n",
        "**Question 9:** Which of the following statements about parallelization in XGBoost is true?\n",
        "\n",
        "a) XGBoost doesn't support parallel processing  \n",
        "b) XGBoost can only parallelize the training process, not the prediction process  \n",
        "c) XGBoost can parallelize both the training and prediction processes  \n",
        "d) Parallelization in XGBoost is only available for GPU processing  \n",
        "\n",
        "**Question 10:** What is early stopping in XGBoost?\n",
        "\n",
        "a) Stopping the model training process when the learning rate is too high  \n",
        "b) Terminating the training process if the validation loss doesn't improve for a certain number of rounds  \n",
        "c) Halting the model training when the regularization parameter is too large  \n",
        "d) Ending the training process as soon as the model reaches 100% accuracy  \n",
        "\n",
        "**Answers:**\n",
        "\n",
        "1. c) Decision Tree Ensemble\n",
        "2. c) Minimize the loss function by adding weak learners\n",
        "3. b) Dropout\n",
        "4. d) Weak Learners\n",
        "5. c) It uses a separate branch to handle missing values\n",
        "6. d) A boosting algorithm that combines weak learners sequentially\n",
        "7. a) It stands for \"Extreme Gradient,\" emphasizing its boosted nature\n",
        "8. c) R-squared (Coefficient of Determination)\n",
        "9. c) XGBoost can parallelize both the training and prediction processes\n",
        "10. b) Terminating the training process if the validation loss doesn't improve for a certain number of rounds"
      ],
      "metadata": {
        "id": "IyKIV1RvjQed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "I1RgN0tx3NmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Prediction**\n",
        "   - **Diabetes Prediction**: Use the Pima Indian Diabetes Dataset to predict the onset of diabetes based on diagnostic measures.\n",
        "   - **Heart Disease Prediction**: Utilize datasets with cardiovascular parameters to predict the likelihood of a patient getting a heart disease.\n",
        "   \n",
        "2. **Readmission Prediction**\n",
        "   - Predict the likelihood of a patient being readmitted to a hospital within 30 days based on their initial medical records and treatment received.\n",
        "\n",
        "3. **Disease Progression**\n",
        "   - **Cancer Progression**: Use genetic and clinical data to predict how quickly a certain type of cancer might progress or metastasize.\n",
        "   - **Chronic Kidney Disease Progression**: Utilize patient records to predict the progression rate of chronic kidney disease.\n",
        "\n",
        "4. **Medical Imaging**\n",
        "   - **Breast Cancer Detection in Mammograms**: Use feature extraction methods on mammograms and then employ XGBoost to differentiate between benign and malignant tumors.\n",
        "   - **Diabetic Retinopathy Detection**: Analyze retinal images to predict the stages of diabetic retinopathy.\n",
        "\n",
        "5. **Genomic Data Analysis**\n",
        "   - **Predicting Disease Susceptibility**: Use genetic data to predict susceptibility to certain diseases.\n",
        "   - **Drug Response**: Analyze patient genetic data to predict how they might respond to certain medications.\n",
        "\n",
        "6. **Treatment Optimization**\n",
        "   - Predict the most effective treatment pathway for patients with complex conditions based on their medical history, genetic data, and other relevant parameters.\n",
        "\n",
        "7. **Cost Prediction**\n",
        "   - Predict the cost of treatment for patients with certain conditions, aiding in healthcare management and insurance premium forecasting.\n",
        "\n",
        "8. **Mortality Prediction**\n",
        "   - Utilize ICU data to predict the likelihood of mortality in critically ill patients.\n",
        "\n",
        "9. **Mental Health Predictions**\n",
        "   - **Depression Onset Prediction**: Use patient data to predict the likelihood of a patient developing depression in the near future.\n",
        "   - **Treatment Outcome**: Predict how well a patient with mental health issues might respond to different treatments.\n",
        "\n",
        "10. **Patient No-show Prediction**\n",
        "   - Predict whether a patient will show up for their scheduled appointment based on historical data and other patient-specific variables.\n",
        "\n",
        "11. **Epidemic Outbreak Predictions**\n",
        "   - Use XGBoost to predict potential outbreaks of diseases in specific areas based on current case numbers, mobility data, and other relevant parameters.\n",
        "\n",
        "12. **Wearable Health Devices**\n",
        "   - Analyze data from wearables (like heart rate, sleep patterns, and activity levels) to predict potential health risks or deteriorations in a patientâ€™s health.\n"
      ],
      "metadata": {
        "id": "ctvLxhCF3PcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "Z-6iJj2kEgbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a working example of training an XGBoost model using real-world health data. In this example, we'll use the famous \"Diabetes\" dataset from the scikit-learn library, which contains ten baseline variables, age, sex, BMI, average blood pressure, and six blood serum measurements for 442 diabetes patients. The target variable is a quantitative measure of disease progression one year after baseline.\n",
        "\n"
      ],
      "metadata": {
        "id": "gU1XNYDg5Mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the diabetes dataset from scikit-learn\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=100,  # Number of boosting rounds (trees)\n",
        "    learning_rate=0.1,  # Step size shrinkage to prevent overfitting\n",
        "    max_depth=3,  # Maximum depth of a tree\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) to evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "ArWDZk7v4-uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we're using the Diabetes dataset to train an XGBoost regression model. We split the data into training and testing sets, create an instance of the XGBoost regressor, train the model on the training data, and then make predictions on the test set. Finally, we calculate the Mean Squared Error (MSE) to evaluate the model's performance.\n",
        "\n",
        "Remember that in a real-world scenario, you would use your own health dataset, preprocess it accordingly, and potentially tune the hyperparameters of the XGBoost model to achieve the best possible results."
      ],
      "metadata": {
        "id": "OM46RMb75RUQ"
      }
    }
  ]
}