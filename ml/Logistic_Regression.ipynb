{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcfN9PxCcoXUXAz7SsswPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model Background"
      ],
      "metadata": {
        "id": "dVON7gPT_JVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical method used for binary classification problems, where the goal is to predict the probability that an instance belongs to a particular class. Despite its name, logistic regression is primarily a classification algorithm, not a regression algorithm. It works by modeling the relationship between the input features and the probability of the target variable being in one of two possible classes (e.g., yes/no, 0/1).\n",
        "\n",
        "The logistic regression model uses the logistic function (also known as the sigmoid function) to transform the output of a linear regression into the range [0, 1]. This output represents the probability of the instance belonging to one class, and the probability of it belonging to the other class is simply 1 minus the first probability.\n",
        "\n",
        "**Pros of Logistic Regression**:\n",
        "\n",
        "1. Simplicity: Logistic regression is relatively simple and easy to understand. It's a linear algorithm that can be implemented quickly.\n",
        "\n",
        "2. Interpretability: The model's coefficients can be interpreted to understand the influence of each feature on the predicted probability.\n",
        "\n",
        "3. Probabilistic Output: Logistic regression provides a probability score for the predicted class, which can be useful when you need to assess uncertainty.\n",
        "\n",
        "4. Works well with small datasets: Logistic regression can perform well even with limited data compared to more complex algorithms.\n",
        "\n",
        "5. Low computational cost: Training and making predictions with logistic regression are computationally efficient, making it suitable for large datasets.\n",
        "\n",
        "**Cons of Logistic Regression**:\n",
        "\n",
        "1. Limited to binary classification: Logistic regression is designed for binary classification problems and may not work well with multi-class classification tasks without modifications.\n",
        "\n",
        "2. Assumption of linearity: Logistic regression assumes a linear relationship between the features and the log-odds of the target variable, which might not hold true in all cases.\n",
        "\n",
        "3. Sensitive to outliers: Logistic regression can be sensitive to outliers, which might negatively impact its performance.\n",
        "\n",
        "4. May not handle complex relationships: For problems with highly nonlinear decision boundaries, logistic regression might not be the best choice.\n",
        "\n",
        "**When to use Logistic Regression**:\n",
        "\n",
        "Logistic Regression is a good choice under the following circumstances:\n",
        "\n",
        "1. Binary classification: When you have a binary classification problem, where you need to predict whether an instance belongs to one of two classes.\n",
        "\n",
        "2. Interpretable results: When you need to understand the contribution of individual features and their impact on the outcome.\n",
        "\n",
        "3. Linearly separable data: When the data is relatively well-separated by a linear decision boundary.\n",
        "\n",
        "4. Low computational resources: When computational resources are limited, as logistic regression is computationally efficient.\n",
        "\n",
        "However, if your problem involves multiple classes or has complex relationships between features and the target, you might consider other algorithms like Support Vector Machines (SVM), Random Forests, Gradient Boosting, or deep learning models like Neural Networks."
      ],
      "metadata": {
        "id": "M-ivCyzd7W3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "HkYHFaM1Ajkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load or create your dataset (replace this with your own data)\n",
        "# For example, if you have a CSV file named 'data.csv':\n",
        "# data = pd.read_csv('data.csv')\n",
        "# X = data.drop('target_column', axis=1)\n",
        "# y = data['target_column']\n",
        "\n",
        "# For demonstration purposes, let's create a synthetic dataset\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "num_features = 5\n",
        "X = np.random.randn(num_samples, num_features)\n",
        "coefficients = np.array([1.5, -2, 0.5, 0, 0])\n",
        "intercept = 1\n",
        "noise = 0.5 * np.random.randn(num_samples)\n",
        "y = (X.dot(coefficients) + intercept + noise) > 0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "id": "gXR-aXN5vuCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "8HeUYS_mGlH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import necessary libraries:\n",
        "   - `numpy`: Used for numerical operations and generating synthetic data.\n",
        "   - `pandas`: Used for data manipulation and handling the dataset (though in this case, we generate synthetic data).\n",
        "   - `train_test_split` from `sklearn.model_selection`: Used to split the data into training and testing sets.\n",
        "   - `LogisticRegression` from `sklearn.linear_model`: Used to create and train a logistic regression model.\n",
        "   - `accuracy_score`, `classification_report`, and `confusion_matrix` from `sklearn.metrics`: Used to evaluate the performance of the model.\n",
        "\n",
        "2. Create a synthetic dataset (for demonstration purposes):\n",
        "   - `np.random.seed(42)`: Sets the random seed for reproducibility.\n",
        "   - `num_samples`: Number of data samples to generate.\n",
        "   - `num_features`: Number of features in each sample.\n",
        "   - `X`: A randomly generated array of shape `(num_samples, num_features)` containing features.\n",
        "   - `coefficients`: An array of coefficients used to generate the target labels.\n",
        "   - `intercept`: An intercept term added to the target labels.\n",
        "   - `noise`: Random noise added to the target labels.\n",
        "   - `y`: Binary target labels generated based on the dot product of features and coefficients, with added noise. If the result is greater than 0, the label is True (1), otherwise False (0).\n",
        "\n",
        "3. Split the data into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data (features `X` and target labels `y`) into training and testing sets. The test set size is 20% of the total data, and the random seed is set to 42 for reproducibility.\n",
        "\n",
        "4. Create and train the Logistic Regression model:\n",
        "   - `model = LogisticRegression()`: Creates an instance of the logistic regression model.\n",
        "   - `model.fit(X_train, y_train)`: Trains the logistic regression model on the training data (`X_train` and `y_train`).\n",
        "\n",
        "5. Make predictions on the test set:\n",
        "   - `y_pred = model.predict(X_test)`: Uses the trained model to predict the target labels for the test set (`X_test`).\n",
        "\n",
        "6. Evaluate the model:\n",
        "   - `accuracy = accuracy_score(y_test, y_pred)`: Computes the accuracy of the model's predictions compared to the true labels from the test set.\n",
        "   - `conf_matrix = confusion_matrix(y_test, y_pred)`: Generates a confusion matrix that provides information about the true positive, true negative, false positive, and false negative predictions.\n",
        "   - `class_report = classification_report(y_test, y_pred)`: Generates a classification report containing precision, recall, F1-score, and support for each class.\n",
        "\n",
        "7. Print the evaluation metrics:\n",
        "   - `print(\"Accuracy:\", accuracy)`: Prints the accuracy of the model's predictions on the test set.\n",
        "   - `print(\"Confusion Matrix:\\n\", conf_matrix)`: Prints the confusion matrix, showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
        "   - `print(\"Classification Report:\\n\", class_report)`: Prints the classification report containing precision, recall, F1-score, and support for each class.\n",
        "\n",
        "The code demonstrates how to create a synthetic dataset, split it into training and testing sets, train a logistic regression model, make predictions, and evaluate the model's performance using accuracy, confusion matrix, and classification report."
      ],
      "metadata": {
        "id": "oBVEx5zIvpG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "OEwTOCUSR092"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of using logistic regression in a healthcare setting to predict the likelihood of a patient having a heart attack based on certain risk factors and medical history.\n",
        "\n",
        "**Problem Statement:**\n",
        "The healthcare facility wants to identify patients who are at a higher risk of experiencing a heart attack in the next year. They have collected data on various risk factors and medical history of a group of patients, along with information about whether they had a heart attack in the past year.\n",
        "\n",
        "**Dataset:**\n",
        "The dataset contains the following features for each patient:\n",
        "1. Age: Age of the patient (numeric).\n",
        "2. Gender: Gender of the patient (categorical: male or female).\n",
        "3. Blood Pressure: Systolic blood pressure of the patient (numeric).\n",
        "4. Cholesterol Level: Cholesterol level of the patient (numeric).\n",
        "5. Diabetes: Whether the patient has diabetes (binary: 1 for yes, 0 for no).\n",
        "6. Smoking: Whether the patient is a smoker (binary: 1 for yes, 0 for no).\n",
        "7. Family History: Whether there is a family history of heart disease (binary: 1 for yes, 0 for no).\n",
        "8. Heart Attack: Whether the patient had a heart attack in the past year (binary: 1 for yes, 0 for no).\n",
        "\n",
        "**Objective:**\n",
        "Build a logistic regression model that takes the patient's features as input and predicts the probability of the patient having a heart attack in the next year (Heart Attack = 1).\n",
        "\n",
        "**Steps:**\n",
        "1. Data Collection: Collect data on the features mentioned above from a sample of patients who visited the healthcare facility.\n",
        "\n",
        "2. Data Preprocessing: Preprocess the data by handling missing values, encoding categorical variables (e.g., one-hot encoding for gender), and splitting the data into training and testing sets.\n",
        "\n",
        "3. Model Training: Use the logistic regression algorithm to train the model on the training data. The model will learn the relationships between the input features and the likelihood of a heart attack.\n",
        "\n",
        "4. Model Evaluation: Evaluate the performance of the trained logistic regression model using metrics like accuracy, precision, recall, and F1-score on the test data. These metrics will provide insights into how well the model is predicting the occurrence of heart attacks.\n",
        "\n",
        "5. Interpretation: Examine the coefficients of the logistic regression model to understand the impact of each feature on the prediction. Positive coefficients indicate that an increase in the feature's value is associated with an increased risk of a heart attack, while negative coefficients indicate the opposite.\n",
        "\n",
        "6. Prediction: Once the logistic regression model is trained and evaluated, it can be used to predict the probability of a heart attack for new patients based on their features. The predicted probabilities can be used to prioritize patients for further screenings or interventions.\n",
        "\n",
        "7. Deployment: Integrate the trained logistic regression model into the healthcare facility's system so that it can be used in real-time to assess the risk of heart attacks for incoming patients.\n",
        "\n",
        "**Important Note:**\n",
        "While logistic regression can provide valuable insights and predictions, it is crucial to remember that healthcare decisions should not be solely based on the predictions of a single model. Predictive models should be used as tools to assist healthcare professionals in making informed decisions. It is essential to consider a holistic approach that combines medical expertise, patient history, and other diagnostic tests in making healthcare decisions."
      ],
      "metadata": {
        "id": "nh237JenD7Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "vQt1zqrMYkNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "- Logistic Regression is a type of statistical model used for binary classification tasks. Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts the probability of an input belonging to a particular class (usually 0 or 1). It uses the logistic function (sigmoid) to map the output to a probability range between 0 and 1.\n",
        "\n",
        "2. Why is it called \"Logistic\" Regression?\n",
        "- The name \"Logistic Regression\" comes from the logistic function used in the model. The logistic function is a sigmoid curve that maps any real-valued number to a value between 0 and 1, which makes it suitable for estimating probabilities in binary classification problems.\n",
        "\n",
        "3. Can Logistic Regression handle multi-class classification tasks?\n",
        "- While Logistic Regression is inherently designed for binary classification, it can be extended to handle multi-class problems using techniques like one-vs-all (also known as one-vs-rest) or softmax regression. One-vs-all involves training multiple binary classifiers for each class, while softmax regression generalizes the concept of logistic regression to multi-class scenarios.\n",
        "\n",
        "4. How does Logistic Regression make predictions based on probabilities?\n",
        "- After computing the probability of an input belonging to class 1 (p), Logistic Regression makes a decision based on a threshold value (usually 0.5). If p is greater than or equal to the threshold, the input is classified as class 1; otherwise, it is classified as class 0.\n",
        "\n",
        "5. Is the decision boundary always linear in Logistic Regression?\n",
        "- No, the decision boundary of Logistic Regression can be linear or non-linear, depending on the data and feature transformation. In simple cases, it may be linear, but when features are combined or transformed, the decision boundary can become non-linear.\n",
        "\n",
        "6. What are the advantages of Logistic Regression?\n",
        "- Logistic Regression is simple, interpretable, and computationally efficient. It works well with small to medium-sized datasets and can handle high-dimensional feature spaces.\n",
        "\n",
        "7. What are the limitations of Logistic Regression?\n",
        "- Logistic Regression may not perform well on complex datasets with non-linear relationships between features and target. It can also be sensitive to outliers and multicollinearity between features.\n",
        "\n",
        "8. Is feature scaling necessary for Logistic Regression?\n",
        "- Feature scaling is not always required for Logistic Regression. It depends on the optimization algorithm used. Gradient-based methods like gradient descent may benefit from feature scaling to improve convergence, but other algorithms like Newton-Raphson may not require it.\n",
        "\n",
        "9. Can Logistic Regression handle missing values in the dataset?\n",
        "- Missing values can be handled in Logistic Regression by either removing the samples with missing values or imputing them with appropriate values (e.g., mean, median, etc.) before training the model.\n",
        "\n",
        "10. What are some real-world applications of Logistic Regression?\n",
        "- Logistic Regression is widely used in various applications, including spam email detection, disease diagnosis, credit risk analysis, sentiment analysis, and predicting customer churn in businesses."
      ],
      "metadata": {
        "id": "4Le3TxIo5sUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "dt4RaIccgT-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What type of problem is Logistic Regression specifically designed to solve?\n",
        "a) Regression problems\n",
        "b) Classification problems\n",
        "c) Clustering problems\n",
        "d) Dimensionality reduction problems\n",
        "\n",
        "**Question 2:** In Logistic Regression, what is the output of the model?\n",
        "a) A continuous numerical value\n",
        "b) A probability score between 0 and 1\n",
        "c) A cluster label\n",
        "d) An angle between vectors\n",
        "\n",
        "**Question 3:** What is the name of the function used to model the probability distribution in Logistic Regression?\n",
        "a) Sigmoid function\n",
        "b) Step function\n",
        "c) Exponential function\n",
        "d) Logarithmic function\n",
        "\n",
        "**Question 4:** What is the cost function commonly used for training Logistic Regression models?\n",
        "a) Mean Squared Error (MSE)\n",
        "b) Mean Absolute Error (MAE)\n",
        "c) Cross-Entropy Loss (Log Loss)\n",
        "d) Hinge Loss\n",
        "\n",
        "**Question 5:** Which technique is used to update the model's weights during training in Logistic Regression?\n",
        "a) Gradient Descent\n",
        "b) K-means clustering\n",
        "c) Random Forest\n",
        "d) Principal Component Analysis (PCA)\n",
        "\n",
        "**Question 6:** What is the primary goal when training a Logistic Regression model?\n",
        "a) Minimize accuracy\n",
        "b) Maximize precision\n",
        "c) Maximize the log likelihood or minimize the loss function\n",
        "d) Minimize the number of features\n",
        "\n",
        "**Question 7:** In a binary classification problem, how many coefficients (weights) does the Logistic Regression model need?\n",
        "a) One for each class\n",
        "b) Two for each class\n",
        "c) Only one, regardless of the number of classes\n",
        "d) The number varies based on the number of features\n",
        "\n",
        "**Question 8:** Which of the following techniques can help prevent overfitting in a Logistic Regression model?\n",
        "a) Increasing the learning rate\n",
        "b) Adding more features\n",
        "c) Reducing the number of iterations\n",
        "d) Regularization techniques like L2 regularization\n",
        "\n",
        "**Question 9:** What is the key assumption of independence made by Logistic Regression?\n",
        "a) Features are independent of each other.\n",
        "b) The model is independent of the data distribution.\n",
        "c) The response variable is normally distributed.\n",
        "d) Observations are independent and identically distributed.\n",
        "\n",
        "**Question 10:** When making predictions using a trained Logistic Regression model, how do you determine the class label for a given input?\n",
        "a) Choose the class with the highest probability\n",
        "b) Choose the class with the lowest probability\n",
        "c) Choose the class with the most features\n",
        "d) Choose any class at random\n",
        "\n",
        "**Answers:**\n",
        "1. b) Classification problems\n",
        "2. b) A probability score between 0 and 1\n",
        "3. a) Sigmoid function\n",
        "4. c) Cross-Entropy Loss (Log Loss)\n",
        "5. a) Gradient Descent\n",
        "6. c) Maximize the log likelihood or minimize the loss function\n",
        "7. c) Only one, regardless of the number of classes\n",
        "8. d) Regularization techniques like L2 regularization\n",
        "9. d) Observations are independent and identically distributed.\n",
        "10. a) Choose the class with the highest probability"
      ],
      "metadata": {
        "id": "3I6MI5jYgVYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "vL82wzVdzBS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Prediction:**\n",
        "    - **Diabetes Mellitus Prediction:** Use a dataset with patient information (age, BMI, family history, blood tests) to predict the onset of diabetes.\n",
        "    - **Heart Disease Identification:** Utilize factors like cholesterol levels, age, blood pressure, and smoking habits to predict the likelihood of heart disease.\n",
        "  \n",
        "2. **Patient Readmission:**\n",
        "    - **Hospital Readmission Rates:** Based on variables like patient age, initial diagnosis, length of previous stay, and number of previous admissions, predict if a patient will be readmitted within 30 days.\n",
        "\n",
        "3. **Mortality Prediction:**\n",
        "    - **ICU Mortality Prediction:** Use patient data from the first 24 hours in the ICU (vitals, lab results, diagnosis) to predict mortality rates.\n",
        "\n",
        "4. **Medical Imaging:**\n",
        "    - **Mammogram Result Prediction:** Using features extracted from mammogram images, as well as patient age and history, predict the likelihood of breast cancer.\n",
        "\n",
        "5. **Treatment Efficacy:**\n",
        "    - **Response to Treatment:** For patients with a certain condition (e.g., hypertension), predict their response to treatment based on their individual characteristics and the type of medication provided.\n",
        "\n",
        "6. **Surgical Outcome Prediction:**\n",
        "    - **Success of Surgery:** Using patient data and type of surgery, predict the likelihood of surgical complications.\n",
        "\n",
        "7. **Risk Modelling:**\n",
        "    - **Fall Risk in Elderly Patients:** Use patient data like age, diagnosis, medications, and physical abilities to predict the likelihood of falls in a hospital setting.\n",
        "    - **Risk of Opioid Addiction:** Based on prescription patterns and patient history, predict which patients are at a higher risk of developing an opioid addiction.\n",
        "\n",
        "8. **Mental Health Predictions:**\n",
        "    - **Depression Diagnosis:** Based on survey results or electronic health record notes, predict which patients are at risk for or currently have depression.\n",
        "  \n",
        "9. **Patient Compliance:**\n",
        "    - **Medication Adherence:** Using patient demographics, past adherence rates, and the type of medication prescribed, predict the likelihood that a patient will follow their medication regimen.\n",
        "  \n",
        "10. **Cost Prediction:**\n",
        "    - **Medical Expenditure Prediction:** Predict the likely medical expenses for patients in the next year based on their medical history, demographics, and other variables.\n",
        "\n",
        "11. **Special Population Predictions:**\n",
        "    - **Neonatal ICU Outcome Prediction:** Using data about neonatal health, maternal health, and birth details, predict outcomes for babies in the NICU.\n",
        "  \n",
        "12. **Epidemiology and Public Health:**\n",
        "    - **Disease Outbreak Prediction:** Use regional healthcare data to predict the likelihood of a disease outbreak, such as the flu or a new emerging illness.\n",
        "\n"
      ],
      "metadata": {
        "id": "9KYSZUGVzFZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "01Nsf2OODhNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a working example of a Logistic Regression model using a real-world health dataset. In this example, I'll use the famous \"Pima Indians Diabetes Database\" dataset, which contains various health measurements of Pima Indian women along with their diabetes status."
      ],
      "metadata": {
        "id": "XzvhBkqqwJ4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunc', 'Age', 'Outcome']\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "OhWQAz0Fvyco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indians Diabetes Database from a URL, split it into features (X) and the target variable (y), preprocess the data by standardizing it, create a Logistic Regression model, train it on the training data, make predictions on the test data, and finally, evaluate the model's performance using accuracy, confusion matrix, and classification report.\n",
        "\n",
        "Remember to replace the URL with the appropriate path if you have downloaded the dataset locally. Also, you can modify and customize the code according to your needs or experiment with different preprocessing techniques and hyperparameters."
      ],
      "metadata": {
        "id": "QpmvSB4TwO8v"
      }
    }
  ]
}