{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlmH9Zz5N0pzt4Dubh3QIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Principal_Component_Analysis_(PCA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) Model Background"
      ],
      "metadata": {
        "id": "J7jNDD68BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a widely used technique in the field of statistics and data analysis. It is a dimensionality reduction method that transforms a set of correlated variables into a new set of uncorrelated variables, called principal components. These components are ordered in such a way that the first principal component explains the largest variance in the data, the second component explains the second-largest variance, and so on.\n",
        "\n",
        "**Here's how PCA works**:\n",
        "\n",
        "1. Standardize the data: If the variables in the dataset have different scales, PCA first standardizes the data to give each variable an equal importance in the analysis.\n",
        "\n",
        "2. Calculate the covariance matrix: PCA calculates the covariance matrix of the standardized data, which represents the relationships between the variables.\n",
        "\n",
        "3. Compute the eigenvectors and eigenvalues: PCA then calculates the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate the magnitude of variance explained by each principal component.\n",
        "\n",
        "4. Select the principal components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The top-k eigenvectors (principal components) are selected to capture the most significant variance in the data.\n",
        "\n",
        "5. Transform the data: Finally, the original data is projected onto the selected principal components to create a new feature space.\n",
        "\n",
        "**Pros of PCA**:\n",
        "1. Dimensionality reduction: PCA helps reduce the number of variables, making the data easier to visualize, analyze, and process, especially when dealing with high-dimensional datasets.\n",
        "\n",
        "2. Feature extraction: PCA transforms the original variables into new, uncorrelated variables (principal components) that are linear combinations of the original features, potentially revealing underlying patterns in the data.\n",
        "\n",
        "3. Noise reduction: By focusing on the components with high variance, PCA can remove noise and retain the most informative aspects of the data.\n",
        "\n",
        "4. Interpretability: In some cases, the principal components might have clear interpretations, allowing for a better understanding of the dominant factors influencing the data.\n",
        "\n",
        "**Cons of PCA**:\n",
        "1. Information loss: The main drawback of PCA is that it reduces data dimensionality by eliminating the components with low variance, which may lead to some loss of information.\n",
        "\n",
        "2. Interpretability challenges: While some principal components might be interpretable, others may not have any straightforward meaning, making it difficult to explain the results.\n",
        "\n",
        "3. Sensitivity to outliers: PCA is sensitive to outliers as they can disproportionately affect the covariance matrix and, consequently, the principal components.\n",
        "\n",
        "4. Non-linear relationships: PCA is effective for finding linear relationships between variables. If the data contains complex non-linear relationships, PCA may not capture them well.\n",
        "\n",
        "**When to use PCA**:\n",
        "PCA is a useful technique in various scenarios:\n",
        "\n",
        "1. Dimensionality reduction: When dealing with high-dimensional datasets and computational resources are limited, PCA can be employed to reduce the number of features while preserving most of the variability.\n",
        "\n",
        "2. Visualization: PCA can be used to project high-dimensional data into a lower-dimensional space (e.g., 2D or 3D) for visualization purposes.\n",
        "\n",
        "3. Data preprocessing: PCA can be utilized as a preprocessing step before applying other machine learning algorithms to reduce noise, improve performance, and prevent overfitting.\n",
        "\n",
        "4. Identifying important features: By analyzing the principal components, you can identify the most influential features in the dataset.\n",
        "\n",
        "5. Collinearity detection: PCA can help detect multicollinearity among variables, which is essential when performing regression analysis.\n",
        "\n",
        "However, it's important to note that PCA might not always be the best choice, especially when non-linear relationships are present, or when interpretability of the components is crucial. In such cases, other dimensionality reduction techniques or domain-specific approaches may be more suitable."
      ],
      "metadata": {
        "id": "swYHvHDH_mR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "1Yn-TgaLbUHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate sample data for demonstration\n",
        "np.random.seed(42)\n",
        "data = np.random.rand(100, 4)  # 100 samples, 4 features\n",
        "\n",
        "# Create a Pandas DataFrame for the data\n",
        "df = pd.DataFrame(data, columns=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4'])\n",
        "\n",
        "# Standardize the data (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Create a new DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "\n",
        "# Plot the original data and the PCA results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['Feature 1'], df['Feature 2'], label='Original Data')\n",
        "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], label='PCA Result')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('PCA Example')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r3Hkyu4eGg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "ha9OjD101Rid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import necessary libraries:**\n",
        "   The code starts by importing the required libraries: `numpy` (as np), `pandas` (as pd), `matplotlib.pyplot` (as plt), `PCA` from `sklearn.decomposition`, and `StandardScaler` from `sklearn.preprocessing`. These libraries will be used for data manipulation, visualization, and performing PCA.\n",
        "\n",
        "2. **Generate sample data for demonstration:**\n",
        "   The code generates a 100x4 NumPy array called `data` using `np.random.rand(100, 4)`. This represents 100 samples with 4 features each. The data is random and is used to demonstrate the PCA process.\n",
        "\n",
        "3. **Create a Pandas DataFrame for the data:**\n",
        "   The data is converted into a Pandas DataFrame called `df`. Each column in the DataFrame corresponds to one of the four features, and the columns are labeled 'Feature 1', 'Feature 2', 'Feature 3', and 'Feature 4'.\n",
        "\n",
        "4. **Standardize the data (mean=0, std=1):**\n",
        "   PCA is sensitive to the scale of the features, so it is essential to standardize the data before performing PCA. The code creates a `StandardScaler` object called `scaler` and then uses it to transform the DataFrame `df` into `scaled_data`, where each feature has a mean of 0 and standard deviation of 1.\n",
        "\n",
        "5. **Perform PCA:**\n",
        "   The code creates a `PCA` object called `pca` and specifies `n_components=2`. This means that PCA will reduce the dimensionality of the data to 2 principal components. The `fit_transform()` method is then called on `scaled_data`, and the result is stored in `pca_result`.\n",
        "\n",
        "6. **Create a new DataFrame for the PCA results:**\n",
        "   The `pca_result` array is converted into another Pandas DataFrame called `pca_df`. This new DataFrame contains the principal components resulting from the PCA, labeled as 'Principal Component 1' and 'Principal Component 2'.\n",
        "\n",
        "7. **Plot the original data and the PCA results:**\n",
        "   The code creates a scatter plot to visualize the original data and the PCA results side by side. The original data from `df` is plotted with 'Feature 1' on the x-axis and 'Feature 2' on the y-axis. The PCA results from `pca_df` are plotted with 'Principal Component 1' on the x-axis and 'Principal Component 2' on the y-axis. The plot shows how the data is transformed after PCA.\n",
        "\n",
        "8. **Labels and Visualization:**\n",
        "   The plot is labeled with appropriate x and y axis labels and given a title. It also includes a legend to differentiate between the original data and the PCA results.\n",
        "\n",
        "9. **Show the plot:**\n",
        "   The `plt.show()` function is called to display the generated plot.\n",
        "\n",
        "In summary, this code generates sample data, performs PCA to reduce the data's dimensionality to two principal components, and then visualizes both the original data and the PCA results on a scatter plot. This provides a clear understanding of how PCA transforms the data by capturing the most significant directions of variance."
      ],
      "metadata": {
        "id": "0RSL3cyLy57J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "n4je4lpG2aBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of Principal Component Analysis (PCA) in a healthcare setting: analyzing electronic health records (EHRs) to identify patterns and reduce dimensionality for efficient processing.\n",
        "\n",
        "Scenario: A hospital has a large dataset of electronic health records (EHRs) containing information on various health attributes of patients, such as age, gender, medical history, vital signs, laboratory results, and other clinical variables. The hospital aims to improve patient care and optimize resource allocation by extracting meaningful insights from this vast dataset.\n",
        "\n",
        "Here's how PCA can be applied to this healthcare scenario:\n",
        "\n",
        "Step 1: Data Preprocessing\n",
        "- Collect and assemble the EHR data, ensuring that it is de-identified and compliant with privacy regulations.\n",
        "- Normalize or standardize the data to ensure that all features have the same scale. This step is essential as PCA is sensitive to the scale of the features.\n",
        "\n",
        "Step 2: Dimensionality Reduction using PCA\n",
        "- Perform PCA on the preprocessed EHR dataset to reduce its dimensionality. PCA will identify the principal components that capture the most significant variability in the data.\n",
        "- Interpret the principal components to understand the underlying patterns in the dataset. Each principal component is a linear combination of the original features and represents different aspects of the patients' health characteristics.\n",
        "\n",
        "Step 3: Visualization and Insights\n",
        "- Visualize the results of PCA to gain insights into the data's structure. For example, plot the data points in the two-dimensional space spanned by the first two principal components. This plot can reveal clusters or patterns in the data that might not have been apparent in the original high-dimensional feature space.\n",
        "- Identify if there are specific principal components that are highly correlated with specific health conditions or outcomes. These components can provide valuable insights into the factors influencing patients' health.\n",
        "\n",
        "Step 4: Patient Stratification and Risk Assessment\n",
        "- Use the reduced-dimensional data to segment patients into different groups based on their health characteristics. Clustering algorithms can be applied to group patients with similar health profiles, allowing healthcare providers to tailor treatment plans to specific patient groups.\n",
        "- Leverage the principal components to assess patient risk scores. Patients with similar principal component values might share similar health risks, helping prioritize interventions for those at higher risk.\n",
        "\n",
        "Step 5: Resource Optimization\n",
        "- PCA can help identify which health attributes contribute most to the overall variability in the dataset. This information can be used to optimize resource allocation and focus on collecting the most relevant health information during patient visits or data collection processes.\n",
        "- PCA can also help reduce data redundancy by identifying the most critical features, which can be particularly useful when working with limited storage capacity or computational resources.\n",
        "\n",
        "In this healthcare example, PCA allows for efficient data exploration, pattern recognition, and dimensionality reduction in large and complex electronic health record datasets. By leveraging the power of PCA, healthcare institutions can make better-informed decisions, improve patient care, and optimize resource allocation, ultimately leading to better health outcomes for patients."
      ],
      "metadata": {
        "id": "5fwKp9bIFx25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "FEKxr-SF4BY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is Principal Component Analysis (PCA)?**\n",
        "   PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It achieves this by finding the principal components, which are orthogonal linear combinations of the original features that capture the most significant variations in the data.\n",
        "\n",
        "2. **What are the primary applications of PCA?**\n",
        "   PCA is widely used in various fields, including image and speech recognition, finance, genetics, data compression, and data visualization. It helps in reducing the computational complexity of models, identifying patterns in data, and removing noise.\n",
        "\n",
        "3. **How does PCA achieve dimensionality reduction?**\n",
        "   PCA finds the principal components by computing the eigenvectors and eigenvalues of the covariance matrix or the singular value decomposition (SVD) of the data matrix. These principal components represent the new orthogonal axes along which the data is projected, resulting in dimensionality reduction.\n",
        "\n",
        "4. **What does it mean when we say PCA preserves variance?**\n",
        "   When performing dimensionality reduction with PCA, the first few principal components retain most of the variability present in the original data. By projecting the data onto these components, we retain the most important information while dropping less significant aspects of the data.\n",
        "\n",
        "5. **Is PCA sensitive to the scale of the features?**\n",
        "   Yes, PCA is sensitive to the scale of the features. It is recommended to standardize or normalize the data before applying PCA to ensure that features with larger magnitudes do not dominate the analysis.\n",
        "\n",
        "6. **What is the significance of eigenvalues in PCA?**\n",
        "   The eigenvalues of the covariance matrix (or the singular values in SVD) correspond to the variance explained by each principal component. Larger eigenvalues indicate that the corresponding principal components capture more variance in the data.\n",
        "\n",
        "7. **How do you determine the number of principal components to retain?**\n",
        "   The number of principal components to retain depends on the desired amount of variance to be preserved or the level of dimensionality reduction required. A common approach is to look at the cumulative explained variance and choose the number of components that retain a significant portion (e.g., 95% or 99%) of the total variance.\n",
        "\n",
        "8. **Can PCA be used for feature selection?**\n",
        "   PCA can be indirectly used for feature selection, as it identifies the most important components that capture the variance in the data. However, it does not provide feature rankings or importance scores as some other feature selection techniques do.\n",
        "\n",
        "9. **Is PCA an unsupervised learning algorithm?**\n",
        "   Yes, PCA is an unsupervised learning algorithm as it does not rely on any class labels for its operation. It solely depends on the input data to find the principal components.\n",
        "\n",
        "10. **Are there any limitations of PCA?**\n",
        "    Yes, PCA has some limitations. For example, it may not perform well on non-linear data patterns, and it assumes that the most significant variations in the data are captured by orthogonal axes, which may not always be true. Additionally, the interpretability of the transformed features may be challenging in some cases.\n",
        "\n",
        "Remember that understanding PCA thoroughly and its appropriate application can greatly benefit data analysis and machine learning tasks."
      ],
      "metadata": {
        "id": "hQm6Hw3z63CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "0uQbOM20g7tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is the main goal of Principal Component Analysis (PCA)?\n",
        "\n",
        "a) To visualize data in higher dimensions.\n",
        "b) To reduce the dimensionality of data while preserving its variance.\n",
        "c) To increase the complexity of the dataset.\n",
        "d) To create new features that are orthogonal to the original features.\n",
        "\n",
        "**Question 2:** In PCA, what are the principal components?\n",
        "\n",
        "a) The features of the dataset.\n",
        "b) The new variables obtained by linearly combining the original features.\n",
        "c) The outliers in the data.\n",
        "d) The labels assigned to each data point.\n",
        "\n",
        "**Question 3:** How are the principal components ordered in PCA?\n",
        "\n",
        "a) In descending order of their eigenvalues.\n",
        "b) In ascending order of their eigenvalues.\n",
        "c) In random order.\n",
        "d) In the order of their original features.\n",
        "\n",
        "**Question 4:** What is the significance of the first principal component in PCA?\n",
        "\n",
        "a) It has the highest eigenvalue and captures the most variance in the data.\n",
        "b) It has the lowest eigenvalue and captures the least variance in the data.\n",
        "c) It is always orthogonal to the other principal components.\n",
        "d) It is a linear combination of the least important features.\n",
        "\n",
        "**Question 5:** PCA can be used for:\n",
        "\n",
        "a) Data compression and reducing storage requirements.\n",
        "b) Increasing the dimensionality of the data.\n",
        "c) Introducing noise to the dataset.\n",
        "d) Generating new data from scratch.\n",
        "\n",
        "**Question 6:** When should you consider using PCA?\n",
        "\n",
        "a) When you want to increase the dimensionality of the dataset.\n",
        "b) When the dataset is small and simple.\n",
        "c) When there are multicollinearity issues among features.\n",
        "d) When you want to retain all the original features.\n",
        "\n",
        "**Question 7:** What is the trade-off when applying PCA to a dataset?\n",
        "\n",
        "a) Increased interpretability and reduced computational complexity.\n",
        "b) Reduced interpretability and increased computational complexity.\n",
        "c) No impact on interpretability or computational complexity.\n",
        "d) Increased interpretability and increased computational complexity.\n",
        "\n",
        "**Question 8:** Can PCA be applied to categorical data?\n",
        "\n",
        "a) Yes, but it requires conversion of categorical data to numerical data.\n",
        "b) No, PCA can only be applied to numerical data.\n",
        "c) Yes, PCA automatically handles categorical data.\n",
        "d) Yes, but only if the categorical data has a specific format.\n",
        "\n",
        "**Question 9:** How do you choose the number of principal components to retain in PCA?\n",
        "\n",
        "a) Choose the same number as the original features.\n",
        "b) Choose any arbitrary number.\n",
        "c) Based on a cumulative explained variance threshold or eigenvalue criterion.\n",
        "d) Retain all principal components to avoid information loss.\n",
        "\n",
        "**Question 10:** Which application does PCA find most useful?\n",
        "\n",
        "a) Image classification.\n",
        "b) Structured data analysis.\n",
        "c) Text data analysis.\n",
        "d) Time series forecasting.\n",
        "\n",
        "**Answers:**\n",
        "1. b) To reduce the dimensionality of data while preserving its variance.\n",
        "2. b) The new variables obtained by linearly combining the original features.\n",
        "3. a) In descending order of their eigenvalues.\n",
        "4. a) It has the highest eigenvalue and captures the most variance in the data.\n",
        "5. a) Data compression and reducing storage requirements.\n",
        "6. c) When there are multicollinearity issues among features.\n",
        "7. b) Reduced interpretability and increased computational complexity.\n",
        "8. a) Yes, but it requires conversion of categorical data to numerical data.\n",
        "9. c) Based on a cumulative explained variance threshold or eigenvalue criterion.\n",
        "10. b) Structured data analysis."
      ],
      "metadata": {
        "id": "gIjfPuqcg9Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "-E7vxSiu0s5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Electronic Health Record (EHR) Data Visualization**:\n",
        "    - **Objective**: Reduce the dimensionality of EHR data and visualize patient clusters to understand the distribution of different health conditions.\n",
        "    - **Dataset**: EHR data with features like medical history, lab results, medications, etc.\n",
        "\n",
        "2. **Genomic Data Interpretation**:\n",
        "    - **Objective**: Use PCA to visualize high-dimensional genomic data to identify patterns or clusters that might be indicative of specific diseases.\n",
        "    - **Dataset**: Genome-wide association studies (GWAS) data.\n",
        "\n",
        "3. **Medical Image Compression**:\n",
        "    - **Objective**: Apply PCA to medical images (like MRIs or X-rays) to understand how much data can be compressed without significant loss of diagnostic information.\n",
        "    - **Dataset**: A collection of medical images in DICOM or another format.\n",
        "\n",
        "4. **Analysis of Physiological Signals**:\n",
        "    - **Objective**: Use PCA to distinguish between different states (like stress vs. rest) based on physiological signals.\n",
        "    - **Dataset**: Signals like electrocardiograms (ECG), electroencephalograms (EEG), or other biometric data.\n",
        "\n",
        "5. **Hospital Resource Allocation**:\n",
        "    - **Objective**: Analyze hospital resource usage (beds, equipment, staff time) using PCA to identify the main drivers of resource consumption.\n",
        "    - **Dataset**: Hospital operation data, including bed utilization, equipment usage, and staffing levels.\n",
        "\n",
        "6. **Clinical Trial Data Analysis**:\n",
        "    - **Objective**: Extract patterns or identify clusters among participants of clinical trials to understand treatment effects or potential side effects better.\n",
        "    - **Dataset**: Data from clinical trials, including patient demographics, treatment protocols, and outcomes.\n",
        "\n",
        "7. **Disease Outbreak Analysis**:\n",
        "    - **Objective**: Use PCA to identify patterns in disease outbreaks over time and space.\n",
        "    - **Dataset**: Epidemiological data with information on disease cases, locations, and time.\n",
        "\n",
        "8. **Patient Satisfaction Survey Analysis**:\n",
        "    - **Objective**: Analyze patient feedback to identify the primary components that contribute to patient satisfaction.\n",
        "    - **Dataset**: Survey data from patients about their hospital or clinic experience.\n",
        "\n",
        "9. **Drug Discovery Data**:\n",
        "    - **Objective**: Apply PCA on molecular descriptors of drugs to cluster potential compounds for specific diseases or understand their mechanism of action.\n",
        "    - **Dataset**: Molecular descriptors for a set of compounds.\n",
        "\n",
        "10. **Treatment Pathway Analysis**:\n",
        "    - **Objective**: Use PCA to analyze the different treatment pathways taken for a particular condition, to identify the most significant contributors to positive outcomes.\n",
        "    - **Dataset**: Patient data showing treatments prescribed and their outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "gTFo3eYh0xKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "118Kdx-NEFpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a working example of how to perform Principal Component Analysis (PCA) using Python and the scikit-learn library with real-world health data. In this example, we'll use the popular \"diabetes\" dataset from the sklearn.datasets module, which contains ten baseline variables (age, sex, BMI, average blood pressure, and six blood serum measurements) for 442 diabetes patients."
      ],
      "metadata": {
        "id": "ThuYP-YP0yZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data = load_diabetes()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "n_components = 2  # Number of components for visualization\n",
        "pca = PCA(n_components=n_components)\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame for the principal components\n",
        "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "explained_var_ratio = pca.explained_variance_ratio_\n",
        "plt.bar(range(1, n_components + 1), explained_var_ratio)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance Ratio by Principal Components')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of the first two principal components\n",
        "plt.scatter(pc_df['PC1'], pc_df['PC2'], c=y, cmap='viridis')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Scatter Plot of First Two Principal Components')\n",
        "plt.colorbar(label='Target')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ytVqcZJ00VtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we first load the diabetes dataset and standardize the features using `StandardScaler` to have zero mean and unit variance. Then, we perform PCA using `PCA` from `scikit-learn`. We choose to visualize the first two principal components using scatter plots and also plot the explained variance ratio of each principal component.\n",
        "\n",
        "\n",
        "Keep in mind that you can replace the \"diabetes\" dataset with your own health data in a similar format. Just make sure that the data is numeric and preprocessed appropriately before applying PCA.\n"
      ],
      "metadata": {
        "id": "R7J1sGKe1nUv"
      }
    }
  ]
}