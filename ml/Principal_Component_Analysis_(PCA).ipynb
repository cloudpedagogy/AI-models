{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSLDwowYd6PVpfz9P9di+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/Principal_Component_Analysis_(PCA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) Model Background"
      ],
      "metadata": {
        "id": "J7jNDD68BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a widely used technique in the field of statistics and data analysis. It is a dimensionality reduction method that transforms a set of correlated variables into a new set of uncorrelated variables, called principal components. These components are ordered in such a way that the first principal component explains the largest variance in the data, the second component explains the second-largest variance, and so on.\n",
        "\n",
        "**Here's how PCA works**:\n",
        "\n",
        "1. Standardize the data: If the variables in the dataset have different scales, PCA first standardizes the data to give each variable an equal importance in the analysis.\n",
        "\n",
        "2. Calculate the covariance matrix: PCA calculates the covariance matrix of the standardized data, which represents the relationships between the variables.\n",
        "\n",
        "3. Compute the eigenvectors and eigenvalues: PCA then calculates the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate the magnitude of variance explained by each principal component.\n",
        "\n",
        "4. Select the principal components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The top-k eigenvectors (principal components) are selected to capture the most significant variance in the data.\n",
        "\n",
        "5. Transform the data: Finally, the original data is projected onto the selected principal components to create a new feature space.\n",
        "\n",
        "**Pros of PCA**:\n",
        "1. Dimensionality reduction: PCA helps reduce the number of variables, making the data easier to visualize, analyze, and process, especially when dealing with high-dimensional datasets.\n",
        "\n",
        "2. Feature extraction: PCA transforms the original variables into new, uncorrelated variables (principal components) that are linear combinations of the original features, potentially revealing underlying patterns in the data.\n",
        "\n",
        "3. Noise reduction: By focusing on the components with high variance, PCA can remove noise and retain the most informative aspects of the data.\n",
        "\n",
        "4. Interpretability: In some cases, the principal components might have clear interpretations, allowing for a better understanding of the dominant factors influencing the data.\n",
        "\n",
        "**Cons of PCA**:\n",
        "1. Information loss: The main drawback of PCA is that it reduces data dimensionality by eliminating the components with low variance, which may lead to some loss of information.\n",
        "\n",
        "2. Interpretability challenges: While some principal components might be interpretable, others may not have any straightforward meaning, making it difficult to explain the results.\n",
        "\n",
        "3. Sensitivity to outliers: PCA is sensitive to outliers as they can disproportionately affect the covariance matrix and, consequently, the principal components.\n",
        "\n",
        "4. Non-linear relationships: PCA is effective for finding linear relationships between variables. If the data contains complex non-linear relationships, PCA may not capture them well.\n",
        "\n",
        "**When to use PCA**:\n",
        "PCA is a useful technique in various scenarios:\n",
        "\n",
        "1. Dimensionality reduction: When dealing with high-dimensional datasets and computational resources are limited, PCA can be employed to reduce the number of features while preserving most of the variability.\n",
        "\n",
        "2. Visualization: PCA can be used to project high-dimensional data into a lower-dimensional space (e.g., 2D or 3D) for visualization purposes.\n",
        "\n",
        "3. Data preprocessing: PCA can be utilized as a preprocessing step before applying other machine learning algorithms to reduce noise, improve performance, and prevent overfitting.\n",
        "\n",
        "4. Identifying important features: By analyzing the principal components, you can identify the most influential features in the dataset.\n",
        "\n",
        "5. Collinearity detection: PCA can help detect multicollinearity among variables, which is essential when performing regression analysis.\n",
        "\n",
        "However, it's important to note that PCA might not always be the best choice, especially when non-linear relationships are present, or when interpretability of the components is crucial. In such cases, other dimensionality reduction techniques or domain-specific approaches may be more suitable."
      ],
      "metadata": {
        "id": "swYHvHDH_mR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "1Yn-TgaLbUHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate sample data for demonstration\n",
        "np.random.seed(42)\n",
        "data = np.random.rand(100, 4)  # 100 samples, 4 features\n",
        "\n",
        "# Create a Pandas DataFrame for the data\n",
        "df = pd.DataFrame(data, columns=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4'])\n",
        "\n",
        "# Standardize the data (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Create a new DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "\n",
        "# Plot the original data and the PCA results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['Feature 1'], df['Feature 2'], label='Original Data')\n",
        "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], label='PCA Result')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('PCA Example')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r3Hkyu4eGg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "ha9OjD101Rid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import necessary libraries:**\n",
        "   The code starts by importing the required libraries: `numpy` (as np), `pandas` (as pd), `matplotlib.pyplot` (as plt), `PCA` from `sklearn.decomposition`, and `StandardScaler` from `sklearn.preprocessing`. These libraries will be used for data manipulation, visualization, and performing PCA.\n",
        "\n",
        "2. **Generate sample data for demonstration:**\n",
        "   The code generates a 100x4 NumPy array called `data` using `np.random.rand(100, 4)`. This represents 100 samples with 4 features each. The data is random and is used to demonstrate the PCA process.\n",
        "\n",
        "3. **Create a Pandas DataFrame for the data:**\n",
        "   The data is converted into a Pandas DataFrame called `df`. Each column in the DataFrame corresponds to one of the four features, and the columns are labeled 'Feature 1', 'Feature 2', 'Feature 3', and 'Feature 4'.\n",
        "\n",
        "4. **Standardize the data (mean=0, std=1):**\n",
        "   PCA is sensitive to the scale of the features, so it is essential to standardize the data before performing PCA. The code creates a `StandardScaler` object called `scaler` and then uses it to transform the DataFrame `df` into `scaled_data`, where each feature has a mean of 0 and standard deviation of 1.\n",
        "\n",
        "5. **Perform PCA:**\n",
        "   The code creates a `PCA` object called `pca` and specifies `n_components=2`. This means that PCA will reduce the dimensionality of the data to 2 principal components. The `fit_transform()` method is then called on `scaled_data`, and the result is stored in `pca_result`.\n",
        "\n",
        "6. **Create a new DataFrame for the PCA results:**\n",
        "   The `pca_result` array is converted into another Pandas DataFrame called `pca_df`. This new DataFrame contains the principal components resulting from the PCA, labeled as 'Principal Component 1' and 'Principal Component 2'.\n",
        "\n",
        "7. **Plot the original data and the PCA results:**\n",
        "   The code creates a scatter plot to visualize the original data and the PCA results side by side. The original data from `df` is plotted with 'Feature 1' on the x-axis and 'Feature 2' on the y-axis. The PCA results from `pca_df` are plotted with 'Principal Component 1' on the x-axis and 'Principal Component 2' on the y-axis. The plot shows how the data is transformed after PCA.\n",
        "\n",
        "8. **Labels and Visualization:**\n",
        "   The plot is labeled with appropriate x and y axis labels and given a title. It also includes a legend to differentiate between the original data and the PCA results.\n",
        "\n",
        "9. **Show the plot:**\n",
        "   The `plt.show()` function is called to display the generated plot.\n",
        "\n",
        "In summary, this code generates sample data, performs PCA to reduce the data's dimensionality to two principal components, and then visualizes both the original data and the PCA results on a scatter plot. This provides a clear understanding of how PCA transforms the data by capturing the most significant directions of variance."
      ],
      "metadata": {
        "id": "0RSL3cyLy57J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "n4je4lpG2aBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of Principal Component Analysis (PCA) in a healthcare setting: analyzing electronic health records (EHRs) to identify patterns and reduce dimensionality for efficient processing.\n",
        "\n",
        "Scenario: A hospital has a large dataset of electronic health records (EHRs) containing information on various health attributes of patients, such as age, gender, medical history, vital signs, laboratory results, and other clinical variables. The hospital aims to improve patient care and optimize resource allocation by extracting meaningful insights from this vast dataset.\n",
        "\n",
        "Here's how PCA can be applied to this healthcare scenario:\n",
        "\n",
        "Step 1: Data Preprocessing\n",
        "- Collect and assemble the EHR data, ensuring that it is de-identified and compliant with privacy regulations.\n",
        "- Normalize or standardize the data to ensure that all features have the same scale. This step is essential as PCA is sensitive to the scale of the features.\n",
        "\n",
        "Step 2: Dimensionality Reduction using PCA\n",
        "- Perform PCA on the preprocessed EHR dataset to reduce its dimensionality. PCA will identify the principal components that capture the most significant variability in the data.\n",
        "- Interpret the principal components to understand the underlying patterns in the dataset. Each principal component is a linear combination of the original features and represents different aspects of the patients' health characteristics.\n",
        "\n",
        "Step 3: Visualization and Insights\n",
        "- Visualize the results of PCA to gain insights into the data's structure. For example, plot the data points in the two-dimensional space spanned by the first two principal components. This plot can reveal clusters or patterns in the data that might not have been apparent in the original high-dimensional feature space.\n",
        "- Identify if there are specific principal components that are highly correlated with specific health conditions or outcomes. These components can provide valuable insights into the factors influencing patients' health.\n",
        "\n",
        "Step 4: Patient Stratification and Risk Assessment\n",
        "- Use the reduced-dimensional data to segment patients into different groups based on their health characteristics. Clustering algorithms can be applied to group patients with similar health profiles, allowing healthcare providers to tailor treatment plans to specific patient groups.\n",
        "- Leverage the principal components to assess patient risk scores. Patients with similar principal component values might share similar health risks, helping prioritize interventions for those at higher risk.\n",
        "\n",
        "Step 5: Resource Optimization\n",
        "- PCA can help identify which health attributes contribute most to the overall variability in the dataset. This information can be used to optimize resource allocation and focus on collecting the most relevant health information during patient visits or data collection processes.\n",
        "- PCA can also help reduce data redundancy by identifying the most critical features, which can be particularly useful when working with limited storage capacity or computational resources.\n",
        "\n",
        "In this healthcare example, PCA allows for efficient data exploration, pattern recognition, and dimensionality reduction in large and complex electronic health record datasets. By leveraging the power of PCA, healthcare institutions can make better-informed decisions, improve patient care, and optimize resource allocation, ultimately leading to better health outcomes for patients."
      ],
      "metadata": {
        "id": "5fwKp9bIFx25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "FEKxr-SF4BY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is Principal Component Analysis (PCA)?**\n",
        "   PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It achieves this by finding the principal components, which are orthogonal linear combinations of the original features that capture the most significant variations in the data.\n",
        "\n",
        "2. **What are the primary applications of PCA?**\n",
        "   PCA is widely used in various fields, including image and speech recognition, finance, genetics, data compression, and data visualization. It helps in reducing the computational complexity of models, identifying patterns in data, and removing noise.\n",
        "\n",
        "3. **How does PCA achieve dimensionality reduction?**\n",
        "   PCA finds the principal components by computing the eigenvectors and eigenvalues of the covariance matrix or the singular value decomposition (SVD) of the data matrix. These principal components represent the new orthogonal axes along which the data is projected, resulting in dimensionality reduction.\n",
        "\n",
        "4. **What does it mean when we say PCA preserves variance?**\n",
        "   When performing dimensionality reduction with PCA, the first few principal components retain most of the variability present in the original data. By projecting the data onto these components, we retain the most important information while dropping less significant aspects of the data.\n",
        "\n",
        "5. **Is PCA sensitive to the scale of the features?**\n",
        "   Yes, PCA is sensitive to the scale of the features. It is recommended to standardize or normalize the data before applying PCA to ensure that features with larger magnitudes do not dominate the analysis.\n",
        "\n",
        "6. **What is the significance of eigenvalues in PCA?**\n",
        "   The eigenvalues of the covariance matrix (or the singular values in SVD) correspond to the variance explained by each principal component. Larger eigenvalues indicate that the corresponding principal components capture more variance in the data.\n",
        "\n",
        "7. **How do you determine the number of principal components to retain?**\n",
        "   The number of principal components to retain depends on the desired amount of variance to be preserved or the level of dimensionality reduction required. A common approach is to look at the cumulative explained variance and choose the number of components that retain a significant portion (e.g., 95% or 99%) of the total variance.\n",
        "\n",
        "8. **Can PCA be used for feature selection?**\n",
        "   PCA can be indirectly used for feature selection, as it identifies the most important components that capture the variance in the data. However, it does not provide feature rankings or importance scores as some other feature selection techniques do.\n",
        "\n",
        "9. **Is PCA an unsupervised learning algorithm?**\n",
        "   Yes, PCA is an unsupervised learning algorithm as it does not rely on any class labels for its operation. It solely depends on the input data to find the principal components.\n",
        "\n",
        "10. **Are there any limitations of PCA?**\n",
        "    Yes, PCA has some limitations. For example, it may not perform well on non-linear data patterns, and it assumes that the most significant variations in the data are captured by orthogonal axes, which may not always be true. Additionally, the interpretability of the transformed features may be challenging in some cases.\n",
        "\n",
        "Remember that understanding PCA thoroughly and its appropriate application can greatly benefit data analysis and machine learning tasks."
      ],
      "metadata": {
        "id": "hQm6Hw3z63CL"
      }
    }
  ]
}