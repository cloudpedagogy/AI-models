{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5bFM7CCUSwyPitRoVenqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests Model Background"
      ],
      "metadata": {
        "id": "_CxPSuxTBV9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is a popular machine learning algorithm used for both classification and regression tasks. It is an ensemble learning method that combines multiple decision trees to make more accurate and robust predictions. Here's how it works:\n",
        "\n",
        "**How Random Forests Work:**\n",
        "1. **Building the Forest:** The algorithm creates a collection of decision trees during the training phase. Each tree is built on a random subset of the original data (sampling with replacement) and a random subset of features. This randomness helps reduce overfitting and enhances the model's generalization ability.\n",
        "\n",
        "2. **Voting (Classification) or Averaging (Regression):** For classification tasks, the Random Forest aggregates the predictions of individual trees through majority voting. For regression tasks, the algorithm takes the average of the predicted values from all the trees.\n",
        "\n",
        "**Pros of Random Forest:**\n",
        "1. **High Accuracy:** Random Forest typically delivers higher accuracy compared to individual decision trees, especially on complex datasets with high-dimensional features.\n",
        "\n",
        "2. **Reduced Overfitting:** By averaging the predictions of multiple trees and using random subsets of data and features, Random Forest mitigates the risk of overfitting and improves generalization.\n",
        "\n",
        "3. **Robustness:** Random Forest handles noisy data well and can maintain decent performance even with missing values or outliers.\n",
        "\n",
        "4. **Feature Importance:** The algorithm can measure the importance of each feature in the prediction process, providing valuable insights into the data.\n",
        "\n",
        "5. **Efficiency:** Random Forest can efficiently handle large datasets with high-dimensional features, making it scalable for various applications.\n",
        "\n",
        "6. **Parallelization:** The training of individual trees in the forest can be done in parallel, allowing for faster training times on multi-core processors.\n",
        "\n",
        "**Cons of Random Forest:**\n",
        "1. **Black Box Model:** Random Forests are not very interpretable compared to simple models like linear regression. Understanding the exact decision-making process can be challenging.\n",
        "\n",
        "2. **Memory and Storage:** As an ensemble of multiple decision trees, Random Forests may require more memory and storage compared to individual trees.\n",
        "\n",
        "3. **Training Time:** While parallelization helps speed up the process, training a large number of trees can still be time-consuming.\n",
        "\n",
        "4. **Bias in Class Imbalanced Data:** Random Forests can be biased towards classes with more samples if the dataset is imbalanced.\n",
        "\n",
        "**When to Use Random Forest:**\n",
        "Random Forests are versatile and can be applied to a wide range of machine learning tasks. Here are some scenarios where you might consider using Random Forests:\n",
        "\n",
        "1. **Classification and Regression Tasks:** Random Forests can be used for both classification (predicting categorical labels) and regression (predicting continuous values) problems.\n",
        "\n",
        "2. **Complex and Non-linear Data:** When the data has complex relationships and non-linear interactions between features, Random Forests tend to perform well.\n",
        "\n",
        "3. **High-Dimensional Data:** Random Forests can handle datasets with a large number of features effectively.\n",
        "\n",
        "4. **Noisy Data:** Random Forests can handle noisy and missing data gracefully, making them suitable for real-world datasets.\n",
        "\n",
        "5. **Data Exploration:** The feature importance information can be helpful in understanding the dataset and identifying significant predictors.\n",
        "\n",
        "6. **Avoiding Overfitting:** If you suspect that a single decision tree might overfit the data, using Random Forests can be a good alternative to improve generalization.\n",
        "\n",
        "Remember that while Random Forests are a powerful and reliable option, it's essential to consider the nature of your data and the specific problem you're trying to solve. Always experiment with multiple algorithms to identify the best-performing model for your particular use case."
      ],
      "metadata": {
        "id": "G8PnnMls78L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "odiGBn2zbQD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1_8b2v4kyc7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "KZJDcsU31FpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import necessary libraries:\n",
        "   - `numpy`: Library for numerical computing in Python.\n",
        "   - `pandas`: Library for data manipulation and analysis.\n",
        "   - `sklearn.datasets`: Module in scikit-learn that provides various toy datasets, including the Iris dataset.\n",
        "   - `sklearn.model_selection`: Module in scikit-learn for splitting data into training and testing sets.\n",
        "   - `sklearn.ensemble`: Module in scikit-learn that provides ensemble learning methods, including Random Forest.\n",
        "   - `sklearn.metrics`: Module in scikit-learn that provides various metrics for evaluating machine learning models.\n",
        "\n",
        "2. Load the Iris dataset:\n",
        "   - `load_iris()`: Loads the Iris dataset, which is a classic dataset for classification tasks. It contains features (sepal length, sepal width, petal length, petal width) of iris flowers and the corresponding species labels (setosa, versicolor, virginica).\n",
        "\n",
        "3. Split the data into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the dataset into training and testing sets. The `X` variable contains the features, and the `y` variable contains the target labels. `test_size=0.2` specifies that 20% of the data will be used for testing, and `random_state=42` ensures reproducibility of the split.\n",
        "\n",
        "4. Initialize the Random Forest Classifier:\n",
        "   - `RandomForestClassifier(n_estimators=100, random_state=42)`: Initializes the Random Forest Classifier with 100 decision trees (`n_estimators=100`) and a random state of 42 for reproducibility.\n",
        "\n",
        "5. Train the Random Forest Classifier:\n",
        "   - `rf_classifier.fit(X_train, y_train)`: Trains the Random Forest Classifier on the training data. It learns the relationships between the features and the target labels.\n",
        "\n",
        "6. Make predictions on the test data:\n",
        "   - `y_pred = rf_classifier.predict(X_test)`: Uses the trained Random Forest Classifier to predict the target labels for the test data (`X_test`).\n",
        "\n",
        "7. Evaluate the model:\n",
        "   - `accuracy_score(y_test, y_pred)`: Calculates the accuracy of the model's predictions by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`). The accuracy is the proportion of correctly classified instances.\n",
        "\n",
        "8. Print the evaluation results:\n",
        "   - `print(\"Accuracy:\", accuracy)`: Prints the accuracy of the model on the test data.\n",
        "   - `print(\"\\nClassification Report:\")`: Prints the classification report, which includes precision, recall, F1-score, and support for each class. It provides more detailed insights into model performance.\n",
        "   - `print(classification_report(y_test, y_pred))`: Prints the actual classification report.\n",
        "   - `print(\"Confusion Matrix:\")`: Prints the confusion matrix, which shows the number of true positive, false positive, true negative, and false negative predictions for each class.\n",
        "   - `print(confusion_matrix(y_test, y_pred))`: Prints the actual confusion matrix.\n",
        "\n",
        "In summary, this code loads the Iris dataset, splits it into training and testing sets, initializes a Random Forest Classifier, trains it on the training data, makes predictions on the test data, and then evaluates the model's performance by calculating the accuracy, printing the classification report, and displaying the confusion matrix."
      ],
      "metadata": {
        "id": "-E2pJa110qjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "sNTobTGg2UPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forests, which is an ensemble learning method, can be highly beneficial in healthcare settings, as it allows for efficient handling of large datasets with many variables, and it is capable of accounting for complex interactions and nonlinearity in these variables.\n",
        "\n",
        "One real world example of using Random Forests in a healthcare setting is in predicting patient readmissions. Readmissions are costly and often indicative of sub-optimal care. Healthcare organizations are keen on minimizing these to improve patient care quality and also reduce costs.\n",
        "\n",
        "For instance, a hospital may collect a range of data about patients, including their age, gender, type and number of previous health conditions, medication use, lab results, lifestyle factors such as smoking and alcohol consumption, length of initial hospital stay, and many more. All this data can be used to build a predictive model.\n",
        "\n",
        "The Random Forest algorithm could be trained on this historical data, with the goal of predicting which patients are likely to be readmitted within a certain time frame (like 30 days). The \"forest\" in the algorithm consists of many decision trees, each trained on a random subset of the data, and their predictions are aggregated to make a final prediction.\n",
        "\n",
        "This model could then be used to predict the risk of readmission for new patients, based on their specific details. High-risk patients could be given extra attention or follow-up care to try and prevent readmissions.\n",
        "\n",
        "Another example can be in the diagnosis of diseases. Random Forests have been used to build models that predict whether or not a patient has a particular disease (such as diabetes or heart disease) based on a set of diagnostic measurements. The model can provide a probability estimate for the disease, which can assist physicians in making diagnostic decisions.\n",
        "\n",
        "Lastly, Random Forests can also be utilized for genomics in personalized medicine. Genomic data tends to be very high dimensional (many genes being assessed for each individual). Random Forests can handle this high dimensionality and identify which genes are most predictive of a particular disease or health outcome, helping guide treatment decisions.\n"
      ],
      "metadata": {
        "id": "KgG2Y7e2GUHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "lMg8d-5p3623"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Random Forest model?\n",
        "   - Random Forest is an ensemble learning method used for both classification and regression tasks. It creates multiple decision trees during training and combines their predictions to make more accurate and robust predictions.\n",
        "\n",
        "2. How does the Random Forest algorithm work?\n",
        "   - Random Forest works by creating a set of decision trees using bootstrapped subsets of the training data and random feature subsets. Each tree votes on the final prediction, and the mode (classification) or average (regression) of these votes becomes the final prediction.\n",
        "\n",
        "3. What are the advantages of using Random Forests?\n",
        "   - Random Forests are robust against overfitting and perform well with noisy data.\n",
        "   - They can handle both numerical and categorical features without the need for extensive data preprocessing.\n",
        "   - Feature importance can be easily obtained, helping in understanding the most influential features.\n",
        "\n",
        "4. Can Random Forests handle missing data?\n",
        "   - Yes, Random Forests can handle missing data effectively. When making predictions, the missing values are imputed based on other available features in the data.\n",
        "\n",
        "5. How do Random Forests handle imbalanced data in classification tasks?\n",
        "   - Random Forests can handle imbalanced data naturally. The algorithm assigns class weights to the individual trees during training, which allows it to take the class imbalance into account.\n",
        "\n",
        "6. Are Random Forests susceptible to the curse of dimensionality?\n",
        "   - While Random Forests can handle high-dimensional data, they may suffer from the curse of dimensionality if the number of features is significantly larger than the number of samples. In such cases, the algorithm may struggle to find meaningful splits in the data.\n",
        "\n",
        "7. Do Random Forests require feature scaling?\n",
        "   - Random Forests do not require feature scaling because they are based on decision trees that work with rank-based splitting rather than absolute distances.\n",
        "\n",
        "8. What is the difference between Random Forests and Decision Trees?\n",
        "   - Random Forests are an ensemble of decision trees, whereas Decision Trees are individual trees. The main difference is that Random Forests reduce overfitting and improve generalization by aggregating the predictions of multiple trees.\n",
        "\n",
        "9. Can Random Forests be used for feature selection?\n",
        "   - Yes, Random Forests can be used for feature selection by utilizing the feature importance scores. Features with higher importance are considered more influential in making predictions and can be selected for the final model.\n",
        "\n",
        "10. Are Random Forests computationally expensive?\n",
        "    - Random Forests can be computationally expensive, especially for large datasets and a large number of trees in the forest. However, they can be efficiently parallelized, and techniques like bagging and feature subsampling help manage the computational load.\n",
        "\n",
        "Remember that Random Forests are a powerful and versatile machine learning algorithm, but they are not immune to limitations. It's essential to understand their strengths and weaknesses to use them effectively in different scenarios."
      ],
      "metadata": {
        "id": "YBL-PaC37QaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "QZ-lrQ1_hLPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**A)** It is a type of deep neural network.\n",
        "**B)** It is an ensemble of decision trees.\n",
        "**C)** It is a type of support vector machine.\n",
        "**D)** It is a linear regression model.\n",
        "\n",
        "**Question 2:** What is bagging in the context of Random Forests?\n",
        "\n",
        "**A)** It refers to the process of selecting a subset of features for training.\n",
        "**B)** It is a technique to reduce overfitting by training on multiple subsets of the dataset.\n",
        "**C)** It is a step in preprocessing data for Random Forests.\n",
        "**D)** It is the final step in building a Random Forests model.\n",
        "\n",
        "**Question 3:** How does Random Forests handle feature selection?\n",
        "\n",
        "**A)** It selects features randomly with no specific criteria.\n",
        "**B)** It uses a greedy algorithm to select the most important features.\n",
        "**C)** It selects all available features for every tree in the forest.\n",
        "**D)** It selects features based on the correlation with the target variable.\n",
        "\n",
        "**Question 4:** What is the purpose of using multiple decision trees in a Random Forests model?\n",
        "\n",
        "**A)** To make the model computation faster.\n",
        "**B)** To confuse the model and avoid overfitting.\n",
        "**C)** To reduce the bias of the model.\n",
        "**D)** To improve accuracy and generalization by averaging the predictions.\n",
        "\n",
        "**Question 5:** In a Random Forests model, how does it make predictions for a new data point?\n",
        "\n",
        "**A)** By selecting the output of the first decision tree in the forest.\n",
        "**B)** By averaging the outputs of all decision trees in the forest.\n",
        "**C)** By selecting the output of the decision tree with the highest accuracy.\n",
        "**D)** By selecting the output of the decision tree with the lowest depth.\n",
        "\n",
        "**Question 6:** What is out-of-bag (OOB) error in Random Forests?\n",
        "\n",
        "**A)** It is the error that occurs when a decision tree is too deep.\n",
        "**B)** It is the error rate of the model on the training dataset.\n",
        "**C)** It is the error rate of the model on the validation dataset.\n",
        "**D)** It is the error estimation obtained from using the training data that were not included in a particular tree's training process.\n",
        "\n",
        "**Question 7:** Which of the following statements about Random Forests is true?\n",
        "\n",
        "**A)** Random Forests are highly susceptible to overfitting.\n",
        "**B)** Random Forests consist of only one decision tree.\n",
        "**C)** Random Forests use bagging and feature randomness to improve performance.\n",
        "**D)** Random Forests are generally only suitable for categorical data.\n",
        "\n",
        "**Question 8:** Can Random Forests be used for both classification and regression tasks?\n",
        "\n",
        "**A)** Yes, they can be used for both tasks.\n",
        "**B)** No, they can only be used for classification tasks.\n",
        "**C)** No, they can only be used for regression tasks.\n",
        "**D)** Yes, but only if the target variable has a specific format.\n",
        "\n",
        "**Question 9:** What is the recommended number of trees in a Random Forests model?\n",
        "\n",
        "**A)** A very small number, around 5-10 trees.\n",
        "**B)** It depends on the size of the dataset and can be determined through hyperparameter tuning.\n",
        "**C)** A fixed number, typically 100 trees.\n",
        "**D)** As many trees as the number of features in the dataset.\n",
        "\n",
        "**Question 10:** Which of the following is a potential drawback of the Random Forests model?\n",
        "\n",
        "**A)** It requires extensive feature engineering.\n",
        "**B)** It tends to perform poorly on large datasets.\n",
        "**C)** It can become too simple and underfit the data.\n",
        "**D)** It can be computationally expensive and memory-intensive.\n",
        "\n",
        "**Answers:**\n",
        "1. B\n",
        "2. B\n",
        "3. C\n",
        "4. D\n",
        "5. B\n",
        "6. D\n",
        "7. C\n",
        "8. A\n",
        "9. B\n",
        "10. D"
      ],
      "metadata": {
        "id": "_TE56TSXhMjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "PttcIWYJ1Tuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Prediction**\n",
        "    - **Objective**: Predict the likelihood of a patient having a specific disease (e.g., diabetes, heart disease) based on their medical records.\n",
        "    - **Data Required**: Patient medical records including age, sex, BMI, cholesterol levels, blood pressure, etc.\n",
        "\n",
        "2. **Drug Response Prediction**\n",
        "    - **Objective**: Predict how a patient will respond to a specific drug based on their genetic makeup.\n",
        "    - **Data Required**: Genetic data of patients and their drug response history.\n",
        "\n",
        "3. **Readmission Prediction**\n",
        "    - **Objective**: Predict if a patient will be readmitted to the hospital within a certain timeframe post-discharge.\n",
        "    - **Data Required**: Hospital admission records, patient demographics, diagnosis, treatment provided, length of stay.\n",
        "\n",
        "4. **Medical Image Analysis**\n",
        "    - **Objective**: Detect abnormalities in medical images like X-rays, MRIs, etc.\n",
        "    - **Data Required**: Labeled medical images indicating the presence or absence of abnormalities.\n",
        "\n",
        "5. **Predicting Disease Outbreaks**\n",
        "    - **Objective**: Predict outbreaks of diseases (like flu) in specific regions based on historical data.\n",
        "    - **Data Required**: Historical disease outbreak data, climate data, population density, vaccination records.\n",
        "\n",
        "6. **Patient Length of Stay Prediction**\n",
        "    - **Objective**: Predict the length of stay for patients in the hospital.\n",
        "    - **Data Required**: Patient demographics, diagnosis, treatment plans, surgical information, previous hospital stays.\n",
        "\n",
        "7. **Early Detection of Sepsis**\n",
        "    - **Objective**: Predict the likelihood of a patient developing sepsis in the ICU.\n",
        "    - **Data Required**: Vital signs, lab results, patient history.\n",
        "\n",
        "8. **Optimizing Hospital Resource Allocation**\n",
        "    - **Objective**: Predict the usage of resources (like ventilators) in the upcoming weeks.\n",
        "    - **Data Required**: Hospital resource usage history, patient admission rates, seasonal trends.\n",
        "\n",
        "9. **Mental Health Prediction**\n",
        "    - **Objective**: Predict the likelihood of a patient having mental health issues based on surveys and behavior patterns.\n",
        "    - **Data Required**: Patient surveys, medical history, behavioral data.\n",
        "\n",
        "10. **Telemedicine Triage System**\n",
        "    - **Objective**: Classify the urgency of telehealth calls to efficiently allocate resources.\n",
        "    - **Data Required**: Telehealth call records, symptoms reported, final diagnosis, outcome.\n",
        "\n",
        "11. **Treatment Efficacy**\n",
        "    - **Objective**: Predict how effective a treatment plan will be for a particular patient.\n",
        "    - **Data Required**: Patient demographics, medical history, treatment details, outcome data.\n",
        "\n",
        "12. **Healthcare Fraud Detection**\n",
        "    - **Objective**: Identify potentially fraudulent claims or anomalies in billing.\n",
        "    - **Data Required**: Health insurance claim data, patient demographics, treatment details.\n",
        "\n",
        "13. **Optimizing Appointment Scheduling**\n",
        "    - **Objective**: Predict patient no-shows or cancellations to optimize appointment scheduling.\n",
        "    - **Data Required**: Patient appointment history, demographics, weather data, traffic data.\n",
        "\n",
        "14. **Cancer Detection and Classification**\n",
        "    - **Objective**: Predict the presence and type of cancer from biopsy results.\n",
        "    - **Data Required**: Biopsy results, patient medical history.\n",
        "\n",
        "15. **Epidemiological Modeling**\n",
        "    - **Objective**: Predict the spread of diseases based on various factors using random forests.\n",
        "    - **Data Required**: Historical spread data, mobility data, vaccination rates, public health interventions.\n"
      ],
      "metadata": {
        "id": "wIAJG16w1VuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "ut-yaKXrEKCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a working example of a Random Forests model using a real-world health dataset. In this example, we'll use the famous \"Diabetes\" dataset from the scikit-learn library. This dataset contains ten baseline variables (age, sex, BMI, average blood pressure, and six blood serum measurements) and a quantitative measure of disease progression one year after baseline."
      ],
      "metadata": {
        "id": "aByaqoBl2Neh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for i in sorted_idx:\n",
        "    print(f\"{data.feature_names[i]}: {feature_importance[i]}\")\n"
      ],
      "metadata": {
        "id": "QFqEMp8y2GqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Diabetes dataset from scikit-learn, split it into training and testing sets, and then create a Random Forest Regressor model with 100 trees. We train the model on the training data and make predictions on the test data. Finally, we calculate the Mean Squared Error (MSE) as a measure of the model's performance and print out the feature importances.\n",
        "\n",
        "Remember that this is a simplified example, and in a real-world scenario, you would need to preprocess the data, perform hyperparameter tuning, and possibly apply more advanced techniques for feature engineering and model evaluation."
      ],
      "metadata": {
        "id": "KKxzMPia2tIu"
      }
    }
  ]
}