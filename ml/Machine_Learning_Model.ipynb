{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7R2wd5KKl+/9PRWpRlzVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/Machine_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Machine Learning Model Example"
      ],
      "metadata": {
        "id": "l9KwDyUdFqR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simplified template of a Machine Learning model using the scikit-learn library.\n",
        "\n",
        "This Python code is implementing a machine learning model called k-Nearest Neighbors (KNN) to classify the species of iris flowers, based on the well-known Iris dataset. The Iris dataset is a classic dataset in machine learning and statistics, consisting of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\n",
        "\n"
      ],
      "metadata": {
        "id": "3Vycq087lBa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import datasets\n",
        "import joblib\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Split the dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features to have mean=0 and variance=1\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Model Training\n",
        "# Train the KNN classifier on the training data\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Make Predictions\n",
        "# Predict the test set results\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 4: Model Evaluation\n",
        "# Print the classification report and confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Awl3Sf8t20C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown\n"
      ],
      "metadata": {
        "id": "Lbh5sY1hFjS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Importing necessary libraries**\n",
        "The first set of lines imports all the necessary libraries to perform the task.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import datasets\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "```\n",
        "\n",
        "**Loading the Iris dataset**\n",
        "The Iris flower dataset is loaded using sklearn's dataset module. The iris dataset is a classic and very easy multi-class classification dataset. It has 150 instances with 4 numeric attributes and a target label, making a total of 3 classes.\n",
        "\n",
        "```python\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "```\n",
        "\n",
        "**Data Preprocessing**\n",
        "In this step, the dataset is split into training set and test set with a ratio of 80% for training and 20% for testing. A random state is set to ensure that the splits generate are reproducible. The features are then standardized to have a mean of 0 and a variance of 1 using StandardScaler, which is beneficial in most of the machine learning algorithms.\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "**Model Training**\n",
        "A K-nearest neighbors classifier is used, which is an instance-based learning or non-generalizing learning. It does not attempt to construct a general internal model, but simply stores instances of the training data. It then fits the model with the training data.\n",
        "\n",
        "```python\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Make Predictions**\n",
        "Predict the test set results using the trained K-nearest neighbors classifier model.\n",
        "\n",
        "```python\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "**Model Evaluation**\n",
        "In the final step, the confusion matrix and classification report (which includes precision, recall, f1-score and support) of the model on the test set is printed. This is used to evaluate the performance of the classification model.\n",
        "\n",
        "```python\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "In the confusion matrix, the diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n",
        "\n",
        "The classification report provides a breakdown of each class by precision, recall, f1-score and support showing excellent results (i.e., high precision and recall). Precision is the ability of the classifier not to label a positive sample as negative, and recall is the ability of the classifier to find all the positive samples. The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. The F-beta score weights recall more than precision by a factor of beta. beta = 1.0 means recall and precision are equally important. Support is the number of occurrences of each class in y_test."
      ],
      "metadata": {
        "id": "WmXdVBbzFWfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting your Machine Learning Model"
      ],
      "metadata": {
        "id": "CPnbp_9HveYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you have trained a machine learning model in Colab (or any other environment), you often want to save or export it for later use in a different setting. This could be for deployment, further evaluation, or to continue training at a later date.\n",
        "\n",
        "Depending on the machine learning library you are using, the method for saving or exporting the model could be different. Below, I will provide an example of how you can save a model with Keras (a popular deep learning library in Python).\n",
        "\n",
        "```python\n",
        "# Assuming 'model' is the trained model you want to save\n",
        "model.save('my_model.h5')  \n",
        "```\n",
        "\n",
        "The `.save()` function saves a Keras model into a single HDF5 file which will contain:\n",
        "\n",
        "- the architecture of the model (allowing the recreation of the model)\n",
        "- the model's weights\n",
        "- the training configuration (loss, optimizer)\n",
        "- the state of the optimizer (allows you to resume the training where you left off)\n",
        "\n",
        "The model is saved in the Colab environment. To download the saved model to your local machine, you can use the following code:\n",
        "\n",
        "```python\n",
        "from google.colab import files\n",
        "files.download('my_model.h5')\n",
        "```\n",
        "\n",
        "If you are using a different library (like `scikit-learn`), the method to save the model will be different. For instance, with `scikit-learn`, you can use `joblib` or `pickle` to serialize the model.\n",
        "\n",
        "```python\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(model, 'model.pkl')\n",
        "\n",
        "# Download the model to your local machine\n",
        "files.download('model.pkl')\n",
        "```\n",
        "\n",
        "Remember, when you close your Colab notebook, all data including the saved models will be deleted. So, make sure you download your model after training."
      ],
      "metadata": {
        "id": "oSF5frvHvjEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Saving your Model Code"
      ],
      "metadata": {
        "id": "MMqsnIsCyGXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Model Saving\n",
        "# Save the model and scaler to drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Save model\n",
        "model_path = \"/content/gdrive/My Drive/knn_model.pkl\"\n",
        "joblib.dump(model, model_path)\n",
        "\n",
        "# Save scaler\n",
        "scaler_path = \"/content/gdrive/My Drive/knn_scaler.pkl\"\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "# Load model and scaler\n",
        "loaded_model = joblib.load(model_path)\n",
        "loaded_scaler = joblib.load(scaler_path)\n",
        "\n",
        "# Check that the loaded model and scaler work\n",
        "X_test_scaled = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "4pumtLh7yOiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "Z2Ypshf40Y1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Model Saving**\n",
        "This step demonstrates how to save (persist) and load (re-use) your trained model and scaler for future use.\n",
        "\n",
        "```python\n",
        "drive.mount('/content/gdrive')\n",
        "```\n",
        "The `drive.mount('/content/gdrive')` line of code is used to connect your Google Colab environment with your Google Drive. This is done to allow you to save and load files directly to and from your Google Drive.\n",
        "\n",
        "```python\n",
        "model_path = \"/content/gdrive/My Drive/knn_model.pkl\"\n",
        "joblib.dump(model, model_path)\n",
        "```\n",
        "The `joblib.dump()` function is then used to save your trained model (K-Nearest Neighbors Classifier) to your Google Drive as a pickle file (\"knn_model.pkl\"). You specify the path to the file in your Google Drive where the model will be saved.\n",
        "\n",
        "```python\n",
        "scaler_path = \"/content/gdrive/My Drive/knn_scaler.pkl\"\n",
        "joblib.dump(scaler, scaler_path)\n",
        "```\n",
        "Similarly, the trained StandardScaler is also saved as a pickle file (\"knn_scaler.pkl\") to your Google Drive using `joblib.dump()`.\n",
        "\n",
        "```python\n",
        "loaded_model = joblib.load(model_path)\n",
        "loaded_scaler = joblib.load(scaler_path)\n",
        "```\n",
        "The `joblib.load()` function is used to load the saved model and scaler from your Google Drive for future use. This loaded model and scaler can be used to transform new data and make predictions in the same way as your original model and scaler.\n",
        "\n",
        "```python\n",
        "X_test_scaled = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "```\n",
        "As a check to make sure the loaded model and scaler work as expected, the test set features are transformed using the loaded scaler, and the loaded model is used to make predictions.\n",
        "\n",
        "```python\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "Finally, the performance of the loaded model on the test set is evaluated by printing the confusion matrix and classification report, the same as was done for the original model. This confirms that the loaded model and scaler work the same as the original ones."
      ],
      "metadata": {
        "id": "gbed53Mh0caJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "OBt2eNeDGZZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the most commonly used machine learning terms:\n",
        "\n",
        "1. **Machine Learning (ML):** An application of artificial intelligence that provides systems the ability to learn and improve from experience without being explicitly programmed.\n",
        "\n",
        "2. **Artificial Intelligence (AI):** The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, etc.\n",
        "\n",
        "3. **Deep Learning:** A subset of machine learning that's based on artificial neural networks with representation learning. It can process a wide range of data resources, requires less data preprocessing by humans, and can often produce more accurate results than traditional ML approaches.\n",
        "\n",
        "4. **Neural Network:** A series of algorithms that attempt to identify underlying relationships in a set of data through a process that mimics how the human brain works.\n",
        "\n",
        "5. **Training Data:** The data on which the machine learning model is trained, and learns from.\n",
        "\n",
        "6. **Test Data:** Independent data used to evaluate the performance of the model. It's important that the model has never seen this data during training.\n",
        "\n",
        "7. **Supervised Learning:** A type of machine learning where the model is trained on a labeled dataset, i.e., each instance in the training dataset includes the expected output.\n",
        "\n",
        "8. **Unsupervised Learning:** A type of machine learning where the model is trained on an unlabeled dataset and must find patterns in the data on its own.\n",
        "\n",
        "9. **Reinforcement Learning:** A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward.\n",
        "\n",
        "10. **Classification:** A type of machine learning task where the model is trained to assign labels to instances based on their features.\n",
        "\n",
        "11. **Regression:** A type of machine learning task where the model is trained to predict a continuous numerical value based on the instance's features.\n",
        "\n",
        "12. **Overfitting:** A concept in machine learning where a model is too closely fit to the training data, causing it to perform poorly on unseen data.\n",
        "\n",
        "13. **Underfitting:** This occurs when a model is too simple and does not fit the training data well enough, causing poor performance on both the training data and unseen data.\n",
        "\n",
        "14. **Regularization:** A technique used to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "15. **Hyperparameter Tuning:** The process of choosing a set of optimal hyperparameters for a machine learning model.\n",
        "\n",
        "16. **Cross-Validation:** A technique for assessing how the statistical analysis will generalize to an independent data set. It's mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
        "\n",
        "17. **Bias-Variance Tradeoff:** A property of learning algorithms that the error obtained on new unseen data (generalization error) can be decomposed into bias, variance, and noise. The bias error is an error from erroneous assumptions in the learning algorithm. Variance is an error from sensitivity to small fluctuations in the training set.\n",
        "\n",
        "18. **Feature Selection:** The process of selecting a subset of relevant features (variables, predictors) for use in model construction.\n",
        "\n",
        "19. **Feature Engineering:** The process of creating new features from existing ones, often to improve machine learning model performance.\n",
        "\n",
        "20. **Gradient Descent:** An optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
        "\n",
        "21. **Stochastic Gradient Descent (SGD):** A variation of the gradient descent algorithm that calculates the gradient and updates the model parameters using a single instance, instead of the entire training dataset.\n",
        "\n",
        "22. **Confusion Matrix:** A table used to describe the performance of a classification model (a classifier) on a set of test data for which the true values are known.\n",
        "\n",
        "23. **Precision:** The number of True Positives divided by the number of True Positives and False Positives. It's intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "24. **Recall (Sensitivity):** The number of True Positives divided by the number of True Positives and the number of False Negatives. It's intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "25. **F1-Score:** The harmonic mean of Precision and Recall. It tries to find the balance between precision and recall.\n",
        "\n",
        "26. **ROC Curve:** A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It's created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
        "\n",
        "27. **Area Under the Curve (AUC):** The area underneath the ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds.\n",
        "\n",
        "There are many more terms and concepts, as machine learning is a vast field with a variety of techniques and theories. This list should provide a good start and covers many of the basic and commonly used terms."
      ],
      "metadata": {
        "id": "zv-u8a0-Gb26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "GiUg6EiHKEZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is the difference between AI, Machine Learning, and Deep Learning?**\n",
        "\n",
        "AI is the broadest term, encompassing the entire field of intelligent machines that can perform tasks usually requiring human intelligence. Machine Learning is a subset of AI where machines can learn from data without being explicitly programmed. Deep Learning is a further subset of Machine Learning that utilizes neural networks with several layers (\"deep\" structures) to make sense of data.\n",
        "\n",
        "2. **What is supervised, unsupervised, and reinforcement learning?**\n",
        "\n",
        "Supervised learning is a type of machine learning where the model is provided with labeled training data. Unsupervised learning is where the model must find patterns and relationships in datasets without any labels. Reinforcement learning is where an agent learns to behave in an environment by performing actions and seeing the results, guided by rewards (reinforcements).\n",
        "\n",
        "3. **What is the bias-variance tradeoff?**\n",
        "\n",
        "The bias-variance tradeoff is a central problem in supervised learning. Ideally, a model should have low bias (not making assumptions about the data) and low variance (changing a lot in response to changes in input data). However, in practice, reducing one can increase the other. The tradeoff is finding a balance so that the model generalizes well and doesn't overfit or underfit.\n",
        "\n",
        "4. **What are hyperparameters in machine learning models?**\n",
        "\n",
        "Hyperparameters are parameters that are set before the learning process begins. These values instruct the learning process and can significantly impact the performance of the model. Examples include learning rate, the number of hidden layers in a neural network, or the number of clusters in a k-means clustering algorithm.\n",
        "\n",
        "5. **What is the curse of dimensionality in Machine Learning?**\n",
        "\n",
        "The curse of dimensionality refers to the phenomena where the feature space becomes increasingly sparse for an increasing number of dimensions of a dataset. Various phenomena that do not occur in low-dimensional space arise in high-dimensional space. In general, as the dimensionality increases, the classifier's performance decreases.\n",
        "\n",
        "6. **Why do we need to normalize or scale our data in Machine Learning?**\n",
        "\n",
        "Normalization or scaling is used to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values or losing information. Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
        "\n",
        "7. **What is cross-validation?**\n",
        "\n",
        "Cross-validation is a resampling method used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. It's useful to get an unbiased estimate of model generalization on unseen data.\n",
        "\n",
        "8. **What are some common problems in the Machine Learning process?**\n",
        "\n",
        "Common problems include overfitting (model performs well on training data but not on unseen data), underfitting (model is too simple to capture patterns in data), lack of quality data, difficulties in feature selection, and the need for manual hyperparameter tuning.\n",
        "\n",
        "9. **What is feature engineering?**\n",
        "\n",
        "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\n",
        "\n",
        "10. **What is the role of a validation dataset?**\n",
        "\n",
        "The role of the validation dataset is to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The validation set is used to prevent overfitting of the model. It's a check to ensure the model doesn't just memorize the training data but learns to generalize from it."
      ],
      "metadata": {
        "id": "bd3nYpBTKFoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future trends"
      ],
      "metadata": {
        "id": "F0Ec2ZSMKSR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some future trends in machine learning that are anticipated to gain prominence in the next few years:\n",
        "\n",
        "1. **AutoML and Automated Data Science**: Automating the end-to-end process of applying machine learning to real-world problems is a hot trend. It includes automating data preprocessing, feature selection, model selection, hyperparameter tuning, and model deployment.\n",
        "\n",
        "2. **Explainable AI**: As machine learning algorithms become more advanced and complex, the demand for transparency and interpretability in models is increasing. Users want to know why and how an AI system makes its decisions. Explainable AI (XAI) aims to address this concern.\n",
        "\n",
        "3. **Reinforcement Learning**: Reinforcement learning involves an agent that learns how to behave in an environment by performing actions and observing the results. Its applications are expected to broaden significantly, including areas like resource management, traffic light control, or personalized recommendations.\n",
        "\n",
        "4. **Federated Learning**: With an increasing focus on data privacy and security, federated learning is becoming popular. It allows for decentralized machine learning where the data doesn't have to leave its original device, yet the global model can learn from all the devices.\n",
        "\n",
        "5. **Edge AI**: With the growth of IoT devices, performing AI computations on the edge (on local devices) instead of the cloud is a growing trend. This reduces the need for data communication with the cloud and allows real-time processing and decision-making on the edge devices.\n",
        "\n",
        "6. **Quantum Machine Learning**: Quantum computers have the potential to vastly speed up processing, which is beneficial for machine learning. Quantum machine learning explores how quantum computing can be used to improve machine learning algorithms.\n",
        "\n",
        "7. **Natural Language Processing (NLP)**: NLP technologies are set to become more sophisticated, allowing for more human-like interactions with AI systems. This includes advancements in language understanding, generation, and translation.\n",
        "\n",
        "8. **AI in Cybersecurity**: Machine learning models will play an increasingly important role in detecting and preventing cyber threats. ML can analyze patterns and learn from them to help predict and identify cyber attacks.\n",
        "\n",
        "9. **Transfer Learning**: This technique allows us to leverage pre-trained models on larger datasets on similar tasks, reducing the need for large-scale data collection and improving model performance.\n",
        "\n",
        "10. **Responsible AI**: As machine learning models get incorporated into more and more systems, there's a rising demand for ethical, unbiased, and fair models. Guidelines and tools for building and evaluating responsible AI are likely to become more prevalent.\n",
        "\n",
        "It's important to remember that the future of machine learning will be shaped by many factors, including advancements in technology, the availability of data, and our understanding of algorithms and methodologies."
      ],
      "metadata": {
        "id": "29Vd1yT2KSDz"
      }
    }
  ]
}