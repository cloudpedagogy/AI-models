{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRnxJJlGw4j7x/cHf38+Pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees Model Background"
      ],
      "metadata": {
        "id": "yGso-vPE2qyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are a popular supervised machine learning algorithm used for both classification and regression tasks. They are versatile and intuitive models that partition the input data into subsets based on different attributes, forming a tree-like structure where each internal node represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents the final prediction or value.\n",
        "\n",
        "Here are some pros and cons of Decision Trees:\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "1. **Interpretability:** Decision Trees are easy to understand and visualize. The tree structure can be interpreted and explained to stakeholders, making it a great choice when transparency is important.\n",
        "\n",
        "2. **No feature scaling required:** Decision Trees are not sensitive to feature scaling, so you don't need to normalize or standardize the input features before training the model.\n",
        "\n",
        "3. **Handles both categorical and numerical data:** Decision Trees can handle a mix of categorical and numerical features, which is not the case for some other algorithms.\n",
        "\n",
        "4. **Nonlinear relationships:** Decision Trees can capture nonlinear relationships between features and the target variable.\n",
        "\n",
        "5. **Automatic feature selection:** The algorithm automatically selects the most relevant features, making it robust to irrelevant or redundant inputs.\n",
        "\n",
        "6. **Fast prediction time:** Once the tree is built, making predictions on new data is fast as it involves traversing the tree from the root to a leaf node.\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "1. **Overfitting:** Decision Trees have a tendency to overfit the training data, capturing noise and specific details of the training set. Pruning techniques or using ensemble methods like Random Forests can help mitigate this issue.\n",
        "\n",
        "2. **Instability:** Small changes in the data can lead to significant changes in the tree structure, which can make the model less robust.\n",
        "\n",
        "3. **Biased towards dominant classes:** In classification tasks with imbalanced class distributions, Decision Trees can be biased towards the dominant class.\n",
        "\n",
        "4. **Global optimality:** Decision Trees use a greedy approach during the construction process, which may not lead to a globally optimal tree.\n",
        "\n",
        "5. **Poor extrapolation:** Decision Trees are not well suited for extrapolation, meaning they may not make accurate predictions for data outside the range of the training set.\n",
        "\n",
        "**When to use Decision Trees**:\n",
        "\n",
        "Decision Trees are suitable for various scenarios, including:\n",
        "\n",
        "1. **Interpretability is essential:** When you need a model that can be easily understood and explained to stakeholders, Decision Trees are a great choice.\n",
        "\n",
        "2. **Mixed data types:** When dealing with datasets containing both categorical and numerical features, Decision Trees can handle them naturally.\n",
        "\n",
        "3. **Nonlinear relationships:** If the underlying relationship between features and the target variable is nonlinear, Decision Trees can be effective in capturing it.\n",
        "\n",
        "4. **Data exploration and feature selection:** Decision Trees can be used as a starting point to explore the data and identify important features that can be further used with other models.\n",
        "\n",
        "5. **Quick prototyping:** Decision Trees are relatively quick to build and evaluate, making them useful for initial model prototyping and exploration.\n",
        "\n",
        "6. **Ensemble learning:** Decision Trees are often used as base learners in ensemble methods like Random Forests and Gradient Boosting, which can enhance performance and reduce overfitting.\n",
        "\n",
        "However, in cases where you have a large dataset, want higher accuracy, and are concerned about overfitting, more advanced algorithms like Random Forests, Gradient Boosting Machines, or Deep Learning models may be more suitable."
      ],
      "metadata": {
        "id": "nDYZtZhQ7vZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "sNg6fN6qB9GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "mRfH1Qb5xzGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "7EWEXz4oQI-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import the required libraries:\n",
        "   - `numpy` (as `np`): A library for numerical computing in Python.\n",
        "   - `pandas` (as `pd`): A library for data manipulation and analysis in Python.\n",
        "   - `sklearn.datasets`: Part of the scikit-learn library, it provides several datasets for machine learning and data analysis.\n",
        "   - `sklearn.model_selection`: This module contains tools for model selection, including splitting data into train and test sets.\n",
        "   - `sklearn.tree`: This module provides tools for working with decision trees and related algorithms.\n",
        "   - `sklearn.metrics`: Contains various metrics for evaluating model performance.\n",
        "\n",
        "2. Load the Iris dataset:\n",
        "   - `load_iris()`: This function is part of scikit-learn and loads the famous Iris dataset. It is a dataset with 150 samples of iris flowers, with 4 features (sepal length, sepal width, petal length, petal width) and 3 classes (Iris setosa, Iris versicolor, Iris virginica).\n",
        "   - `X, y = iris.data, iris.target`: The variable `X` contains the features (input data), and `y` contains the target labels (output data).\n",
        "\n",
        "3. Split the data into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: This function is used to split the dataset into training and testing sets. It randomly shuffles the data and separates a portion (20% in this case) as the test set, leaving the rest as the training set. The `random_state` parameter is set to 42 to ensure reproducibility of the random splitting.\n",
        "\n",
        "4. Create a Decision Tree classifier:\n",
        "   - `DecisionTreeClassifier()`: This creates an instance of the DecisionTreeClassifier, which is a popular algorithm for classification tasks based on decision trees.\n",
        "\n",
        "5. Train the classifier on the training data:\n",
        "   - `clf.fit(X_train, y_train)`: The `fit` method is used to train the Decision Tree classifier on the training data (`X_train`, `y_train`). The classifier learns to make decisions based on the input features to predict the target labels.\n",
        "\n",
        "6. Make predictions on the test data:\n",
        "   - `y_pred = clf.predict(X_test)`: The `predict` method is used to make predictions on the test data (`X_test`). The trained classifier uses the learned rules to predict the target labels for the test samples.\n",
        "\n",
        "7. Calculate accuracy:\n",
        "   - `accuracy_score(y_test, y_pred)`: The `accuracy_score` function is used to calculate the accuracy of the model's predictions. It compares the predicted labels (`y_pred`) with the actual labels from the test set (`y_test`) and computes the fraction of correct predictions.\n",
        "\n",
        "8. Print the accuracy:\n",
        "   - `print(\"Accuracy:\", accuracy)`: The accuracy value is printed to the console, representing the fraction of correctly predicted samples in the test set.\n",
        "\n",
        "In summary, this code demonstrates the typical workflow for training and evaluating a Decision Tree classifier using the Iris dataset. It splits the data into training and testing sets, trains the classifier on the training data, makes predictions on the test data, and calculates and prints the accuracy of the model. The accuracy value indicates how well the model performs on unseen data, providing an estimate of its generalization performance."
      ],
      "metadata": {
        "id": "wOzmfbktiRwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "oJtwTRMfS3Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees have numerous real-world applications in healthcare settings. One such example is using decision trees to predict patient outcomes based on various clinical features. Let's consider a specific scenario:\n",
        "\n",
        "**Example: Predicting Diabetic Retinopathy Progression**\n",
        "\n",
        "**Problem:** Diabetic retinopathy is a common complication of diabetes that affects the eyes, leading to potential vision loss. Early detection and timely intervention are crucial to prevent further deterioration. We want to predict the progression of diabetic retinopathy in patients based on their clinical characteristics and test results.\n",
        "\n",
        "**Dataset:** A dataset is collected, consisting of patient records with features such as age, diabetes duration, blood sugar levels, blood pressure, cholesterol levels, results from various eye tests, etc. The dataset also includes the progression status of diabetic retinopathy for each patient (e.g., stable or progressing).\n",
        "\n",
        "**Objective:** Build a decision tree model to predict whether a patient's diabetic retinopathy will progress based on their clinical features.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Data Preprocessing:** The dataset needs to be preprocessed to handle missing values, categorical variables, and outliers. This step ensures the data is clean and suitable for modeling.\n",
        "\n",
        "2. **Feature Selection:** Select the relevant features that are most informative for predicting diabetic retinopathy progression. This process may involve domain expertise and statistical analysis.\n",
        "\n",
        "3. **Train-Test Split:** Split the dataset into training and testing sets. The training set will be used to build the decision tree model, and the testing set will be used to evaluate its performance.\n",
        "\n",
        "4. **Decision Tree Construction:** Using the training data, build a decision tree model that learns to map the clinical features to the diabetic retinopathy progression status.\n",
        "\n",
        "5. **Hyperparameter Tuning:** Fine-tune hyperparameters of the decision tree algorithm to improve its performance. For example, adjusting the tree depth, minimum samples per leaf, or impurity measure.\n",
        "\n",
        "6. **Model Evaluation:** Evaluate the decision tree model on the testing set to assess its accuracy, precision, recall, and other relevant metrics.\n",
        "\n",
        "7. **Interpretation:** Analyze the decision tree to understand which clinical features are most critical in predicting the progression of diabetic retinopathy. This interpretability is a significant advantage of decision trees, especially in healthcare settings where explanations for predictions are vital.\n",
        "\n",
        "8. **Validation and Deployment:** The decision tree model needs to be validated on new, unseen data to ensure its generalization performance. Once the model is deemed satisfactory, it can be deployed in a clinical setting to assist healthcare professionals in making decisions about patient care.\n",
        "\n",
        "By using a decision tree for this task, healthcare providers can quickly identify patients at higher risk of diabetic retinopathy progression, enabling them to take proactive measures to prevent vision loss and improve patient outcomes. Additionally, the decision tree's interpretability allows clinicians to understand the reasoning behind the predictions, enhancing trust and adoption of the model in real-world practice."
      ],
      "metadata": {
        "id": "G3s_b75r8i1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "hOV2rkyBaKiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Decision Tree model?\n",
        "A Decision Tree is a popular supervised machine learning algorithm used for both classification and regression tasks. It recursively splits the dataset into subsets based on the most significant feature, creating a tree-like structure of decisions that lead to the final prediction.\n",
        "\n",
        "2. How does a Decision Tree make decisions?\n",
        "The Decision Tree makes decisions by following a path from the root node to a leaf node. At each internal node, it tests a specific feature and branches down the tree based on the feature's value. This process continues until it reaches a leaf node, which contains the final prediction.\n",
        "\n",
        "3. What are the advantages of using Decision Trees?\n",
        "- Decision Trees are easy to interpret and visualize, making them useful for explaining the decision-making process to non-experts.\n",
        "- They can handle both numerical and categorical data.\n",
        "- Decision Trees require relatively little data preprocessing compared to other algorithms.\n",
        "- They are capable of handling both classification and regression tasks.\n",
        "\n",
        "4. What are the limitations of Decision Trees?\n",
        "- Decision Trees can easily overfit the training data, leading to poor generalization on unseen data.\n",
        "- They are sensitive to small changes in the data and can produce different trees for slightly different training sets.\n",
        "- Decision Trees may not be the best option for capturing complex relationships between variables.\n",
        "- They can be biased towards features with more levels or values.\n",
        "\n",
        "5. How can we overcome overfitting in Decision Trees?\n",
        "There are several techniques to address overfitting in Decision Trees:\n",
        "- Pruning: Removing nodes that do not significantly improve the model's performance on validation data.\n",
        "- Setting a minimum number of samples required to split a node or a minimum number of samples required in a leaf node.\n",
        "- Limiting the depth of the tree.\n",
        "\n",
        "6. Can Decision Trees handle missing values?\n",
        "Yes, Decision Trees can handle missing values by using surrogate splits. A surrogate split is a substitute split that is used when the primary split is not possible due to missing values in a feature.\n",
        "\n",
        "7. Are there any ensemble methods based on Decision Trees?\n",
        "Yes, two popular ensemble methods based on Decision Trees are Random Forests and Gradient Boosting Machines (GBMs). Random Forests build multiple Decision Trees and aggregate their predictions, while GBMs build trees sequentially, focusing on the errors of the previous tree.\n",
        "\n",
        "8. In what fields are Decision Trees commonly used?\n",
        "Decision Trees find applications in various fields, such as finance (credit risk assessment), medicine (diagnosis), marketing (customer segmentation), and natural language processing (sentiment analysis), among others.\n",
        "\n",
        "9. Can Decision Trees handle multi-output problems?\n",
        "Yes, Decision Trees can handle multi-output problems, making them suitable for multi-label classification tasks where each instance can be associated with multiple target labels.\n",
        "\n",
        "10. Are there any popular Python libraries for building Decision Trees?\n",
        "Yes, there are popular libraries in Python like scikit-learn and XGBoost that provide implementations for Decision Trees and ensemble methods based on them."
      ],
      "metadata": {
        "id": "nlYGsQ1JYiaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "ggAWml2xb5z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:** What type of machine learning algorithm is a Decision Tree?\n",
        "a) Supervised Learning\n",
        "b) Unsupervised Learning\n",
        "c) Semi-Supervised Learning\n",
        "d) Reinforcement Learning\n",
        "\n",
        "**Question 2:** What is the main purpose of a Decision Tree in machine learning?\n",
        "a) Clustering\n",
        "b) Dimensionality Reduction\n",
        "c) Classification and Regression\n",
        "d) Anomaly Detection\n",
        "\n",
        "**Question 3:** In a Decision Tree, what are the internal nodes responsible for?\n",
        "a) Representing the output classes\n",
        "b) Storing the training instances\n",
        "c) Making predictions\n",
        "d) Asking questions about the features\n",
        "\n",
        "**Question 4:** How does a Decision Tree handle continuous (numeric) features during the splitting process?\n",
        "a) By creating a separate branch for each unique value\n",
        "b) By transforming them into categorical features\n",
        "c) By calculating the average and using it as a threshold\n",
        "d) By ignoring them during the splitting process\n",
        "\n",
        "**Question 5:** What is the goal of the splitting criterion used in a Decision Tree?\n",
        "a) To maximize the number of nodes\n",
        "b) To minimize the number of leaf nodes\n",
        "c) To maximize the information gain or Gini impurity reduction\n",
        "d) To equally distribute the classes among all nodes\n",
        "\n",
        "**Question 6:** What is \"Pruning\" in the context of Decision Trees?\n",
        "a) Removing irrelevant features from the dataset\n",
        "b) Trimming the tree by removing branches that provide little predictive power\n",
        "c) Enlarging the tree by adding more nodes\n",
        "d) Adjusting the hyperparameters of the tree algorithm\n",
        "\n",
        "**Question 7:** Which of the following is a disadvantage of using a Decision Tree?\n",
        "a) It is not suitable for both classification and regression tasks\n",
        "b) It cannot handle missing values in the dataset\n",
        "c) It tends to overfit the training data\n",
        "d) It requires a large amount of computational resources\n",
        "\n",
        "**Question 8:** What is the maximum depth of a Decision Tree?\n",
        "a) The number of features in the dataset\n",
        "b) The number of leaf nodes\n",
        "c) The square root of the number of data points\n",
        "d) A hyperparameter that can be set by the user\n",
        "\n",
        "**Question 9:** When would you prefer to use an ensemble of Decision Trees, such as Random Forest, instead of a single Decision Tree?\n",
        "a) When you have limited computational resources\n",
        "b) When you need a small and interpretable model\n",
        "c) When you want to reduce bias in the predictions\n",
        "d) When you want to improve the model's performance and generalization\n",
        "\n",
        "**Question 10:** Which algorithm can be used to build a Decision Tree for both classification and regression tasks?\n",
        "a) ID3\n",
        "b) k-means\n",
        "c) C4.5\n",
        "d) CART\n",
        "\n",
        "**Answers:**\n",
        "1. a) Supervised Learning\n",
        "2. c) Classification and Regression\n",
        "3. d) Asking questions about the features\n",
        "4. c) By calculating the average and using it as a threshold\n",
        "5. c) To maximize the information gain or Gini impurity reduction\n",
        "6. b) Trimming the tree by removing branches that provide little predictive power\n",
        "7. c) It tends to overfit the training data\n",
        "8. d) A hyperparameter that can be set by the user\n",
        "9. d) When you want to improve the model's performance and generalization\n",
        "10. d) CART"
      ],
      "metadata": {
        "id": "jMSoTUVob7LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "hVwxGtTZtc5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Prediction**:\n",
        "    - Predict the likelihood of a patient getting a certain disease (like diabetes, heart disease) based on medical records and personal attributes.\n",
        "\n",
        "2. **Treatment Recommendation**:\n",
        "    - Recommend the most appropriate treatment for a patient based on their symptoms, medical history, and other factors.\n",
        "\n",
        "3. **Hospital Readmission Prediction**:\n",
        "    - Based on a patient's initial information and treatment, predict whether they are likely to be readmitted to the hospital within a specified timeframe (e.g., 30 days).\n",
        "\n",
        "4. **Patient Triage System**:\n",
        "    - Design a triage system that categorizes emergency room patients into various priority levels based on their symptoms and vitals.\n",
        "\n",
        "5. **Cost Prediction**:\n",
        "    - Predict the cost of treatment for patients based on their diagnosis, treatment plan, and other relevant factors.\n",
        "\n",
        "6. **Medication Side-effects Prediction**:\n",
        "    - Predict potential side-effects a patient might experience from medication based on their medical history and genetic data.\n",
        "\n",
        "7. **Optimizing Hospital Operations**:\n",
        "    - Use decision trees to optimize hospital bed allocation or operation theater schedules based on patient data and predicted length of stay.\n",
        "\n",
        "8. **Predictive Maintenance for Medical Equipment**:\n",
        "    - Predict when hospital equipment (like MRI machines) are likely to fail or need maintenance.\n",
        "\n",
        "9. **Healthcare Fraud Detection**:\n",
        "    - Design a system that detects fraudulent healthcare claims based on patterns in claim data.\n",
        "\n",
        "10. **Personalized Health Plans**:\n",
        "    - Recommend personalized health plans or exercises for individuals based on their health data and preferences.\n",
        "\n",
        "11. **Genetic Testing & Disease Susceptibility**:\n",
        "    - Predict an individual's susceptibility to certain genetic disorders based on their genetic data.\n",
        "\n",
        "12. **Epidemic Outbreak Prediction**:\n",
        "    - Predict potential outbreaks or hotspots for diseases like flu or dengue based on regional health data.\n",
        "\n",
        "13. **Patient No-show Prediction**:\n",
        "    - Predict which patients are likely to miss their appointments, allowing for better scheduling and reminder systems.\n",
        "\n",
        "14. **Optimizing Telemedicine**:\n",
        "    - Predict the most effective way to offer telemedicine consultations (audio vs. video, time of day, etc.) based on patient demographics and technology availability.\n",
        "\n",
        "15. **Post-surgical Complication Prediction**:\n",
        "    - Predict the likelihood of complications after surgeries based on patient data and type of surgery.\n"
      ],
      "metadata": {
        "id": "eeCM7_ZHtgG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "XT9mvtPZbqbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pima Indian Diabetes dataset is a popular dataset for binary classification. It contains 8 input variables and 1 output variable. The goal is to predict whether a patient has diabetes based on various diagnostic measurements.\n",
        "\n",
        "Here's a simple example of how to use a Decision Tree classifier on this dataset using Python's `scikit-learn` library:\n",
        "\n",
        "1. First, let's install the required libraries and download the dataset:"
      ],
      "metadata": {
        "id": "EzrydoAKbup-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Split the data into input and output variables\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split the data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree model and fit it to the training data\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "HsddmTiQb0J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After running the above code, you'll see the accuracy of the model on the test set as well as a classification report. Please note that decision trees can be prone to overfitting, especially on small datasets like this. You may want to consider tuning the model or using a different approach like Random Forests for improved performance.\n",
        "\n",
        "You can adjust the hyperparameters of the `DecisionTreeClassifier` such as `max_depth`, `min_samples_split`, and `min_samples_leaf` to tune the tree's depth and complexity, which can help with overfitting."
      ],
      "metadata": {
        "id": "ZolXJVjjbsw5"
      }
    }
  ]
}