{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVJqDVQcuuYIFqiBoW50cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Ridge_Regression_(L2_Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression (L2 Regularization) Model Background"
      ],
      "metadata": {
        "id": "6ggIdPdXA_9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression, also known as L2 regularization, is a linear regression technique that adds a penalty term to the cost function to prevent overfitting and stabilize the model's coefficients. It is particularly useful when dealing with multicollinearity (high correlation between predictor variables) and when there are more predictors than observations in the dataset (a situation known as the \"p > n\" problem).\n",
        "\n",
        "The Ridge Regression cost function is defined as follows:\n",
        "\n",
        "Cost = Sum of Squared Residuals + λ * (sum of squared coefficients)\n",
        "\n",
        "Where:\n",
        "- Sum of Squared Residuals: The typical linear regression term that measures the difference between the predicted values and the actual target values.\n",
        "- λ (lambda): The regularization parameter, also known as the penalty term or regularization strength. It's a hyperparameter that determines the amount of regularization applied to the model.\n",
        "- Sum of Squared Coefficients: The term that penalizes the model for large coefficients. Ridge regression aims to minimize the coefficients, making them close to zero.\n",
        "\n",
        "**Pros of Ridge Regression (L2 Regularization)**:\n",
        "1. Reduces overfitting: By penalizing large coefficients, Ridge Regression helps prevent overfitting and improves the generalization of the model.\n",
        "2. Handles multicollinearity: When predictor variables are highly correlated, ordinary least squares regression may result in unstable coefficient estimates. Ridge Regression can mitigate this issue by shrinking the coefficients toward zero.\n",
        "3. Works well with \"p > n\" problem: When the number of predictors exceeds the number of observations, Ridge Regression remains effective by regularizing the model.\n",
        "\n",
        "**Cons of Ridge Regression (L2 Regularization)**:\n",
        "1. Adds complexity: Introducing the regularization term adds complexity to the model and requires the tuning of the hyperparameter λ.\n",
        "2. Biased estimates: While Ridge Regression improves coefficient stability, it can introduce a slight bias in the coefficient estimates, pulling them toward zero.\n",
        "3. Non-sparse solutions: Ridge Regression tends to produce non-zero coefficients for all predictors, which might not be desirable when feature selection is important.\n",
        "\n",
        "**When to use Ridge Regression**:\n",
        "Ridge Regression should be considered in the following situations:\n",
        "1. Multicollinearity: When you have highly correlated predictor variables, Ridge Regression can help stabilize the coefficient estimates.\n",
        "2. Preventing overfitting: When dealing with complex models or datasets with noise, Ridge Regression can be used to reduce overfitting and improve model generalization.\n",
        "3. \"p > n\" problem: When the number of predictors is larger than the number of observations, Ridge Regression can still provide reasonable results by applying regularization.\n",
        "\n",
        "It's worth noting that if you suspect that some predictors are irrelevant or should have exactly zero coefficients (i.e., performing feature selection), you may consider using Lasso Regression (L1 regularization) instead of Ridge Regression, as Lasso tends to produce sparse solutions with exactly zero coefficients for some predictors. The choice between Ridge and Lasso (or Elastic Net, a combination of L1 and L2 regularization) depends on the specific characteristics of your dataset and the objectives of your modeling task."
      ],
      "metadata": {
        "id": "FbtSehZHEVee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "BC2624ykbHmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries (uncomment if needed)\n",
        "# !pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 2: Generate some sample data\n",
        "# Let's create a simple dataset with a linear relationship y = 2x + 3, but with some noise added.\n",
        "\n",
        "np.random.seed(42)\n",
        "X = 5 * np.random.rand(100, 1)\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create and train the Ridge Regression model\n",
        "alpha = 1.0  # The regularization strength. You can adjust this value to control the regularization effect.\n",
        "ridge_model = Ridge(alpha=alpha)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = ridge_model.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate the mean squared error on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Step 7: Plot the results\n",
        "plt.scatter(X, y, color='b', label='Original data')\n",
        "plt.plot(X_test, y_pred, color='r', label='Ridge Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Ridge Regression (L2 Regularization)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C08BHc6fRU4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "lFKCA1q_0vRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 1: Install required libraries (uncomment if needed)**\n",
        "\n",
        "This step is a comment indicating that if you are running the code in an environment where the required libraries are not installed, you can uncomment the line `!pip install scikit-learn` to install scikit-learn library using pip.\n",
        "\n",
        "**Step 2: Generate some sample data**\n",
        "\n",
        "- `np.random.seed(42)`: This sets the random seed to ensure reproducibility of the results.\n",
        "- `X = 5 * np.random.rand(100, 1)`: Generates an array `X` of shape (100, 1) containing random numbers between 0 and 5. This will be the input feature for our regression problem.\n",
        "- `y = 2 * X + 3 + np.random.randn(100, 1)`: Generates the target labels `y` by adding some random noise to the linear relationship `y = 2X + 3`. This simulates a simple dataset with a linear relationship and some noise.\n",
        "\n",
        "**Step 3: Split the data into training and testing sets**\n",
        "\n",
        "- `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data into training and testing sets. 20% of the data is used for testing, and 80% is used for training. The `random_state` parameter sets the random seed for reproducibility.\n",
        "\n",
        "**Step 4: Create and train the Ridge Regression model**\n",
        "\n",
        "- `alpha = 1.0`: This is the regularization strength hyperparameter for Ridge Regression, controlling the amount of regularization applied to the model. You can adjust this value to control the regularization effect.\n",
        "- `ridge_model = Ridge(alpha=alpha)`: Creates a Ridge Regression model with the specified regularization strength (`alpha`).\n",
        "- `ridge_model.fit(X_train, y_train)`: Trains the Ridge Regression model on the training data (`X_train` and `y_train`).\n",
        "\n",
        "**Step 5: Make predictions on the test set**\n",
        "\n",
        "- `y_pred = ridge_model.predict(X_test)`: Uses the trained Ridge Regression model to make predictions on the test set (`X_test`).\n",
        "\n",
        "**Step 6: Calculate the mean squared error on the test set**\n",
        "\n",
        "- `mse = mean_squared_error(y_test, y_pred)`: Calculates the mean squared error (MSE) between the true labels (`y_test`) and the predicted labels (`y_pred`). The MSE is a common metric to evaluate regression models, measuring the average squared difference between the true and predicted values.\n",
        "\n",
        "**Step 7: Plot the results**\n",
        "\n",
        "- `plt.scatter(X, y, color='b', label='Original data')`: Plots the original data points as blue dots.\n",
        "- `plt.plot(X_test, y_pred, color='r', label='Ridge Regression')`: Plots the predicted values (`y_pred`) as a red line using the input data `X_test`.\n",
        "- `plt.xlabel('X')`: Sets the x-axis label to 'X'.\n",
        "- `plt.ylabel('y')`: Sets the y-axis label to 'y'.\n",
        "- `plt.legend()`: Shows the legend indicating the original data and the Ridge Regression line.\n",
        "- `plt.title('Ridge Regression (L2 Regularization)')`: Sets the title of the plot to 'Ridge Regression (L2 Regularization)'.\n",
        "- `plt.show()`: Displays the plot with the original data points and the Ridge Regression line.\n",
        "\n",
        "The output will show the mean squared error (MSE) and the plot will visualize how the Ridge Regression model fits the data. The red line represents the predictions made by the Ridge Regression model, approximating the underlying linear relationship of the data."
      ],
      "metadata": {
        "id": "HjiBd_8T1wCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "MOHNJnhf2Fys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression (L2 Regularization) is a popular linear regression technique used in various fields, including healthcare, to handle multicollinearity and prevent overfitting. One real-world example of its application in the healthcare setting is predicting patient medical costs based on various factors.\n",
        "\n",
        "Let's consider a scenario where a healthcare provider wants to predict the medical expenses (hospitalization costs) of patients based on their age, BMI (Body Mass Index), and the number of chronic conditions they have. The data collected from previous patients includes age, BMI, number of chronic conditions, and their corresponding medical expenses.\n",
        "\n",
        "Here's how Ridge Regression could be applied in this context:\n",
        "\n",
        "Step 1: Data Collection\n",
        "Collect a dataset containing patient information and their medical expenses.\n",
        "\n",
        "| Patient ID | Age | BMI | Chronic Conditions | Medical Expenses |\n",
        "|------------|-----|-----|--------------------|------------------|\n",
        "| 1          | 45  | 26  | 2                  | $5,000           |\n",
        "| 2          | 62  | 29  | 3                  | $8,000           |\n",
        "| 3          | 38  | 21  | 1                  | $3,500           |\n",
        "| ...        | ... | ... | ...                | ...              |\n",
        "\n",
        "Step 2: Data Preprocessing\n",
        "Normalize or standardize the data to ensure all features are on the same scale.\n",
        "\n",
        "Step 3: Ridge Regression Model Building\n",
        "Build a Ridge Regression model with age, BMI, and the number of chronic conditions as the independent variables and medical expenses as the dependent variable.\n",
        "\n",
        "Step 4: L2 Regularization\n",
        "In Ridge Regression, L2 regularization is applied to the model. It adds a penalty term to the linear regression cost function, which is proportional to the square of the magnitude of the coefficients. The regularization parameter, lambda (λ), controls the amount of regularization applied. Higher values of λ lead to stronger regularization and help prevent overfitting.\n",
        "\n",
        "Step 5: Model Evaluation\n",
        "Evaluate the model's performance using appropriate metrics, such as Mean Squared Error (MSE) or R-squared (R2).\n",
        "\n",
        "Step 6: Predicting Medical Expenses\n",
        "Once the model is trained and evaluated, it can be used to predict the medical expenses of new patients based on their age, BMI, and number of chronic conditions.\n",
        "\n",
        "The Ridge Regression model with L2 regularization helps to avoid overfitting, especially when there is multicollinearity among the predictor variables (e.g., age and BMI might be correlated). The regularization term limits the magnitude of the coefficients, making the model more robust and stable when dealing with new patient data.\n",
        "\n",
        "By predicting medical expenses accurately, healthcare providers can better manage their resources, plan for patient treatments, and estimate potential healthcare costs for patients with different characteristics."
      ],
      "metadata": {
        "id": "VyPHYCIwIAox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "QSkDppOS3hhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is Ridge Regression, and how does it differ from ordinary linear regression?**\n",
        "   Ridge Regression is a linear regression model with L2 regularization. It adds a penalty term based on the square of the coefficients' magnitudes to the ordinary linear regression objective function. This penalty helps to prevent overfitting and reduces the impact of multicollinearity in the data.\n",
        "\n",
        "2. **Why is Ridge Regression also called L2 Regularization?**\n",
        "   Ridge Regression is called L2 Regularization because the regularization term added to the objective function is the L2 norm (Euclidean norm) of the coefficients vector. It is the sum of the squares of the coefficients multiplied by a regularization parameter (lambda).\n",
        "\n",
        "3. **How does Ridge Regression handle multicollinearity?**\n",
        "   Multicollinearity refers to the situation when independent variables in a regression model are highly correlated with each other. Ridge Regression addresses multicollinearity by shrinking the coefficients towards zero, which reduces their magnitudes and prevents the model from relying too heavily on any one variable.\n",
        "\n",
        "4. **When should I use Ridge Regression over ordinary linear regression?**\n",
        "   Ridge Regression should be used when you suspect multicollinearity among the predictor variables or when the model tends to overfit the training data. It is especially useful when the number of predictors is larger than the number of data points.\n",
        "\n",
        "5. **What is the significance of the regularization parameter (lambda) in Ridge Regression?**\n",
        "   The regularization parameter, often denoted as lambda (λ), controls the amount of regularization applied to the model. A higher value of lambda will result in stronger regularization, leading to more shrinkage of coefficients. Conversely, a lower value of lambda reduces the regularization effect, making the model similar to ordinary linear regression.\n",
        "\n",
        "6. **Does Ridge Regression eliminate any variables from the model?**\n",
        "   Ridge Regression does not eliminate variables entirely from the model; instead, it shrinks their coefficients. In practice, it keeps all the predictors in the model, but some may have very small coefficients close to zero.\n",
        "\n",
        "7. **What are the advantages of using Ridge Regression?**\n",
        "   Ridge Regression helps in stabilizing the model by reducing the impact of multicollinearity and handling situations with high-dimensional data effectively. It can improve the model's generalization and prevent overfitting.\n",
        "\n",
        "8. **Are there any limitations to using Ridge Regression?**\n",
        "   Ridge Regression may not perform well when the relationship between the predictors and the target variable is truly sparse (i.e., when only a few predictors are significant). In such cases, other regularization techniques like LASSO (L1 Regularization) might be more appropriate.\n",
        "\n",
        "9. **How can I choose the optimal value of the regularization parameter (lambda)?**\n",
        "   The optimal value of lambda in Ridge Regression can be determined through techniques like cross-validation, grid search, or using regularization paths. These methods help in finding the lambda that provides the best trade-off between model simplicity and predictive performance.\n",
        "\n",
        "10. **Can Ridge Regression handle categorical variables?**\n",
        "    Yes, Ridge Regression can handle categorical variables, but they need to be encoded properly (e.g., using one-hot encoding) to be used in the model. Proper encoding ensures that the regularization is applied appropriately to the categorical variables as well.\n",
        "\n",
        "Remember that Ridge Regression is just one of many regularization techniques, and the choice of the appropriate technique depends on the characteristics of your data and the goals of your analysis."
      ],
      "metadata": {
        "id": "SqPgAjm18J4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "T-X4Iq2-h7uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is the main purpose of Ridge Classification?\n",
        "\n",
        "a) To perform binary classification using linear regression.\n",
        "b) To perform multi-class classification using linear regression.\n",
        "c) To improve the performance of a Logistic Regression model by reducing overfitting through L2 regularization.\n",
        "d) To increase model complexity by removing regularization.\n",
        "\n",
        "**Question 2:** What problem does L2 regularization in Ridge Classification address?\n",
        "\n",
        "a) Underfitting\n",
        "b) High variance and overfitting\n",
        "c) Low bias and high variance\n",
        "d) Data preprocessing challenges\n",
        "\n",
        "**Question 3:** In Ridge Classification, what is the key idea behind L2 regularization?\n",
        "\n",
        "a) Adding more features to the model to increase complexity.\n",
        "b) Adding a penalty term based on the absolute values of the coefficients.\n",
        "c) Adding a penalty term based on the squared values of the coefficients.\n",
        "d) Removing features to reduce the dimensionality of the data.\n",
        "\n",
        "**Question 4:** Which parameter controls the strength of the L2 regularization penalty in Ridge Classification?\n",
        "\n",
        "a) Learning rate\n",
        "b) Batch size\n",
        "c) Lambda (λ)\n",
        "d) Dropout rate\n",
        "\n",
        "**Question 5:** As the value of the L2 regularization parameter (λ) increases in Ridge Classification, what happens to the magnitude of the coefficients?\n",
        "\n",
        "a) They become larger.\n",
        "b) They become more sparse.\n",
        "c) They remain unchanged.\n",
        "d) They become smaller and approach zero.\n",
        "\n",
        "**Question 6:** Which of the following statements about Ridge Classification is true?\n",
        "\n",
        "a) Ridge Classification only works well with small datasets.\n",
        "b) Ridge Classification is not suitable for high-dimensional datasets.\n",
        "c) Ridge Classification always outperforms standard Logistic Regression.\n",
        "d) Ridge Classification can help prevent multicollinearity by shrinking correlated coefficients.\n",
        "\n",
        "**Question 7:** In Ridge Classification, how is the value of the L2 regularization parameter (λ) typically determined?\n",
        "\n",
        "a) By using a fixed value such as 1.0.\n",
        "b) Through trial and error without any specific guidelines.\n",
        "c) By finding the value that minimizes the training error.\n",
        "d) Through techniques like cross-validation to find the optimal value that balances bias and variance.\n",
        "\n",
        "**Question 8:** Which evaluation metric(s) is/are commonly used for assessing the performance of Ridge Classification?\n",
        "\n",
        "a) Mean Absolute Error (MAE)\n",
        "b) R-squared (R2)\n",
        "c) Accuracy, Precision, Recall, and F1-score\n",
        "d) Area Under the Curve (AUC)\n",
        "\n",
        "**Question 9:** What is the primary advantage of Ridge Classification over standard Logistic Regression?\n",
        "\n",
        "a) Simplicity and ease of implementation.\n",
        "b) Better handling of imbalanced datasets.\n",
        "c) Improved predictive performance by reducing overfitting.\n",
        "d) Ability to handle non-linear relationships in data.\n",
        "\n",
        "**Question 10:** In Ridge Classification, when should you consider using the regularization parameter λ?\n",
        "\n",
        "a) Only when your model is underfitting.\n",
        "b) Only when your model is overfitting.\n",
        "c) In both cases of underfitting and overfitting.\n",
        "d) Only when your model's bias is too high.\n",
        "\n",
        "**Answers:**\n",
        "\n",
        "1. c) To improve the performance of a Logistic Regression model by reducing overfitting through L2 regularization.\n",
        "2. b) High variance and overfitting.\n",
        "3. c) Adding a penalty term based on the squared values of the coefficients.\n",
        "4. c) Lambda (λ)\n",
        "5. d) They become smaller and approach zero.\n",
        "6. d) Ridge Classification can help prevent multicollinearity by shrinking correlated coefficients.\n",
        "7. d) Through techniques like cross-validation to find the optimal value that balances bias and variance.\n",
        "8. c) Accuracy, Precision, Recall, and F1-score.\n",
        "9. c) Improved predictive performance by reducing overfitting.\n",
        "10. c) In both cases of underfitting and overfitting."
      ],
      "metadata": {
        "id": "kRXKtTqQh_qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "17Tc3BWN2cUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Predicting Patient Length of Stay**\n",
        "    - **Dataset:** Hospital inpatient datasets with features like diagnosis, patient age, gender, previous admissions, comorbidities, etc.\n",
        "    - **Objective:** Predict the length of stay for patients, which can help in resource allocation and hospital management.\n",
        "\n",
        "2. **Disease Diagnosis**\n",
        "    - **Dataset:** Medical datasets related to specific diseases, e.g., breast cancer dataset with features like tumor size, age, and other diagnostic measures.\n",
        "    - **Objective:** Diagnose the presence or absence of the disease based on the available features.\n",
        "\n",
        "3. **Drug Response Prediction**\n",
        "    - **Dataset:** Genomic data or patient medical records where different patients receive a specific medication.\n",
        "    - **Objective:** Predict how effectively a patient will respond to a drug based on their genomic or medical data.\n",
        "\n",
        "4. **Readmission Risk**\n",
        "    - **Dataset:** Hospital readmission data with features like initial diagnosis, treatment given, patient’s age, etc.\n",
        "    - **Objective:** Predict the likelihood of a patient getting readmitted within 30 days.\n",
        "\n",
        "5. **Medical Cost Prediction**\n",
        "    - **Dataset:** Patient datasets with features like age, BMI, smoking status, number of children, etc.\n",
        "    - **Objective:** Predict the medical charges for patients to aid in insurance premium estimation.\n",
        "\n",
        "6. **Heart Disease Prediction**\n",
        "    - **Dataset:** Cardiovascular datasets with features like cholesterol levels, age, blood pressure, etc.\n",
        "    - **Objective:** Predict the risk of heart disease for patients.\n",
        "\n",
        "7. **Disease Progression Modeling**\n",
        "    - **Dataset:** Data on patients with chronic diseases like diabetes, capturing features over time such as blood sugar levels, medication dosage, weight, etc.\n",
        "    - **Objective:** Model the progression of the disease to inform treatment plans.\n",
        "\n",
        "8. **Impact of Lifestyle on Health**\n",
        "    - **Dataset:** Survey data capturing information on diet, exercise, sleep, stress, etc., along with health metrics like BMI, cholesterol, blood pressure.\n",
        "    - **Objective:** Understand and predict how different lifestyle factors influence specific health outcomes.\n",
        "\n",
        "9. **Mental Health Outcomes**\n",
        "    - **Dataset:** Datasets capturing various factors like trauma exposure, substance use, genetic data, etc., of individuals diagnosed with mental health disorders.\n",
        "    - **Objective:** Predict the severity or prognosis of mental health conditions based on the given features.\n",
        "\n",
        "10. **Predicting Treatment Outcomes**\n",
        "    - **Dataset:** Data of patients undergoing a specific treatment with outcomes captured post-treatment.\n",
        "    - **Objective:** Predict the success or outcome of a treatment based on patient characteristics and treatment details.\n",
        "\n"
      ],
      "metadata": {
        "id": "qkBaPmKc2emp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "nrvzNSL0ET3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a working example of Ridge Regression using a real-world health dataset. In this example, I'll use the `diabetes` dataset from the `sklearn.datasets` module, which is a commonly used dataset for regression tasks.\n",
        "\n",
        "Ridge Regression is a linear regression technique that adds an L2 regularization term to the linear regression cost function. This helps to prevent overfitting and stabilize the model's coefficients. The regularization term is controlled by a parameter called the alpha parameter.\n",
        "\n",
        "Here's how you can implement Ridge Regression using Python and the `scikit-learn` library:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rFUDS6VG4MO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "alpha = 1.0  # Regularization strength, you can experiment with different values\n",
        "ridge_model = Ridge(alpha=alpha)\n",
        "\n",
        "# Train the model\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "quOfhUgb4Cdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `alpha` parameter controls the strength of regularization. You can experiment with different values of `alpha` to see how it affects the model's performance. A larger value of `alpha` will result in stronger regularization, which may lead to a simpler model with potentially higher bias but lower variance.\n",
        "\n",
        "Remember that this example uses the `diabetes` dataset for demonstration purposes. In a real-world scenario, you would replace the dataset with your actual health data. Additionally, preprocessing steps like handling missing values, feature engineering, and feature selection might be necessary depending on the specifics of your dataset."
      ],
      "metadata": {
        "id": "VUzR62bu4UCB"
      }
    }
  ]
}