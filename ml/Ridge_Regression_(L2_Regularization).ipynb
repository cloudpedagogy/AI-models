{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrBJi1KCukLIPZVZqYXqe+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/Ridge_Regression_(L2_Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "6ggIdPdXA_9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression, also known as L2 regularization, is a linear regression technique that adds a penalty term to the cost function to prevent overfitting and stabilize the model's coefficients. It is particularly useful when dealing with multicollinearity (high correlation between predictor variables) and when there are more predictors than observations in the dataset (a situation known as the \"p > n\" problem).\n",
        "\n",
        "The Ridge Regression cost function is defined as follows:\n",
        "\n",
        "Cost = Sum of Squared Residuals + λ * (sum of squared coefficients)\n",
        "\n",
        "Where:\n",
        "- Sum of Squared Residuals: The typical linear regression term that measures the difference between the predicted values and the actual target values.\n",
        "- λ (lambda): The regularization parameter, also known as the penalty term or regularization strength. It's a hyperparameter that determines the amount of regularization applied to the model.\n",
        "- Sum of Squared Coefficients: The term that penalizes the model for large coefficients. Ridge regression aims to minimize the coefficients, making them close to zero.\n",
        "\n",
        "Pros of Ridge Regression (L2 Regularization):\n",
        "1. Reduces overfitting: By penalizing large coefficients, Ridge Regression helps prevent overfitting and improves the generalization of the model.\n",
        "2. Handles multicollinearity: When predictor variables are highly correlated, ordinary least squares regression may result in unstable coefficient estimates. Ridge Regression can mitigate this issue by shrinking the coefficients toward zero.\n",
        "3. Works well with \"p > n\" problem: When the number of predictors exceeds the number of observations, Ridge Regression remains effective by regularizing the model.\n",
        "\n",
        "Cons of Ridge Regression (L2 Regularization):\n",
        "1. Adds complexity: Introducing the regularization term adds complexity to the model and requires the tuning of the hyperparameter λ.\n",
        "2. Biased estimates: While Ridge Regression improves coefficient stability, it can introduce a slight bias in the coefficient estimates, pulling them toward zero.\n",
        "3. Non-sparse solutions: Ridge Regression tends to produce non-zero coefficients for all predictors, which might not be desirable when feature selection is important.\n",
        "\n",
        "When to use Ridge Regression:\n",
        "Ridge Regression should be considered in the following situations:\n",
        "1. Multicollinearity: When you have highly correlated predictor variables, Ridge Regression can help stabilize the coefficient estimates.\n",
        "2. Preventing overfitting: When dealing with complex models or datasets with noise, Ridge Regression can be used to reduce overfitting and improve model generalization.\n",
        "3. \"p > n\" problem: When the number of predictors is larger than the number of observations, Ridge Regression can still provide reasonable results by applying regularization.\n",
        "\n",
        "It's worth noting that if you suspect that some predictors are irrelevant or should have exactly zero coefficients (i.e., performing feature selection), you may consider using Lasso Regression (L1 regularization) instead of Ridge Regression, as Lasso tends to produce sparse solutions with exactly zero coefficients for some predictors. The choice between Ridge and Lasso (or Elastic Net, a combination of L1 and L2 regularization) depends on the specific characteristics of your dataset and the objectives of your modeling task."
      ],
      "metadata": {
        "id": "FbtSehZHEVee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "BC2624ykbHmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries (uncomment if needed)\n",
        "# !pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 2: Generate some sample data\n",
        "# Let's create a simple dataset with a linear relationship y = 2x + 3, but with some noise added.\n",
        "\n",
        "np.random.seed(42)\n",
        "X = 5 * np.random.rand(100, 1)\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create and train the Ridge Regression model\n",
        "alpha = 1.0  # The regularization strength. You can adjust this value to control the regularization effect.\n",
        "ridge_model = Ridge(alpha=alpha)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = ridge_model.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate the mean squared error on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Step 7: Plot the results\n",
        "plt.scatter(X, y, color='b', label='Original data')\n",
        "plt.plot(X_test, y_pred, color='r', label='Ridge Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Ridge Regression (L2 Regularization)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C08BHc6fRU4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "lFKCA1q_0vRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 1: Install required libraries (uncomment if needed)**\n",
        "\n",
        "This step is a comment indicating that if you are running the code in an environment where the required libraries are not installed, you can uncomment the line `!pip install scikit-learn` to install scikit-learn library using pip.\n",
        "\n",
        "**Step 2: Generate some sample data**\n",
        "\n",
        "- `np.random.seed(42)`: This sets the random seed to ensure reproducibility of the results.\n",
        "- `X = 5 * np.random.rand(100, 1)`: Generates an array `X` of shape (100, 1) containing random numbers between 0 and 5. This will be the input feature for our regression problem.\n",
        "- `y = 2 * X + 3 + np.random.randn(100, 1)`: Generates the target labels `y` by adding some random noise to the linear relationship `y = 2X + 3`. This simulates a simple dataset with a linear relationship and some noise.\n",
        "\n",
        "**Step 3: Split the data into training and testing sets**\n",
        "\n",
        "- `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data into training and testing sets. 20% of the data is used for testing, and 80% is used for training. The `random_state` parameter sets the random seed for reproducibility.\n",
        "\n",
        "**Step 4: Create and train the Ridge Regression model**\n",
        "\n",
        "- `alpha = 1.0`: This is the regularization strength hyperparameter for Ridge Regression, controlling the amount of regularization applied to the model. You can adjust this value to control the regularization effect.\n",
        "- `ridge_model = Ridge(alpha=alpha)`: Creates a Ridge Regression model with the specified regularization strength (`alpha`).\n",
        "- `ridge_model.fit(X_train, y_train)`: Trains the Ridge Regression model on the training data (`X_train` and `y_train`).\n",
        "\n",
        "**Step 5: Make predictions on the test set**\n",
        "\n",
        "- `y_pred = ridge_model.predict(X_test)`: Uses the trained Ridge Regression model to make predictions on the test set (`X_test`).\n",
        "\n",
        "**Step 6: Calculate the mean squared error on the test set**\n",
        "\n",
        "- `mse = mean_squared_error(y_test, y_pred)`: Calculates the mean squared error (MSE) between the true labels (`y_test`) and the predicted labels (`y_pred`). The MSE is a common metric to evaluate regression models, measuring the average squared difference between the true and predicted values.\n",
        "\n",
        "**Step 7: Plot the results**\n",
        "\n",
        "- `plt.scatter(X, y, color='b', label='Original data')`: Plots the original data points as blue dots.\n",
        "- `plt.plot(X_test, y_pred, color='r', label='Ridge Regression')`: Plots the predicted values (`y_pred`) as a red line using the input data `X_test`.\n",
        "- `plt.xlabel('X')`: Sets the x-axis label to 'X'.\n",
        "- `plt.ylabel('y')`: Sets the y-axis label to 'y'.\n",
        "- `plt.legend()`: Shows the legend indicating the original data and the Ridge Regression line.\n",
        "- `plt.title('Ridge Regression (L2 Regularization)')`: Sets the title of the plot to 'Ridge Regression (L2 Regularization)'.\n",
        "- `plt.show()`: Displays the plot with the original data points and the Ridge Regression line.\n",
        "\n",
        "The output will show the mean squared error (MSE) and the plot will visualize how the Ridge Regression model fits the data. The red line represents the predictions made by the Ridge Regression model, approximating the underlying linear relationship of the data."
      ],
      "metadata": {
        "id": "HjiBd_8T1wCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "MOHNJnhf2Fys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression (L2 Regularization) is a popular linear regression technique used in various fields, including healthcare, to handle multicollinearity and prevent overfitting. One real-world example of its application in the healthcare setting is predicting patient medical costs based on various factors.\n",
        "\n",
        "Let's consider a scenario where a healthcare provider wants to predict the medical expenses (hospitalization costs) of patients based on their age, BMI (Body Mass Index), and the number of chronic conditions they have. The data collected from previous patients includes age, BMI, number of chronic conditions, and their corresponding medical expenses.\n",
        "\n",
        "Here's how Ridge Regression could be applied in this context:\n",
        "\n",
        "Step 1: Data Collection\n",
        "Collect a dataset containing patient information and their medical expenses.\n",
        "\n",
        "| Patient ID | Age | BMI | Chronic Conditions | Medical Expenses |\n",
        "|------------|-----|-----|--------------------|------------------|\n",
        "| 1          | 45  | 26  | 2                  | $5,000           |\n",
        "| 2          | 62  | 29  | 3                  | $8,000           |\n",
        "| 3          | 38  | 21  | 1                  | $3,500           |\n",
        "| ...        | ... | ... | ...                | ...              |\n",
        "\n",
        "Step 2: Data Preprocessing\n",
        "Normalize or standardize the data to ensure all features are on the same scale.\n",
        "\n",
        "Step 3: Ridge Regression Model Building\n",
        "Build a Ridge Regression model with age, BMI, and the number of chronic conditions as the independent variables and medical expenses as the dependent variable.\n",
        "\n",
        "Step 4: L2 Regularization\n",
        "In Ridge Regression, L2 regularization is applied to the model. It adds a penalty term to the linear regression cost function, which is proportional to the square of the magnitude of the coefficients. The regularization parameter, lambda (λ), controls the amount of regularization applied. Higher values of λ lead to stronger regularization and help prevent overfitting.\n",
        "\n",
        "Step 5: Model Evaluation\n",
        "Evaluate the model's performance using appropriate metrics, such as Mean Squared Error (MSE) or R-squared (R2).\n",
        "\n",
        "Step 6: Predicting Medical Expenses\n",
        "Once the model is trained and evaluated, it can be used to predict the medical expenses of new patients based on their age, BMI, and number of chronic conditions.\n",
        "\n",
        "The Ridge Regression model with L2 regularization helps to avoid overfitting, especially when there is multicollinearity among the predictor variables (e.g., age and BMI might be correlated). The regularization term limits the magnitude of the coefficients, making the model more robust and stable when dealing with new patient data.\n",
        "\n",
        "By predicting medical expenses accurately, healthcare providers can better manage their resources, plan for patient treatments, and estimate potential healthcare costs for patients with different characteristics."
      ],
      "metadata": {
        "id": "VyPHYCIwIAox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "QSkDppOS3hhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is Ridge Regression, and how does it differ from ordinary linear regression?**\n",
        "   Ridge Regression is a linear regression model with L2 regularization. It adds a penalty term based on the square of the coefficients' magnitudes to the ordinary linear regression objective function. This penalty helps to prevent overfitting and reduces the impact of multicollinearity in the data.\n",
        "\n",
        "2. **Why is Ridge Regression also called L2 Regularization?**\n",
        "   Ridge Regression is called L2 Regularization because the regularization term added to the objective function is the L2 norm (Euclidean norm) of the coefficients vector. It is the sum of the squares of the coefficients multiplied by a regularization parameter (lambda).\n",
        "\n",
        "3. **How does Ridge Regression handle multicollinearity?**\n",
        "   Multicollinearity refers to the situation when independent variables in a regression model are highly correlated with each other. Ridge Regression addresses multicollinearity by shrinking the coefficients towards zero, which reduces their magnitudes and prevents the model from relying too heavily on any one variable.\n",
        "\n",
        "4. **When should I use Ridge Regression over ordinary linear regression?**\n",
        "   Ridge Regression should be used when you suspect multicollinearity among the predictor variables or when the model tends to overfit the training data. It is especially useful when the number of predictors is larger than the number of data points.\n",
        "\n",
        "5. **What is the significance of the regularization parameter (lambda) in Ridge Regression?**\n",
        "   The regularization parameter, often denoted as lambda (λ), controls the amount of regularization applied to the model. A higher value of lambda will result in stronger regularization, leading to more shrinkage of coefficients. Conversely, a lower value of lambda reduces the regularization effect, making the model similar to ordinary linear regression.\n",
        "\n",
        "6. **Does Ridge Regression eliminate any variables from the model?**\n",
        "   Ridge Regression does not eliminate variables entirely from the model; instead, it shrinks their coefficients. In practice, it keeps all the predictors in the model, but some may have very small coefficients close to zero.\n",
        "\n",
        "7. **What are the advantages of using Ridge Regression?**\n",
        "   Ridge Regression helps in stabilizing the model by reducing the impact of multicollinearity and handling situations with high-dimensional data effectively. It can improve the model's generalization and prevent overfitting.\n",
        "\n",
        "8. **Are there any limitations to using Ridge Regression?**\n",
        "   Ridge Regression may not perform well when the relationship between the predictors and the target variable is truly sparse (i.e., when only a few predictors are significant). In such cases, other regularization techniques like LASSO (L1 Regularization) might be more appropriate.\n",
        "\n",
        "9. **How can I choose the optimal value of the regularization parameter (lambda)?**\n",
        "   The optimal value of lambda in Ridge Regression can be determined through techniques like cross-validation, grid search, or using regularization paths. These methods help in finding the lambda that provides the best trade-off between model simplicity and predictive performance.\n",
        "\n",
        "10. **Can Ridge Regression handle categorical variables?**\n",
        "    Yes, Ridge Regression can handle categorical variables, but they need to be encoded properly (e.g., using one-hot encoding) to be used in the model. Proper encoding ensures that the regularization is applied appropriately to the categorical variables as well.\n",
        "\n",
        "Remember that Ridge Regression is just one of many regularization techniques, and the choice of the appropriate technique depends on the characteristics of your data and the goals of your analysis."
      ],
      "metadata": {
        "id": "SqPgAjm18J4x"
      }
    }
  ]
}