{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwiQXgA3cGvMWlUApuAMM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/k_Means_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Means Clustering Model Background"
      ],
      "metadata": {
        "id": "71fTtWxl-kTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-Means Clustering is a popular unsupervised machine learning algorithm used for clustering data into distinct groups based on similarities in the data points. It aims to partition the data into 'k' clusters, where each cluster is represented by its centroid (the mean of the data points within that cluster).\n",
        "\n",
        "Here's how the k-Means Clustering algorithm works:\n",
        "\n",
        "1. Initialization: Select 'k' initial centroids randomly from the data points or use a specific initialization method.\n",
        "2. Assignment: Assign each data point to the nearest centroid, forming 'k' clusters.\n",
        "3. Update: Recalculate the centroids for each cluster by taking the mean of the data points within that cluster.\n",
        "4. Repeat: Repeat steps 2 and 3 until the centroids stabilize or the maximum number of iterations is reached.\n",
        "\n",
        "**Pros of k-Means Clustering**:\n",
        "\n",
        "1. Simplicity: k-Means is relatively easy to understand and implement, making it a good starting point for clustering tasks.\n",
        "2. Efficiency: It is computationally efficient and can handle large datasets effectively.\n",
        "3. Scalability: Works well with a large number of variables or dimensions, making it suitable for high-dimensional data.\n",
        "4. Interpretability: The clusters formed by k-Means are easy to interpret and visualize.\n",
        "5. Works well with circular clusters: k-Means performs well when the clusters have a roughly circular shape.\n",
        "\n",
        "**Cons of k-Means Clustering**:\n",
        "\n",
        "1. Sensitive to initialization: The quality of the clusters depends on the initial choice of centroids, and different initializations can lead to different results.\n",
        "2. Fixed number of clusters: You need to specify the number of clusters 'k' beforehand, which may not always be known in advance or may be subjective.\n",
        "3. Sensitive to outliers: Outliers can significantly impact the centroid calculation and, therefore, the final clustering.\n",
        "4. Assumes spherical clusters: k-Means assumes that the clusters are spherical and of similar size, which might not hold for complex data distributions.\n",
        "\n",
        "**When to use k-Means Clustering**:\n",
        "\n",
        "k-Means Clustering is suitable for scenarios where:\n",
        "\n",
        "1. You have a large dataset and want a computationally efficient clustering algorithm.\n",
        "2. The number of clusters 'k' is known or can be reasonably estimated.\n",
        "3. The clusters are well-separated or roughly circular in shape.\n",
        "4. You need an interpretable and easy-to-understand clustering solution.\n",
        "5. You want to use clustering as a preprocessing step for other algorithms or tasks.\n",
        "\n",
        "Keep in mind that k-Means is just one of many clustering algorithms available, and its performance depends on the nature of your data and the specific clustering task at hand. Always consider exploring other clustering algorithms like hierarchical clustering, DBSCAN, or Gaussian Mixture Models, especially if your data has complex structures or varying cluster densities."
      ],
      "metadata": {
        "id": "gcoR-EF9BD-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "iq0ZVDnqBuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate sample data\n",
        "data, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
        "\n",
        "# Perform k-Means clustering\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(data)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Plot the data points and centroids\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', edgecolors='k')\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
        "plt.title('k-Means Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6OYlnVl6H93b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "4n2plxTUG_Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Import necessary libraries:**\n",
        "   - `numpy`: A library for numerical operations in Python.\n",
        "   - `matplotlib.pyplot`: A library for data visualization using plots in Python.\n",
        "   - `make_blobs`: A function from scikit-learn that generates synthetic data points with clusters.\n",
        "   - `KMeans`: A class from scikit-learn for performing k-Means clustering.\n",
        "\n",
        "2. **Generate sample data:**\n",
        "   - `make_blobs`: Creates a dataset with synthetic data points that are clustered around specified centers.\n",
        "   - `n_samples=300`: Generates 300 data points.\n",
        "   - `centers=4`: Specifies the number of clusters (centers) to create.\n",
        "   - `random_state=42`: Sets the random seed for reproducibility.\n",
        "   - `cluster_std=1.0`: Specifies the standard deviation of the clusters around their centers.\n",
        "\n",
        "3. **Perform k-Means clustering:**\n",
        "   - `num_clusters = 4`: Defines the number of clusters (same as the number of centers).\n",
        "   - `KMeans(n_clusters=num_clusters, random_state=42)`: Initializes the k-Means clustering algorithm with the specified number of clusters and random state for reproducibility.\n",
        "   - `kmeans.fit_predict(data)`: Fits the k-Means model to the data and predicts the cluster labels for each data point. The result is stored in the `clusters` variable.\n",
        "   - `kmeans.cluster_centers_`: Accesses the coordinates of the cluster centroids and stores them in the `centroids` variable.\n",
        "\n",
        "4. **Plot the data points and centroids:**\n",
        "   - `plt.figure(figsize=(8, 6))`: Creates a new plot with the specified figure size (8 inches wide and 6 inches tall).\n",
        "   - `plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', edgecolors='k')`: Plots the data points with `data[:, 0]` representing the x-coordinate and `data[:, 1]` representing the y-coordinate. The `c=clusters` parameter assigns a unique color to each cluster based on the cluster labels. The `cmap='viridis'` parameter sets the color map for the clusters, and `edgecolors='k'` adds black edges to the data points.\n",
        "   - `plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)`: Plots the cluster centroids with `centroids[:, 0]` representing the x-coordinate and `centroids[:, 1]` representing the y-coordinate. The `c='red'` parameter sets the color of the centroids to red, `marker='X'` selects the marker style as 'X', and `s=200` sets the marker size to 200.\n",
        "   - `plt.title('k-Means Clustering')`: Sets the title of the plot to 'k-Means Clustering'.\n",
        "   - `plt.xlabel('Feature 1')`: Sets the label of the x-axis to 'Feature 1'.\n",
        "   - `plt.ylabel('Feature 2')`: Sets the label of the y-axis to 'Feature 2'.\n",
        "   - `plt.show()`: Displays the plot.\n",
        "\n",
        "In summary, this code generates sample data with four clusters, performs k-Means clustering on the data to identify the clusters, and then plots the data points and the cluster centroids using matplotlib. The plot helps visualize the effectiveness of k-Means clustering in grouping the data points into distinct clusters based on their features."
      ],
      "metadata": {
        "id": "ZxPLMaRUuUzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "nhx9PkyPSO_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of applying k-Means clustering in a healthcare setting is patient segmentation based on health-related data. In this scenario, k-Means clustering can be used to group patients with similar characteristics and health conditions, allowing healthcare providers to tailor treatments and interventions more effectively.\n",
        "\n",
        "Let's consider a specific example to illustrate how k-Means clustering can be applied in healthcare:\n",
        "\n",
        "**Example: Patient Segmentation for Diabetes Management**\n",
        "\n",
        "Objective: To segment diabetic patients based on their health parameters and identify distinct subgroups with similar health profiles.\n",
        "\n",
        "Data Collection:\n",
        "- A dataset is collected from diabetic patients, including features such as age, gender, BMI (Body Mass Index), HbA1c levels (glycated hemoglobin), fasting blood glucose levels, postprandial blood glucose levels, and average number of daily insulin units.\n",
        "\n",
        "Data Preprocessing:\n",
        "- The data is preprocessed to handle missing values, normalize numerical features, and encode categorical features if present.\n",
        "\n",
        "Applying k-Means Clustering:\n",
        "- The k-Means clustering algorithm is applied to the preprocessed data.\n",
        "- The number of clusters, 'k,' is determined based on domain knowledge or using techniques like the elbow method or silhouette score to find the optimal number of clusters.\n",
        "\n",
        "Interpreting the Clusters:\n",
        "- Once the clustering is complete, each patient is assigned to one of the k clusters.\n",
        "- The clusters are analyzed to understand the characteristics of each group of patients.\n",
        "- Healthcare providers can interpret the clusters to identify meaningful patterns and correlations between health parameters and patient outcomes.\n",
        "\n",
        "Benefits and Applications:\n",
        "- Personalized Treatment Plans: Clustering helps in tailoring treatment plans based on the specific needs of each patient group. For example, patients in one cluster might respond better to certain medications, while patients in another cluster might benefit from lifestyle interventions like diet and exercise.\n",
        "\n",
        "- Early Detection of Risks: Identifying high-risk clusters can enable early detection of potential complications, allowing healthcare providers to intervene proactively and prevent adverse events.\n",
        "\n",
        "- Resource Allocation: Understanding patient subgroups can help allocate healthcare resources more efficiently. For instance, a cluster with higher insulin requirements may need specialized diabetes care and support.\n",
        "\n",
        "- Clinical Research: Researchers can use the insights gained from patient segmentation to conduct targeted studies and identify factors influencing disease progression and treatment responses.\n",
        "\n",
        "Overall, k-Means clustering in the healthcare setting allows for more personalized and effective patient care by identifying distinct patient subgroups and understanding the relationships between health parameters and outcomes. It enables healthcare providers to make informed decisions and optimize healthcare interventions for different patient populations."
      ],
      "metadata": {
        "id": "BMN3w6tm_jzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "buWM1VrSZAZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is k-Means Clustering?\n",
        "   - k-Means Clustering is an unsupervised machine learning algorithm used to partition data into k clusters based on their similarity.\n",
        "\n",
        "2. How does k-Means Clustering work?\n",
        "   - The k-Means algorithm iteratively assigns data points to the nearest cluster centroid and then updates the centroids based on the mean of the points in each cluster.\n",
        "\n",
        "3. What is the objective function in k-Means Clustering?\n",
        "   - The objective of k-Means is to minimize the sum of squared distances between data points and their assigned cluster centroids.\n",
        "\n",
        "4. How is the value of 'k' determined in k-Means Clustering?\n",
        "   - Selecting the right value of 'k' is essential. Common methods for determining 'k' include the elbow method, silhouette score, and gap statistics.\n",
        "\n",
        "5. What are the limitations of k-Means Clustering?\n",
        "   - k-Means is sensitive to the initial placement of centroids and can converge to local optima. It also assumes equal-sized and spherical clusters.\n",
        "\n",
        "6. Can k-Means handle non-spherical or overlapping clusters?\n",
        "   - No, k-Means assumes spherical clusters and struggles with non-linear and overlapping data distributions.\n",
        "\n",
        "7. How can you handle the sensitivity to initialization in k-Means?\n",
        "   - Multiple runs with different initializations and choosing the best result based on the lowest objective function value can help mitigate sensitivity to initialization.\n",
        "\n",
        "8. What are some popular distance metrics used in k-Means Clustering?\n",
        "   - Euclidean distance is the most commonly used metric, but other distance metrics like Manhattan distance and cosine similarity can also be employed.\n",
        "\n",
        "9. Is it necessary for k-Means to converge in every run?\n",
        "   - No, k-Means may not converge to the global optimum in every run, but it generally converges to a local optimum that minimizes the objective function.\n",
        "\n",
        "10. Can k-Means handle high-dimensional data effectively?\n",
        "    - k-Means can struggle with high-dimensional data due to the \"curse of dimensionality,\" where distances between points lose meaning in high-dimensional spaces.\n",
        "\n",
        "11. How can you evaluate the performance of k-Means Clustering?\n",
        "    - Internal evaluation metrics like silhouette score or external validation metrics like adjusted Rand index can be used to assess the quality of the clustering.\n",
        "\n",
        "12. Can k-Means be used for data with categorical features?\n",
        "    - No, k-Means is designed for numerical data and may not work well with categorical features. For such cases, other clustering algorithms like k-Modes or k-Prototypes are more appropriate.\n",
        "\n",
        "13. How is k-Means used in image compression?\n",
        "    - In image compression, k-Means can be used to cluster similar colors together and represent them with fewer colors, reducing the image size while preserving visual quality.\n",
        "\n",
        "14. Can k-Means be sensitive to outliers in the data?\n",
        "    - Yes, k-Means is sensitive to outliers as they can significantly impact the cluster centroids, leading to suboptimal clustering results.\n",
        "\n",
        "15. Is it possible for k-Means to converge to different solutions for different runs on the same data?\n",
        "    - Yes, k-Means may converge to different solutions for different runs due to the sensitivity to initialization, especially when the data contains overlapping clusters or when 'k' is not well-defined."
      ],
      "metadata": {
        "id": "nHt0uW653zVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "CPNsNYgzeLj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is the main objective of k-Means Clustering?\n",
        "\n",
        "a) Classification of data into predefined categories.\n",
        "b) Regression analysis for predicting continuous values.\n",
        "c) Dimensionality reduction of feature space.\n",
        "d) Partitioning data into distinct groups based on similarity.\n",
        "\n",
        "**Question 2:** How does the k-Means algorithm work?\n",
        "\n",
        "a) It uses supervised learning to classify data points.\n",
        "b) It fits a line to the data to minimize the error.\n",
        "c) It calculates distances between data points and cluster centroids to assign points to clusters.\n",
        "d) It uses gradient descent to optimize the cluster boundaries.\n",
        "\n",
        "**Question 3:** In the context of k-Means, what does \"k\" represent?\n",
        "\n",
        "a) The number of features in the dataset.\n",
        "b) The number of clusters to be formed.\n",
        "c) The dimensionality of the data.\n",
        "d) The distance metric used for clustering.\n",
        "\n",
        "**Question 4:** Which of the following distance metrics is commonly used in k-Means Clustering?\n",
        "\n",
        "a) Pearson correlation coefficient.\n",
        "b) Mahalanobis distance.\n",
        "c) Jaccard similarity.\n",
        "d) Manhattan distance.\n",
        "\n",
        "**Question 5:** What is the goal of choosing the optimal number of clusters in k-Means?\n",
        "\n",
        "a) To make the algorithm converge faster.\n",
        "b) To achieve 100% accuracy in clustering.\n",
        "c) To prevent overfitting and underfitting.\n",
        "d) To eliminate outliers from the dataset.\n",
        "\n",
        "**Question 6:** What is the elbow method used for in k-Means Clustering?\n",
        "\n",
        "a) Estimating the number of clusters by observing the \"elbow point\" in the cost (inertia) plot.\n",
        "b) Fitting a linear regression to the clustered data.\n",
        "c) Calculating the variance of the features.\n",
        "d) Determining the optimal number of features for clustering.\n",
        "\n",
        "**Question 7:** Which statement about k-Means initialization is true?\n",
        "\n",
        "a) Initialization has no impact on the convergence of the algorithm.\n",
        "b) Random initialization of cluster centroids can lead to different outcomes.\n",
        "c) The algorithm always converges to a global minimum regardless of initialization.\n",
        "d) Initialization is only performed once after the clustering process.\n",
        "\n",
        "**Question 8:** What is a limitation of k-Means Clustering?\n",
        "\n",
        "a) It cannot handle high-dimensional data.\n",
        "b) It requires labeled training data.\n",
        "c) It is not sensitive to the initial choice of centroids.\n",
        "d) It is not affected by outliers in the dataset.\n",
        "\n",
        "**Question 9:** How does k-Means deal with outliers?\n",
        "\n",
        "a) It assigns outliers to a separate cluster automatically.\n",
        "b) It eliminates outliers from the dataset.\n",
        "c) It tends to incorporate outliers into the nearest cluster.\n",
        "d) It adjusts the distance metric to ignore outliers.\n",
        "\n",
        "**Question 10:** After convergence in k-Means, what defines each cluster?\n",
        "\n",
        "a) The cluster's centroid and the sum of squared distances between data points and the centroid.\n",
        "b) The cluster's average feature values and the total number of data points.\n",
        "c) The cluster's median values and the standard deviation of feature values.\n",
        "d) The cluster's maximum and minimum feature values.\n",
        "\n",
        "**Answers:**\n",
        "1. d) Partitioning data into distinct groups based on similarity.\n",
        "2. c) It calculates distances between data points and cluster centroids to assign points to clusters.\n",
        "3. b) The number of clusters to be formed.\n",
        "4. d) Manhattan distance.\n",
        "5. c) To prevent overfitting and underfitting.\n",
        "6. a) Estimating the number of clusters by observing the \"elbow point\" in the cost (inertia) plot.\n",
        "7. b) Random initialization of cluster centroids can lead to different outcomes.\n",
        "8. a) It cannot handle high-dimensional data.\n",
        "9. c) It tends to incorporate outliers into the nearest cluster.\n",
        "10. a) The cluster's centroid and the sum of squared distances between data points and the centroid."
      ],
      "metadata": {
        "id": "vBPT4tAFeMyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "vop9LvFcvX7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Patient Segmentation for Hospital Resource Allocation**:\n",
        "   - **Objective**: To group patients based on their medical histories and needs.\n",
        "   - **Dataset**: Patient medical records (anonymized), treatment histories, and medication data.\n",
        "   - **Outcome**: Efficient allocation of hospital resources like beds, specialized doctors, and equipment.\n",
        "\n",
        "2. **Predicting Hospital Readmission Rates**:\n",
        "   - **Objective**: Cluster patients based on likelihood of readmission.\n",
        "   - **Dataset**: Past admission records, duration between admissions, and reasons for readmission.\n",
        "   - **Outcome**: Implement preventive measures or follow-up consultations for high-risk clusters.\n",
        "\n",
        "3. **Analyzing Medical Imagery**:\n",
        "   - **Objective**: Classify and segment medical images, like MRI or X-rays, based on patterns.\n",
        "   - **Dataset**: Anonymized medical imagery dataset.\n",
        "   - **Outcome**: Identification of regions of interest in medical images.\n",
        "\n",
        "4. **Drug Response Clusters**:\n",
        "   - **Objective**: Determine clusters of patients based on their responses to specific drugs or treatments.\n",
        "   - **Dataset**: Patient drug administration records and subsequent medical results.\n",
        "   - **Outcome**: Personalized drug recommendations or dosages.\n",
        "\n",
        "5. **Mental Health Patient Grouping**:\n",
        "   - **Objective**: Cluster patients based on their mental health symptoms and treatment responses.\n",
        "   - **Dataset**: Patient mental health records, therapy notes, and medication data.\n",
        "   - **Outcome**: Better understanding of mental health conditions and effective treatments.\n",
        "\n",
        "6. **Optimizing Hospital Department Workflow**:\n",
        "   - **Objective**: Group hospital departments or units based on patient flow, resource usage, and treatment types.\n",
        "   - **Dataset**: Hospital operational data, patient count, and resource allocation records.\n",
        "   - **Outcome**: Efficient patient movement and resource allocation between departments.\n",
        "\n",
        "7. **Genomic Data Clustering**:\n",
        "   - **Objective**: Classify patients based on genomic data to understand genetic predispositions.\n",
        "   - **Dataset**: Anonymized genomic sequencing results.\n",
        "   - **Outcome**: Potential genetic insights into disease vulnerability or drug responses.\n",
        "\n",
        "8. **Clustering Medical Research Data**:\n",
        "   - **Objective**: Organize medical research papers or findings based on topic similarities.\n",
        "   - **Dataset**: Abstracts or keywords from a collection of medical research papers.\n",
        "   - **Outcome**: Enhanced literature reviews, research meta-analyses, or identification of research gaps.\n",
        "\n",
        "9. **Healthcare Expense Analysis**:\n",
        "   - **Objective**: Group patients or treatments based on the costs involved.\n",
        "   - **Dataset**: Billing data, medical procedure codes, and patient demographic data.\n",
        "   - **Outcome**: Insight into high-cost areas, potential cost-saving areas, or insurance pricing.\n",
        "\n",
        "10. **Health Monitoring Device Data Clustering**:\n",
        "   - **Objective**: Classify data from health monitoring devices like heart rate monitors or sleep trackers.\n",
        "   - **Dataset**: Anonymized device data logs.\n",
        "   - **Outcome**: Insights into healthy patterns, anomaly detection, or prediction of potential health risks.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVU2XLDdvas_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "WRrx8IlwC8ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of implementing the k-Means clustering model using a real-world health dataset. In this example, we'll use the \"Heart Disease UCI\" dataset, which contains various attributes related to heart health and whether or not a patient has heart disease."
      ],
      "metadata": {
        "id": "-zJwqy5WhmLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Drop missing values\n",
        "data = data.replace(\"?\", np.nan)\n",
        "data = data.dropna()\n",
        "\n",
        "# Select relevant features\n",
        "X = data.drop(\"target\", axis=1)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Find optimal number of clusters using Elbow Method\n",
        "inertia = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "plt.show()\n",
        "\n",
        "# Based on the elbow method, let's choose k=3\n",
        "k = 3\n",
        "\n",
        "# Fit k-Means clustering model\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Add cluster labels to the original data\n",
        "data['cluster'] = kmeans.labels_\n",
        "\n",
        "# Analyze the clusters\n",
        "cluster_summary = data.groupby('cluster').mean()\n",
        "print(cluster_summary)\n",
        "\n",
        "# Visualize the clusters (for 2D visualization, you can choose two features)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_, cmap='viridis', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('k-Means Clustering')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q2J2x_G3hjK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates the process of loading the dataset, preprocessing it, finding the optimal number of clusters using the elbow method, fitting the k-Means model, and visualizing the results. You can modify this code to work with other health datasets or explore different features for clustering."
      ],
      "metadata": {
        "id": "nIqesMUmiHa-"
      }
    }
  ]
}