{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL9wg8D7YWOlUkWJjQ370g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/machine_learning_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Machine Learning Model Example"
      ],
      "metadata": {
        "id": "l9KwDyUdFqR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Split the dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features to have mean=0 and variance=1\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Model Training\n",
        "# Train the KNN classifier on the training data\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Make Predictions\n",
        "# Predict the test set results\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 4: Model Evaluation\n",
        "# Print the classification report and confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wlANyJqrCMEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown\n"
      ],
      "metadata": {
        "id": "Lbh5sY1hFjS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Import necessary libraries:**\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import datasets\n",
        "```\n",
        "Here, we are importing all the necessary libraries and functions for our machine learning pipeline. The `sklearn` library provides us with most of the tools we need to create machine learning models.\n",
        "\n",
        "**2. Load the Iris dataset:**\n",
        "```python\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "```\n",
        "The Iris dataset is a classic dataset in machine learning and statistics. It is included in sklearn's datasets module. It includes 150 samples of iris flowers with four features (sepal length, sepal width, petal length, petal width) and a target variable which is the species of the flower. `X` is the matrix of feature variables and `y` is the vector of target variables.\n",
        "\n",
        "**3. Data Preprocessing:**\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "Here we're using the `train_test_split` function to split our data into a training set and a test set. 80% of the data will be used for training, and 20% for testing.\n",
        "\n",
        "```python\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "The `StandardScaler` standardizes features by removing the mean and scaling to unit variance. The standard score of a sample `x` is `(x - mean) / std_dev`. We first fit the scaler using the training data `X_train` and then transform `X_train` and `X_test`.\n",
        "\n",
        "**4. Model Training:**\n",
        "```python\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "We are training a K-Nearest Neighbors (KNN) Classifier on our training data. The `n_neighbors` parameter is the number of neighbors to consider, which we've set to 3. The `fit` method trains the model on the training data.\n",
        "\n",
        "**5. Make Predictions:**\n",
        "```python\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "Now we use our trained model to make predictions on the test data. The `predict` method makes predictions for each instance in `X_test`.\n",
        "\n",
        "**6. Model Evaluation:**\n",
        "```python\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "Finally, we evaluate how well our model did. The `confusion_matrix` function gives us a matrix where the entry C[i, j] is the number of observations known to be in group `i` but predicted to be in group `j`. The `classification_report` function gives us a text report showing the main classification metrics, including precision, recall, f1-score, and support (the number of instances for each label). These are important metrics for understanding the performance of our classification model."
      ],
      "metadata": {
        "id": "WmXdVBbzFWfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "OBt2eNeDGZZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the most commonly used machine learning terms:\n",
        "\n",
        "1. **Machine Learning (ML):** An application of artificial intelligence that provides systems the ability to learn and improve from experience without being explicitly programmed.\n",
        "\n",
        "2. **Artificial Intelligence (AI):** The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, etc.\n",
        "\n",
        "3. **Deep Learning:** A subset of machine learning that's based on artificial neural networks with representation learning. It can process a wide range of data resources, requires less data preprocessing by humans, and can often produce more accurate results than traditional ML approaches.\n",
        "\n",
        "4. **Neural Network:** A series of algorithms that attempt to identify underlying relationships in a set of data through a process that mimics how the human brain works.\n",
        "\n",
        "5. **Training Data:** The data on which the machine learning model is trained, and learns from.\n",
        "\n",
        "6. **Test Data:** Independent data used to evaluate the performance of the model. It's important that the model has never seen this data during training.\n",
        "\n",
        "7. **Supervised Learning:** A type of machine learning where the model is trained on a labeled dataset, i.e., each instance in the training dataset includes the expected output.\n",
        "\n",
        "8. **Unsupervised Learning:** A type of machine learning where the model is trained on an unlabeled dataset and must find patterns in the data on its own.\n",
        "\n",
        "9. **Reinforcement Learning:** A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward.\n",
        "\n",
        "10. **Classification:** A type of machine learning task where the model is trained to assign labels to instances based on their features.\n",
        "\n",
        "11. **Regression:** A type of machine learning task where the model is trained to predict a continuous numerical value based on the instance's features.\n",
        "\n",
        "12. **Overfitting:** A concept in machine learning where a model is too closely fit to the training data, causing it to perform poorly on unseen data.\n",
        "\n",
        "13. **Underfitting:** This occurs when a model is too simple and does not fit the training data well enough, causing poor performance on both the training data and unseen data.\n",
        "\n",
        "14. **Regularization:** A technique used to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "15. **Hyperparameter Tuning:** The process of choosing a set of optimal hyperparameters for a machine learning model.\n",
        "\n",
        "16. **Cross-Validation:** A technique for assessing how the statistical analysis will generalize to an independent data set. It's mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
        "\n",
        "17. **Bias-Variance Tradeoff:** A property of learning algorithms that the error obtained on new unseen data (generalization error) can be decomposed into bias, variance, and noise. The bias error is an error from erroneous assumptions in the learning algorithm. Variance is an error from sensitivity to small fluctuations in the training set.\n",
        "\n",
        "18. **Feature Selection:** The process of selecting a subset of relevant features (variables, predictors) for use in model construction.\n",
        "\n",
        "19. **Feature Engineering:** The process of creating new features from existing ones, often to improve machine learning model performance.\n",
        "\n",
        "20. **Gradient Descent:** An optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
        "\n",
        "21. **Stochastic Gradient Descent (SGD):** A variation of the gradient descent algorithm that calculates the gradient and updates the model parameters using a single instance, instead of the entire training dataset.\n",
        "\n",
        "22. **Confusion Matrix:** A table used to describe the performance of a classification model (a classifier) on a set of test data for which the true values are known.\n",
        "\n",
        "23. **Precision:** The number of True Positives divided by the number of True Positives and False Positives. It's intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "24. **Recall (Sensitivity):** The number of True Positives divided by the number of True Positives and the number of False Negatives. It's intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "25. **F1-Score:** The harmonic mean of Precision and Recall. It tries to find the balance between precision and recall.\n",
        "\n",
        "26. **ROC Curve:** A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It's created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
        "\n",
        "27. **Area Under the Curve (AUC):** The area underneath the ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds.\n",
        "\n",
        "There are many more terms and concepts, as machine learning is a vast field with a variety of techniques and theories. This list should provide a good start and covers many of the basic and commonly used terms."
      ],
      "metadata": {
        "id": "zv-u8a0-Gb26"
      }
    }
  ]
}