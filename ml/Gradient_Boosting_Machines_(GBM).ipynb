{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNReLypjW6Dc+cvWWG4fhIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Gradient_Boosting_Machines_(GBM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Machines (GBM) Model Background"
      ],
      "metadata": {
        "id": "RE8wMP7b-V4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Machines (GBM) is a popular machine learning technique used for both regression and classification tasks. It is an ensemble learning method that builds multiple weak learners (usually decision trees) sequentially, each correcting the errors of its predecessor. The primary idea behind GBM is to combine the predictions of these weak learners to create a more accurate and robust final model.\n",
        "\n",
        "Here are the pros and cons of Gradient Boosting Machines:\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "1. High accuracy: GBM often provides superior predictive performance compared to many other algorithms, especially when dealing with complex and non-linear relationships in the data.\n",
        "\n",
        "2. Robustness to outliers: GBM can handle outliers in the data effectively, thanks to its use of decision trees as base learners, which are not strongly influenced by extreme values.\n",
        "\n",
        "3. Feature importance: GBM provides a measure of feature importance, helping you identify the most relevant features for making predictions and gaining insights into the data.\n",
        "\n",
        "4. Handles different data types: GBM can handle a mix of data types (e.g., numerical and categorical), making it suitable for a wide range of datasets.\n",
        "\n",
        "5. Low risk of overfitting: GBM has mechanisms like early stopping and shrinkage (regularization) that reduce the risk of overfitting, especially when compared to models like decision trees.\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "1. Computationally expensive: Training GBM can be computationally intensive and time-consuming, especially if the dataset is large or the number of iterations is high.\n",
        "\n",
        "2. Hyperparameter tuning: GBM has several hyperparameters that need to be carefully tuned, and finding the optimal combination can require significant effort and computational resources.\n",
        "\n",
        "3. Prone to overfitting with large depth: While GBM can prevent overfitting to some extent, using very deep trees as base learners can still lead to overfitting.\n",
        "\n",
        "4. Potential for memory issues: Storing multiple decision trees in memory can be memory-intensive, especially with large datasets.\n",
        "\n",
        "**When to use Gradient Boosting Machines**:\n",
        "\n",
        "You should consider using GBM in the following situations:\n",
        "\n",
        "1. **High-performance requirements:** When you need high accuracy and predictive power, especially in situations where other algorithms may not be performing well.\n",
        "\n",
        "2. **Structured data:** GBM works well with structured data (tabular data with features and labels), making it suitable for many real-world problems.\n",
        "\n",
        "3. **Feature importance analysis:** When you need to understand the importance of different features in your data and their impact on predictions.\n",
        "\n",
        "4. **Handling missing data:** GBM can naturally handle missing data, reducing the need for extensive data imputation.\n",
        "\n",
        "5. **Medium to large-sized datasets:** While GBM can work with small datasets, it is more effective and efficient when dealing with medium to large-sized datasets.\n",
        "\n",
        "6. **Regression and classification tasks:** GBM can be used for both regression and classification problems, making it versatile across different types of tasks.\n",
        "\n",
        "In summary, Gradient Boosting Machines are a powerful and flexible ensemble learning method with excellent predictive performance. They are best suited for scenarios where high accuracy is crucial, and you have structured data with a moderate to large number of samples. However, be prepared to invest time and computational resources in hyperparameter tuning and training."
      ],
      "metadata": {
        "id": "xolYLe6399q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "Z2abBfP1B0CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install scikit-learn in case it's not already installed\n",
        "# !pip install scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data: Replace this with your own dataset or load it from a file\n",
        "# The dataset should have features in the columns and the target variable in a separate column\n",
        "# For this example, we will generate some random data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "n_features = 5\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "y = np.random.randint(2, size=n_samples)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the GBM model\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = gbm_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "SbkT2rZHCbmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "a4r85w_cHXrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Install scikit-learn (Optional):** If the scikit-learn library is not already installed in your Python environment, you can install it using `pip install scikit-learn`. The code includes a commented-out line that shows how to install it using pip.\n",
        "\n",
        "2. **Import necessary libraries:** The code imports required libraries: `numpy`, `pandas`, `train_test_split` function from `sklearn.model_selection`, `GradientBoostingClassifier` from `sklearn.ensemble`, and `accuracy_score` from `sklearn.metrics`.\n",
        "\n",
        "3. **Generate Sample Data:** The code generates sample data for demonstration purposes. It creates a random dataset with 1000 samples and 5 features (`X`) and a binary target variable (`y`) with values 0 or 1. In real-world applications, you would typically load your dataset from a file or a database.\n",
        "\n",
        "4. **Split the Data into Training and Testing Sets:** The code uses the `train_test_split` function from scikit-learn to split the data into training and testing sets. It assigns 80% of the data to the training set (`X_train` and `y_train`) and 20% to the testing set (`X_test` and `y_test`). The `random_state` parameter ensures reproducibility of the split.\n",
        "\n",
        "5. **Initialize the Gradient Boosting Classifier (GBM) Model:** The code creates an instance of the GradientBoostingClassifier class with specified hyperparameters:\n",
        "   - `n_estimators=100`: The number of boosting stages (trees) to be fit. This parameter controls the number of weak learners (decision trees) in the ensemble.\n",
        "   - `learning_rate=0.1`: The shrinkage parameter, which scales the contribution of each tree. A smaller value means slower learning, but often better performance.\n",
        "   - `random_state=42`: The random seed for reproducibility.\n",
        "\n",
        "6. **Train the Model:** The model is trained on the training data (`X_train` and `y_train`) using the `fit` method of the GradientBoostingClassifier. The model iteratively fits weak learners (decision trees) to correct the errors made by the previous models.\n",
        "\n",
        "7. **Make Predictions on the Test Data:** Once the model is trained, it makes predictions on the test data (`X_test`) using the `predict` method of the GradientBoostingClassifier. The predicted target values are stored in `y_pred`.\n",
        "\n",
        "8. **Calculate the Accuracy of the Model:** The code calculates the accuracy of the model's predictions by comparing the predicted values (`y_pred`) with the actual target values (`y_test`). It uses the `accuracy_score` function from scikit-learn, which computes the fraction of correct predictions.\n",
        "\n",
        "9. **Print the Accuracy:** The code prints the accuracy of the model on the test data, providing an estimate of how well the model performs on unseen data.\n",
        "\n",
        "Please note that the provided code is a simple example to demonstrate how to use the Gradient Boosting Classifier in scikit-learn. In real-world scenarios, you would typically preprocess your data, perform hyperparameter tuning, and evaluate the model's performance more comprehensively using techniques like cross-validation. Additionally, you would load your dataset from external sources like CSV files or databases instead of generating random sample data."
      ],
      "metadata": {
        "id": "Thg0ay7gtdtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "qXFOOmyUSbeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a healthcare setting, Gradient Boosting Machines (GBM) can be used for various tasks, such as disease prediction, patient risk stratification, and medical image analysis. Let's take a real-world example of using GBM for predicting the risk of cardiovascular disease based on patient data.\n",
        "\n",
        "**Example: Cardiovascular Disease Risk Prediction**\n",
        "\n",
        "**Objective:** To predict the risk of cardiovascular disease (CVD) for patients based on their demographic information, medical history, and biomarkers.\n",
        "\n",
        "**Data:** A dataset containing records of patients with features like age, gender, body mass index (BMI), blood pressure, cholesterol levels, smoking status, family history of CVD, etc. The dataset also includes a binary label indicating whether the patient has experienced a cardiovascular event in the past.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Data Preprocessing:** The dataset needs to be preprocessed before using it to train the GBM model. This step involves handling missing values, encoding categorical variables, and splitting the data into training and testing sets.\n",
        "\n",
        "2. **Feature Engineering:** Additional features can be derived from the existing data, such as calculating the patient's BMI from height and weight or computing risk scores based on biomarker values.\n",
        "\n",
        "3. **Model Training:** The preprocessed data is used to train the GBM model. GBM is an ensemble learning method that combines multiple weak learners (decision trees) to create a strong predictive model. The model learns from the data to make accurate predictions about CVD risk.\n",
        "\n",
        "4. **Hyperparameter Tuning:** GBM has several hyperparameters that need to be tuned to optimize model performance. Grid search or random search can be employed to find the best combination of hyperparameters.\n",
        "\n",
        "5. **Model Evaluation:** The trained model is evaluated on a separate test dataset to assess its performance. Common evaluation metrics include accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC).\n",
        "\n",
        "6. **Interpretability and Explainability:** GBM models can be less interpretable compared to some other models. Techniques like feature importance analysis and SHAP (SHapley Additive exPlanations) can be used to gain insights into the factors contributing to CVD risk predictions.\n",
        "\n",
        "7. **Deployment and Integration:** Once the GBM model is deemed satisfactory, it can be deployed in the healthcare system to predict CVD risk for new patients. This might involve integrating the model into electronic health record (EHR) systems or other healthcare applications.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "- GBM models are capable of handling complex and non-linear relationships between features, making them suitable for healthcare data where interactions between various risk factors can be intricate.\n",
        "- The interpretability of GBM can be enhanced using feature importance analysis, enabling healthcare professionals to understand the key factors influencing the risk predictions.\n",
        "- With proper feature engineering and tuning, GBM models can achieve high accuracy in predicting CVD risk, helping clinicians identify patients at higher risk and providing personalized preventive measures.\n",
        "\n",
        "**Note:** In real-world healthcare applications, it is critical to ensure data privacy, obtain necessary ethical approvals, and conduct thorough validation to avoid biases and ensure the model's safety and effectiveness before deploying it in the clinical environment."
      ],
      "metadata": {
        "id": "vhM9LuOu-bS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "delF87HKZNFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Gradient Boosting Machine (GBM)?\n",
        "   - Gradient Boosting Machine is a popular ensemble learning technique used in machine learning. It is based on the concept of combining multiple weak learners (typically decision trees) to create a strong predictive model.\n",
        "\n",
        "2. How does GBM differ from other ensemble methods like Random Forest?\n",
        "   - Unlike Random Forest, which builds multiple decision trees independently and then averages their predictions, GBM builds trees sequentially in a boosting fashion, with each tree trying to correct the errors of the previous ones.\n",
        "\n",
        "3. What are the key components of GBM?\n",
        "   - GBM consists of three main components: a) Weak learners (usually decision trees), b) Loss function (used to measure the difference between predicted and actual values), and c) Gradient descent optimization (used to minimize the loss function).\n",
        "\n",
        "4. What is the role of boosting in GBM?\n",
        "   - Boosting in GBM refers to the process of sequentially adding weak learners to the ensemble, with each tree focused on reducing the errors made by the previous ones. This iterative process results in a strong learner that improves its predictive performance.\n",
        "\n",
        "5. What are the advantages of using GBM?\n",
        "   - GBM is known for its high predictive accuracy and robustness against overfitting. It can handle both numerical and categorical data and is effective for regression and classification tasks. GBM can also handle missing data well.\n",
        "\n",
        "6. What are the potential drawbacks of GBM?\n",
        "   - GBM can be computationally expensive and time-consuming, especially if the dataset is large and the number of boosting iterations (trees) is high. It may also require careful tuning of hyperparameters to prevent overfitting.\n",
        "\n",
        "7. How does GBM handle class imbalances in classification problems?\n",
        "   - GBM can handle class imbalances by assigning higher weights to misclassified samples from the minority class during the training process. This allows the model to focus more on the minority class and improve its predictive performance for imbalanced datasets.\n",
        "\n",
        "8. Can GBM handle missing values in the dataset?\n",
        "   - Yes, GBM can handle missing values. It uses surrogate splits during tree construction to handle missing values effectively, making it a suitable choice for datasets with missing data.\n",
        "\n",
        "9. Are there variations of GBM available?\n",
        "   - Yes, there are several variations of GBM, such as XGBoost, LightGBM, and CatBoost, which have been developed to optimize performance and address specific challenges related to speed, memory usage, and categorical variable handling.\n",
        "\n",
        "10. In which real-world applications is GBM commonly used?\n",
        "    - GBM is widely used in various domains, including finance, healthcare, marketing, and natural language processing. It is applied in tasks such as credit risk assessment, disease prediction, customer churn analysis, and sentiment analysis, among others."
      ],
      "metadata": {
        "id": "pBk7wK-Wk6CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "d1ehhMEXc8GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:** What is Gradient Boosting?\n",
        "\n",
        "a) A machine learning algorithm for clustering data.\n",
        "b) A technique for reducing the dimensionality of data.\n",
        "c) An ensemble learning method that combines multiple weak models to create a strong predictive model.\n",
        "d) A method for normalizing features in a dataset.\n",
        "\n",
        "**Question 2:** In Gradient Boosting, what does the term \"Gradient\" refer to?\n",
        "\n",
        "a) It refers to the steepness of the learning curve.\n",
        "b) It's a mathematical term that represents the direction of steepest ascent of a function.\n",
        "c) It's a measure of the model's accuracy.\n",
        "d) It's a metric for evaluating feature importance.\n",
        "\n",
        "**Question 3:** Which of the following statements is true regarding the training process of a Gradient Boosting Machine?\n",
        "\n",
        "a) All weak learners are trained simultaneously.\n",
        "b) Each weak learner is trained independently of the others.\n",
        "c) Weak learners are added sequentially, where each new learner corrects the errors of the previous ones.\n",
        "d) Weak learners are only trained on the most important features.\n",
        "\n",
        "**Question 4:** What is the purpose of the \"learning rate\" parameter in Gradient Boosting?\n",
        "\n",
        "a) It controls the speed of convergence during training.\n",
        "b) It determines the number of weak learners to use.\n",
        "c) It sets the maximum depth of the decision trees.\n",
        "d) It regulates the contribution of each weak learner to the ensemble.\n",
        "\n",
        "**Question 5:** Which of the following is a potential drawback of Gradient Boosting?\n",
        "\n",
        "a) It is not capable of handling categorical features.\n",
        "b) It is highly resistant to overfitting.\n",
        "c) It can be sensitive to noisy data and outliers.\n",
        "d) It only works well with linearly separable data.\n",
        "\n",
        "**Question 6:** What is the main difference between AdaBoost and Gradient Boosting?\n",
        "\n",
        "a) AdaBoost uses decision trees as weak learners, while Gradient Boosting uses linear models.\n",
        "b) AdaBoost only works for binary classification, while Gradient Boosting works for both classification and regression.\n",
        "c) AdaBoost focuses on reducing bias, while Gradient Boosting focuses on reducing variance.\n",
        "d) AdaBoost combines weak learners in parallel, while Gradient Boosting combines them sequentially.\n",
        "\n",
        "**Question 7:** How does Gradient Boosting prevent overfitting?\n",
        "\n",
        "a) By using a large number of weak learners.\n",
        "b) By using a small learning rate.\n",
        "c) By using decision trees with shallow depth.\n",
        "d) By ignoring the training data and relying on validation data.\n",
        "\n",
        "**Question 8:** Which evaluation metric is commonly used for Gradient Boosting models in classification tasks?\n",
        "\n",
        "a) Mean Squared Error (MSE).\n",
        "b) R-squared (R2).\n",
        "c) Log Loss (Logarithmic Loss).\n",
        "d) Silhouette Score.\n",
        "\n",
        "**Question 9:** In a Gradient Boosting Machine, which component provides the initial predictions?\n",
        "\n",
        "a) The first decision tree in the ensemble.\n",
        "b) The mean or median of the target variable.\n",
        "c) Randomly generated values.\n",
        "d) The residuals or errors from the previous iterations.\n",
        "\n",
        "**Question 10:** What happens during the prediction phase of a Gradient Boosting model?\n",
        "\n",
        "a) The model retrains all weak learners.\n",
        "b) The final prediction is the mean prediction of all weak learners.\n",
        "c) Each weak learner makes a prediction, and their contributions are combined to get the final prediction.\n",
        "d) The learning rate is adjusted based on the test data.\n",
        "\n",
        "**Answers:**\n",
        "1. c) An ensemble learning method that combines multiple weak models to create a strong predictive model.\n",
        "2. b) It's a mathematical term that represents the direction of steepest ascent of a function.\n",
        "3. c) Weak learners are added sequentially, where each new learner corrects the errors of the previous ones.\n",
        "4. d) It regulates the contribution of each weak learner to the ensemble.\n",
        "5. c) It can be sensitive to noisy data and outliers.\n",
        "6. d) AdaBoost combines weak learners in parallel, while Gradient Boosting combines them sequentially.\n",
        "7. c) By using decision trees with shallow depth.\n",
        "8. c) Log Loss (Logarithmic Loss).\n",
        "9. b) The mean or median of the target variable.\n",
        "10. c) Each weak learner makes a prediction, and their contributions are combined to get the final prediction."
      ],
      "metadata": {
        "id": "JfVgmCujc9fG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "enC436YrumLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Prediction**\n",
        "    - Predict the likelihood of a patient developing diabetes based on various features such as age, BMI, glucose levels, and family history.\n",
        "    - Classify patients into different stages of chronic kidney disease based on lab results and clinical measurements.\n",
        "\n",
        "2. **Medical Image Analysis**\n",
        "    - Predict the presence of tumors in medical images (like X-rays or MRIs) and classify them as benign or malignant.\n",
        "    - Detect early signs of retinal diseases in eye images.\n",
        "\n",
        "3. **Readmission Prediction**\n",
        "    - Predict the probability of a patient being readmitted to a hospital within 30 days based on their medical history, medications, and treatments.\n",
        "\n",
        "4. **Treatment Efficacy**\n",
        "    - Predict the efficacy of a treatment regimen on a particular patient based on their medical history and genetic information.\n",
        "    - Predict the progression of diseases like Alzheimer's based on current treatments and patient characteristics.\n",
        "\n",
        "5. **Drug Response**\n",
        "    - Predict how patients will respond to a particular medication based on genetic factors and drug interactions.\n",
        "\n",
        "6. **Resource Allocation**\n",
        "    - Use GBM to forecast hospital bed usage or ICU demand, which can be crucial for hospital resource allocation, especially during outbreaks or pandemics.\n",
        "\n",
        "7. **Predicting Disease Outbreaks**\n",
        "    - Utilize GBM to predict potential outbreaks in certain regions based on various features such as previous occurrences, climate data, and population movement.\n",
        "\n",
        "8. **Mortality Prediction**\n",
        "    - Predict patient mortality in ICU settings based on vitals, interventions, and other clinical data.\n",
        "\n",
        "9. **Clinical Note Classification**\n",
        "    - Classify clinical notes into various categories such as diagnostics, prescriptions, or patient feedback. This can help in automating the process of medical record organization.\n",
        "\n",
        "10. **Patient Segmentation**\n",
        "    - Segment patients based on their likelihood to suffer from multiple comorbidities, which can aid in targeted interventions and personalized treatment plans.\n",
        "\n",
        "11. **Cost Prediction**\n",
        "    - Predict the future healthcare costs of patients based on their current health status, treatments, and medical history. This can be useful for insurance companies.\n",
        "\n",
        "12. **Wearable Data Analysis**\n",
        "    - Use GBM to analyze data from wearable health devices (like Fitbit or Apple Watch) to predict potential health risks or recommend health interventions.\n",
        "\n",
        "13. **Diet and Nutrition Analysis**\n",
        "    - Predict the potential health risks or benefits for patients based on their dietary habits and nutritional intake.\n",
        "\n",
        "14. **Genetic Data Analysis**\n",
        "    - Predict susceptibility to certain diseases based on genetic data and family history.\n",
        "\n",
        "15. **Medical Fraud Detection**\n",
        "    - Use GBM to detect potential fraudulent claims or activities in the healthcare industry by analyzing patterns in billing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfdzu8HRuofz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "W1nVXX_ogHWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to implement a Gradient Boosting Machines (GBM) model using Python's scikit-learn library and a real-world health dataset. We'll use the \"Heart Disease UCI\" dataset, which is available on the UCI Machine Learning Repository and contains various features related to heart disease diagnosis."
      ],
      "metadata": {
        "id": "UJaEmZAnggaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "column_names = [\n",
        "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\",\n",
        "    \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
        "]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Replace missing values with NaN\n",
        "data.replace(\"?\", np.nan, inplace=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Convert categorical columns to numeric using one-hot encoding\n",
        "data = pd.get_dummies(data, columns=[\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"])\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data.drop(\"target\", axis=1)\n",
        "y = data[\"target\"]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the GBM model\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "id": "GTfUs4B5gLhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we loaded the \"Heart Disease UCI\" dataset, preprocessed it by handling missing values and converting categorical variables to numerical using one-hot encoding. We split the data into training and testing sets, then initialized and trained a Gradient Boosting Classifier using the training data. Finally, we evaluated the model's performance using accuracy and a classification report.\n",
        "\n",
        "Remember that real-world data preprocessing and model tuning can be more involved, and you might want to consider further steps like hyperparameter tuning and cross-validation to ensure optimal model performance."
      ],
      "metadata": {
        "id": "M0BXmCcugkEP"
      }
    }
  ]
}