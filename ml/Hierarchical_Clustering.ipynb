{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyBtaqjOBkzcM7wPAzrRgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Hierarchical_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering Model Background"
      ],
      "metadata": {
        "id": "_j5B3fzg-ZkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering is a popular unsupervised machine learning technique used for clustering data points into groups based on their similarities. The algorithm creates a hierarchical representation of the data in the form of a tree-like structure called a dendrogram. Each data point starts as its own cluster, and the algorithm recursively merges similar clusters until all the data points belong to a single cluster or a predefined number of clusters is reached.\n",
        "\n",
        "There are two main types of Hierarchical Clustering:\n",
        "\n",
        "1. Agglomerative (bottom-up): Starts with individual data points as separate clusters and then merges the closest clusters iteratively until the desired number of clusters is reached.\n",
        "2. Divisive (top-down): Begins with all data points in one cluster and recursively splits clusters into smaller ones based on dissimilarity until each data point is in its cluster.\n",
        "\n",
        "**Pros of Hierarchical Clustering**:\n",
        "\n",
        "1. **No need to specify the number of clusters:** Unlike other clustering techniques, you don't need to predefine the number of clusters before running the algorithm. The dendrogram allows you to choose the number of clusters later by cutting the tree at a specific level.\n",
        "\n",
        "2. **Hierarchical representation:** The dendrogram provides a visual representation of how data points are grouped, showing the hierarchical structure and allowing better understanding of relationships between clusters.\n",
        "\n",
        "3. **Doesn't require prior knowledge:** Hierarchical Clustering is suitable for exploratory analysis when you don't have prior knowledge about the optimal number of clusters.\n",
        "\n",
        "4. **Robust to noise:** It can handle noisy data effectively by grouping similar points even if they have minor variations.\n",
        "\n",
        "**Cons of Hierarchical Clustering**:\n",
        "\n",
        "1. **Computational complexity:** Hierarchical Clustering can be computationally expensive, especially for large datasets, as it requires pairwise distance calculations between all data points.\n",
        "\n",
        "2. **Lack of scalability:** Due to its complexity, the algorithm may not be suitable for very large datasets.\n",
        "\n",
        "3. **Sensitivity to distance metrics:** The choice of distance metric significantly affects the clustering result. Using inappropriate metrics may lead to suboptimal clusters.\n",
        "\n",
        "4. **Difficulty in handling non-globular clusters:** Hierarchical Clustering tends to produce nested, globular clusters, making it less effective for datasets with irregularly shaped or non-globular clusters.\n",
        "\n",
        "**When to use Hierarchical Clustering**:\n",
        "\n",
        "- **Small to medium-sized datasets:** Hierarchical Clustering works well for datasets with a moderate number of data points due to its computationally intensive nature.\n",
        "\n",
        "- **Exploratory data analysis:** When you don't know the optimal number of clusters and want to gain insights into the hierarchical structure of the data, Hierarchical Clustering can be a good choice.\n",
        "\n",
        "- **Noisy data:** Hierarchical Clustering can effectively handle noisy data and still produce meaningful clusters.\n",
        "\n",
        "- **Visualization:** If you need a visual representation of how data points are clustered and their hierarchical relationships, the dendrogram provided by Hierarchical Clustering is beneficial.\n",
        "\n",
        "- **When clustering interpretability is important:** The hierarchical representation of clusters and the ability to cut the dendrogram at different levels allow for more interpretable results."
      ],
      "metadata": {
        "id": "a3w8KaJ6CYNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "hIrpYAaTByxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Generate sample data using make_blobs\n",
        "n_samples = 150\n",
        "X, y = make_blobs(n_samples=n_samples, centers=3, cluster_std=0.6, random_state=42)\n",
        "\n",
        "# Hierarchical Clustering\n",
        "model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "clusters = model.fit_predict(X)\n",
        "\n",
        "# Plot the data points with their assigned clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.title(\"Hierarchical Clustering\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "dend = dendrogram(linkage(X, method='ward'))\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "93-0YgOLJKtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "wyJEQEaIHMTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Importing Libraries**: The code starts by importing necessary libraries:\n",
        "   - `numpy` (as `np`): Used for numerical computations with arrays.\n",
        "   - `matplotlib.pyplot` (as `plt`): Used for data visualization.\n",
        "   - `make_blobs` from `sklearn.datasets`: A function to generate synthetic clustered data.\n",
        "   - `AgglomerativeClustering` from `sklearn.cluster`: A class for performing agglomerative hierarchical clustering.\n",
        "   - `dendrogram` and `linkage` from `scipy.cluster.hierarchy`: Functions for generating and plotting dendrograms.\n",
        "\n",
        "2. **Generating Sample Data**: The code generates synthetic sample data using the `make_blobs` function from `sklearn.datasets`. It creates 150 samples (data points) with 3 centers (clusters), each having a standard deviation of 0.6. The `X` variable represents the data points, and `y` represents the cluster labels for each data point (though not used in this code).\n",
        "\n",
        "3. **Hierarchical Clustering**:\n",
        "   - A hierarchical clustering model is created using `AgglomerativeClustering`.\n",
        "   - `n_clusters=3` specifies the number of clusters we want to find.\n",
        "   - `affinity='euclidean'` specifies that Euclidean distance will be used as the distance metric.\n",
        "   - `linkage='ward'` specifies that the Ward linkage criterion will be used, which minimizes the variance of distances between clusters.\n",
        "\n",
        "4. **Cluster Assignment**: The `AgglomerativeClustering` model is used to predict the cluster assignments for each data point in `X`. The resulting cluster assignments are stored in the `clusters` variable.\n",
        "\n",
        "5. **Data Visualization**:\n",
        "   - The data points along with their assigned clusters are visualized using `plt.scatter`.\n",
        "   - `c=clusters` maps the clusters to colors using the 'viridis' colormap.\n",
        "   - `plt.xlabel` and `plt.ylabel` set the labels for the x and y axes, respectively.\n",
        "   - `plt.title` sets the title of the plot as \"Hierarchical Clustering\".\n",
        "   - `plt.show()` displays the plot.\n",
        "\n",
        "6. **Plotting the Dendrogram**:\n",
        "   - A dendrogram is a tree-like diagram that shows the hierarchical relationships between data points.\n",
        "   - The code uses `plt.figure(figsize=(10, 6))` to set the size of the dendrogram plot.\n",
        "   - `plt.title` sets the title of the dendrogram plot.\n",
        "   - `dendrogram(linkage(X, method='ward'))` computes the hierarchical clustering linkage and generates the dendrogram plot.\n",
        "   - `plt.xlabel` and `plt.ylabel` set the labels for the x and y axes, respectively.\n",
        "   - `plt.show()` displays the dendrogram plot.\n",
        "\n",
        "In summary, this code demonstrates hierarchical clustering on synthetic data generated using `make_blobs`. It first performs hierarchical clustering using the Ward linkage criterion and then visualizes the clustered data points using a scatter plot. Additionally, it displays the hierarchical relationships among the data points using a dendrogram. The goal is to showcase how hierarchical clustering groups similar data points together to form clusters."
      ],
      "metadata": {
        "id": "Orz1mxSZts4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "RF54JG-ASXzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of using Hierarchical Clustering in a healthcare setting to identify patient subgroups based on their health parameters. This example involves grouping patients based on their physiological measurements and can be used for personalized healthcare or medical research purposes.\n",
        "\n",
        "**Example: Identifying Patient Subgroups using Hierarchical Clustering**\n",
        "\n",
        "**Objective:** To cluster patients based on their physiological measurements to identify distinct subgroups with similar health profiles.\n",
        "\n",
        "**Data:** The dataset contains physiological measurements for a group of patients, including features such as blood pressure, heart rate, cholesterol levels, body mass index (BMI), and blood glucose levels.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Data Collection:** Collect physiological measurements from a group of patients. Each patient's data includes multiple features representing their health parameters.\n",
        "\n",
        "2. **Data Preprocessing:** Perform any necessary data preprocessing steps, such as handling missing values, standardizing the features, and normalizing the data.\n",
        "\n",
        "3. **Distance Metric:** Choose an appropriate distance metric to calculate the similarity between patients. The choice of distance metric will depend on the nature of the features and the characteristics of the data.\n",
        "\n",
        "4. **Hierarchical Clustering:** Apply Hierarchical Clustering to group patients based on their physiological measurements. There are two main approaches to hierarchical clustering: Agglomerative (bottom-up) and Divisive (top-down). In this example, we'll use the Agglomerative approach.\n",
        "\n",
        "5. **Dendrogram:** After performing hierarchical clustering, visualize the results using a dendrogram. A dendrogram is a tree-like diagram that illustrates the hierarchical relationships between the clusters.\n",
        "\n",
        "6. **Cutting the Dendrogram:** Decide on the number of clusters to form by cutting the dendrogram at an appropriate height. This step involves selecting the desired number of clusters based on domain knowledge or using statistical methods like the Elbow Method.\n",
        "\n",
        "7. **Assigning Patients to Clusters:** Assign each patient to a specific cluster based on the clustering results.\n",
        "\n",
        "8. **Cluster Analysis:** Perform cluster analysis to understand the characteristics of each patient subgroup. This may involve computing the average values of physiological measurements within each cluster or conducting statistical tests to identify significant differences between clusters.\n",
        "\n",
        "9. **Interpretation and Applications:** Interpret the results to gain insights into patient subgroups. For example, you may identify a cluster with patients having high blood pressure and high cholesterol, another cluster with patients having low BMI and normal glucose levels, etc. These insights can be valuable for personalized healthcare plans or medical research studies.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "- Personalized Healthcare: Hierarchical clustering can help identify patient subgroups with similar health profiles, enabling healthcare professionals to tailor treatments and interventions to specific patient clusters.\n",
        "\n",
        "- Research Insights: By grouping patients based on health parameters, researchers can identify distinct subgroups with unique characteristics, leading to new insights into disease patterns and potential risk factors.\n",
        "\n",
        "- Efficient Resource Allocation: Healthcare facilities can allocate resources more efficiently by understanding the distribution of patient subgroups and their specific needs.\n",
        "\n",
        "Please note that this example is for illustrative purposes. In a real-world scenario, the process may involve more extensive data analysis, validation, and expert domain knowledge to ensure the reliability and validity of the results."
      ],
      "metadata": {
        "id": "0fhB4Xbh-nmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "T-l1k2z-ZJ8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Hierarchical Clustering?\n",
        "   Hierarchical Clustering is a popular unsupervised learning technique used in data mining and machine learning to group similar data points into clusters in a hierarchical manner. It creates a tree-like structure of nested clusters.\n",
        "\n",
        "2. How does Hierarchical Clustering work?\n",
        "   Hierarchical Clustering starts by considering each data point as its own cluster and then iteratively merges the closest clusters until all data points belong to a single cluster or a specified number of clusters is reached.\n",
        "\n",
        "3. What are the two main types of Hierarchical Clustering?\n",
        "   Hierarchical Clustering can be of two types: Agglomerative Hierarchical Clustering (bottom-up) and Divisive Hierarchical Clustering (top-down). Agglomerative starts with individual data points as clusters and merges them, while divisive starts with all data points in one cluster and recursively divides them.\n",
        "\n",
        "4. What distance metrics are commonly used in Hierarchical Clustering?\n",
        "   Common distance metrics used in Hierarchical Clustering include Euclidean distance, Manhattan distance, Pearson correlation coefficient, and others, depending on the nature of the data.\n",
        "\n",
        "5. How is the similarity between clusters measured in Hierarchical Clustering?\n",
        "   The similarity between clusters is measured using linkage criteria, such as single linkage (nearest neighbor), complete linkage (farthest neighbor), and average linkage (average distance), among others.\n",
        "\n",
        "6. What is the dendrogram in Hierarchical Clustering?\n",
        "   A dendrogram is a tree-like diagram that represents the hierarchical structure of clusters. It shows how clusters are merged at each step, and the vertical height in the dendrogram represents the distance between clusters.\n",
        "\n",
        "7. Can Hierarchical Clustering handle large datasets efficiently?\n",
        "   Hierarchical Clustering can become computationally expensive for large datasets, as the time complexity is O(n^3) for agglomerative clustering. Techniques like BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) can be used to handle large datasets more efficiently.\n",
        "\n",
        "8. Is Hierarchical Clustering sensitive to the order of data points?\n",
        "   Yes, the order of data points can influence the clustering outcome in Hierarchical Clustering, especially in agglomerative clustering. Changing the order of data points can lead to different final cluster configurations.\n",
        "\n",
        "9. What are some common applications of Hierarchical Clustering?\n",
        "   Hierarchical Clustering finds applications in various fields, such as customer segmentation, document clustering, image segmentation, and gene expression analysis.\n",
        "\n",
        "10. Can Hierarchical Clustering handle non-Euclidean data?\n",
        "    Yes, Hierarchical Clustering can handle non-Euclidean data by using appropriate distance metrics or similarity measures tailored to the specific data types, such as text data or genetic sequences.\n",
        "\n",
        "Remember that Hierarchical Clustering is just one of many clustering algorithms, and its performance and suitability depend on the specific dataset and problem at hand."
      ],
      "metadata": {
        "id": "Y71q2zZLlGW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "GALOku6Fd4qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:** What is Hierarchical Clustering?\n",
        "\n",
        "a) A type of machine learning algorithm that classifies data into distinct categories.\n",
        "b) A technique used to reduce the dimensions of data.\n",
        "c) A method of grouping similar data points into clusters based on their similarity.\n",
        "d) A process of optimizing weights in a neural network.\n",
        "\n",
        "**Question 2:** Which of the following statements is true about Hierarchical Clustering?\n",
        "\n",
        "a) It requires the number of clusters to be specified in advance.\n",
        "b) It can only handle numerical data.\n",
        "c) It generates a tree-like structure of clusters.\n",
        "d) It is computationally efficient for large datasets.\n",
        "\n",
        "**Question 3:** What is the main advantage of Agglomerative Hierarchical Clustering?\n",
        "\n",
        "a) It is faster and requires less memory compared to Divisive Hierarchical Clustering.\n",
        "b) It starts with individual data points and recursively merges them into clusters.\n",
        "c) It can handle non-numerical (categorical) data effectively.\n",
        "d) It doesn't require a linkage criterion.\n",
        "\n",
        "**Question 4:** In the context of Hierarchical Clustering, what does \"linkage\" refer to?\n",
        "\n",
        "a) The process of selecting the number of clusters in advance.\n",
        "b) The method used to measure the dissimilarity between clusters.\n",
        "c) The number of levels in the hierarchical tree.\n",
        "d) The visualization technique used to represent clusters.\n",
        "\n",
        "**Question 5:** Which of the following linkage methods tends to create compact, spherical clusters?\n",
        "\n",
        "a) Single Linkage\n",
        "b) Complete Linkage\n",
        "c) Average Linkage\n",
        "d) Ward's Linkage\n",
        "\n",
        "**Question 6:** What is the dendrogram in Hierarchical Clustering?\n",
        "\n",
        "a) A graphical representation of the similarity matrix.\n",
        "b) The final output of the clustering algorithm.\n",
        "c) A tree-like diagram that shows the sequence of merging clusters.\n",
        "d) The matrix containing the pairwise distances between data points.\n",
        "\n",
        "**Question 7:** What is the primary drawback of Hierarchical Clustering in terms of scalability?\n",
        "\n",
        "a) It is not suitable for handling noisy data.\n",
        "b) It can only handle a limited number of data points.\n",
        "c) It requires a lot of memory and computational resources for large datasets.\n",
        "d) It doesn't work well with high-dimensional data.\n",
        "\n",
        "**Question 8:** Which of the following is NOT a distance metric used in Hierarchical Clustering?\n",
        "\n",
        "a) Euclidean Distance\n",
        "b) Manhattan Distance\n",
        "c) Pearson Correlation\n",
        "d) Logistic Loss\n",
        "\n",
        "**Question 9:** At which step does Hierarchical Clustering stop?\n",
        "\n",
        "a) When the desired number of clusters is reached.\n",
        "b) When all data points are isolated.\n",
        "c) When the dendrogram is fully formed.\n",
        "d) When the silhouette score is maximized.\n",
        "\n",
        "**Question 10:** Which type of data is most suitable for Hierarchical Clustering?\n",
        "\n",
        "a) Data with a clear linear separation between clusters.\n",
        "b) Data with a high number of dimensions.\n",
        "c) Data with no discernible structure.\n",
        "d) Data with an inherent hierarchical organization.\n",
        "\n",
        "**Answers:**\n",
        "\n",
        "1. c) A method of grouping similar data points into clusters based on their similarity.\n",
        "2. c) It generates a tree-like structure of clusters.\n",
        "3. b) It starts with individual data points and recursively merges them into clusters.\n",
        "4. b) The method used to measure the dissimilarity between clusters.\n",
        "5. d) Ward's Linkage\n",
        "6. c) A tree-like diagram that shows the sequence of merging clusters.\n",
        "7. c) It requires a lot of memory and computational resources for large datasets.\n",
        "8. d) Logistic Loss\n",
        "9. b) When all data points are isolated.\n",
        "10. d) Data with an inherent hierarchical organization."
      ],
      "metadata": {
        "id": "wwTTK42jd6KJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "43O7TQByvAgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Patient Segmentation**:\n",
        "    - **Objective**: To group patients based on their medical histories or health behaviors.\n",
        "    - **Dataset**: Electronic health records (EHR) or surveys.\n",
        "    - **Application**: Targeted health campaigns, personalized treatment, and resource allocation.\n",
        "\n",
        "2. **Disease Subtype Identification**:\n",
        "    - **Objective**: Identify subtypes of a disease based on symptom profiles.\n",
        "    - **Dataset**: EHR, symptom logs.\n",
        "    - **Application**: Precision medicine, personalized therapy.\n",
        "\n",
        "3. **Genomic Data Analysis**:\n",
        "    - **Objective**: Classify genes based on their expression profiles.\n",
        "    - **Dataset**: Gene expression data from microarray experiments or RNA-seq data.\n",
        "    - **Application**: Discovering groups of genes that function together or are co-regulated.\n",
        "\n",
        "4. **Medical Imaging**:\n",
        "    - **Objective**: Group similar radiology images to identify patterns or anomalies.\n",
        "    - **Dataset**: MRI, CT scans, or X-rays of a particular organ or condition.\n",
        "    - **Application**: Improved diagnosis, disease progression monitoring.\n",
        "\n",
        "5. **Drug Reaction Profiling**:\n",
        "    - **Objective**: Group patients based on their reactions to a specific medication or treatment.\n",
        "    - **Dataset**: Patient drug reaction records.\n",
        "    - **Application**: Safety monitoring, personalized medicine.\n",
        "\n",
        "6. **Medical Literature Clustering**:\n",
        "    - **Objective**: Cluster medical research papers based on their topics or content.\n",
        "    - **Dataset**: Abstracts or full texts from medical journals.\n",
        "    - **Application**: Improved literature review, meta-analysis, identifying research gaps.\n",
        "\n",
        "7. **Healthcare Provider Profiling**:\n",
        "    - **Objective**: Group healthcare providers based on their practice patterns or patient outcomes.\n",
        "    - **Dataset**: Insurance claim data, patient reviews.\n",
        "    - **Application**: Quality assurance, network optimization.\n",
        "\n",
        "8. **Epidemiological Study**:\n",
        "    - **Objective**: Group regions or communities based on the prevalence of a particular disease or health condition.\n",
        "    - **Dataset**: Disease prevalence data across regions.\n",
        "    - **Application**: Resource allocation, targeted interventions.\n",
        "\n",
        "9. **Wearable Data Analysis**:\n",
        "    - **Objective**: Segment users based on their health and activity metrics from wearables.\n",
        "    - **Dataset**: Wearable device data like heart rate, step count, sleep patterns.\n",
        "    - **Application**: Personalized fitness recommendations, health monitoring.\n",
        "\n",
        "10. **Mental Health Analysis**:\n",
        "    - **Objective**: Classify patients based on their cognitive, behavioral, or emotional patterns.\n",
        "    - **Dataset**: Psychological assessments, therapy session notes.\n",
        "    - **Application**: Tailored therapeutic interventions, improved patient care.\n",
        "\n"
      ],
      "metadata": {
        "id": "WaA0lq7VvCfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "-aa9fvO1C1zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clustering is a method used to group similar data points into clusters based on their pairwise similarity or distance. In this example, I'll walk you through a basic implementation of hierarchical clustering using a real-world health dataset. We'll use the \"Pima Indians Diabetes Database\" dataset, which contains information about diabetes patients.\n",
        "\n"
      ],
      "metadata": {
        "id": "wp_R3BRMhTxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigree\", \"Age\", \"Outcome\"]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(\"Outcome\", axis=1)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linked = linkage(X_scaled, method='ward')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering model\n",
        "num_clusters = 3  # You can choose the number of clusters based on the dendrogram\n",
        "cluster_model = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "data['Cluster'] = cluster_model.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Glucose', y='BMI', hue='Cluster', data=data, palette='viridis')\n",
        "plt.title('Hierarchical Clustering - Glucose vs. BMI')\n",
        "plt.xlabel('Glucose')\n",
        "plt.ylabel('BMI')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Oc3wa3skhE_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indians Diabetes Database dataset, preprocess the features by standardizing them, perform hierarchical clustering using the Ward linkage method, and visualize the results using a dendrogram and scatter plot.\n",
        "\n",
        "Keep in mind that the choice of linkage method and number of clusters can significantly affect the results. This example provides a basic illustration of the process, but in real-world scenarios, you might need to experiment with different parameters and methods to find the best clustering solution for your specific dataset."
      ],
      "metadata": {
        "id": "uDLQCW8XhXDJ"
      }
    }
  ]
}