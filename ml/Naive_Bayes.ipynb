{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTj2wSf01atAnQ3ODHqsYj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Model Background"
      ],
      "metadata": {
        "id": "xHIxFK9k_w4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a popular classification algorithm based on Bayes' theorem, which is used for supervised learning tasks, particularly in the field of machine learning and natural language processing. Despite its simplicity, Naive Bayes can be surprisingly effective in many real-world scenarios.\n",
        "\n",
        "**How Naive Bayes works:**\n",
        "Naive Bayes is a probabilistic model that makes predictions by calculating the probability of a given input belonging to a particular class. It assumes that all features are independent of each other, which is where the term \"naive\" comes from. This is a strong assumption and may not hold true in all cases, but in practice, Naive Bayes can still perform well, especially with discrete features or when there are many irrelevant features that cancel each other out.\n",
        "\n",
        "The algorithm is based on Bayes' theorem:\n",
        "\n",
        "```\n",
        "P(y|x) = (P(x|y) * P(y)) / P(x)\n",
        "```\n",
        "\n",
        "where:\n",
        "- `P(y|x)` is the posterior probability of class `y` given input `x`.\n",
        "- `P(x|y)` is the likelihood of input `x` given class `y`.\n",
        "- `P(y)` is the prior probability of class `y`.\n",
        "- `P(x)` is the evidence probability of input `x`.\n",
        "\n",
        "**Pros of Naive Bayes:**\n",
        "1. **Simplicity and Speed:** Naive Bayes is easy to implement and computationally efficient. It is particularly useful for large datasets.\n",
        "2. **Scalability:** Due to its simplicity and low computational overhead, Naive Bayes can handle high-dimensional data well.\n",
        "3. **Good with small datasets:** It requires fewer training data compared to more complex algorithms, making it suitable for cases with limited labeled data.\n",
        "4. **Real-time predictions:** Because of its speed, Naive Bayes is often used for real-time applications.\n",
        "5. **Interpretability:** The results are easily interpretable, providing insights into the importance of different features.\n",
        "\n",
        "**Cons of Naive Bayes:**\n",
        "1. **Naive Assumption:** The independence assumption may not hold in many real-world scenarios, which can lead to suboptimal predictions.\n",
        "2. **Limited Expressiveness:** Due to its simplicity, Naive Bayes may not capture complex relationships in the data as well as more sophisticated models.\n",
        "3. **Zero Probability Issue:** If a certain feature value in the test data wasn't present in the training data for a particular class, Naive Bayes assigns a zero probability, resulting in inaccurate predictions.\n",
        "4. **Sensitive to Feature Correlations:** Since it assumes independence among features, correlated features can negatively impact performance.\n",
        "5. **Continuous Features Handling:** Naive Bayes works well with discrete features but may require some data preprocessing for continuous features.\n",
        "\n",
        "**When to use Naive Bayes:**\n",
        "Naive Bayes is most suitable when:\n",
        "1. The independence assumption is reasonably valid for the given problem.\n",
        "2. You have a large dataset, and efficiency is a priority.\n",
        "3. The problem at hand is relatively simple, and complex interactions between features are not essential for accurate predictions.\n",
        "4. You have limited labeled data, and you want a quick and interpretable solution.\n",
        "5. You need to make real-time predictions.\n",
        "\n",
        "It's important to note that while Naive Bayes can be a good starting point, especially for text classification tasks, it's always a good idea to compare its performance with other more complex algorithms to ensure you are getting the best possible results for your specific problem."
      ],
      "metadata": {
        "id": "uWZ7ADCl9Sdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "ieaa5DGFa4Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset from scikit-learn\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "KrosLHeXz3Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "EVzh1Jkx0Qsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import the required libraries:\n",
        "   - `numpy` (aliased as `np`) for numerical operations.\n",
        "   - `pandas` (aliased as `pd`) for data manipulation and analysis.\n",
        "   - `train_test_split` from `sklearn.model_selection` to split the dataset into training and testing sets.\n",
        "   - `GaussianNB` from `sklearn.naive_bayes` to create and train the Naive Bayes classifier.\n",
        "   - `accuracy_score`, `classification_report`, and `confusion_matrix` from `sklearn.metrics` to evaluate the classifier's performance.\n",
        "\n",
        "2. Load the Iris dataset from scikit-learn:\n",
        "   - The Iris dataset is one of the standard datasets available in scikit-learn, representing iris flower species with four features (sepal length, sepal width, petal length, petal width) and one target variable (species class).\n",
        "\n",
        "3. Split the dataset into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: This function randomly splits the data into training and testing sets. It takes the features (X) and target labels (y) as input and returns four sets: `X_train`, `X_test`, `y_train`, and `y_test`. The test set size is set to 20% of the entire dataset, and `random_state=42` ensures reproducibility by fixing the random seed.\n",
        "\n",
        "4. Create and train the Naive Bayes classifier:\n",
        "   - `GaussianNB()`: Creates an instance of the Gaussian Naive Bayes classifier, which is appropriate for continuous features like the ones in the Iris dataset.\n",
        "   - `nb_classifier.fit(X_train, y_train)`: Trains the Naive Bayes classifier using the training data (`X_train` and `y_train`).\n",
        "\n",
        "5. Make predictions on the test set:\n",
        "   - `y_pred = nb_classifier.predict(X_test)`: The trained classifier is used to predict the target labels (`y_pred`) for the test set (`X_test`).\n",
        "\n",
        "6. Evaluate the classifier's performance:\n",
        "   - `accuracy_score(y_test, y_pred)`: Calculates the accuracy of the classifier by comparing the predicted labels (`y_pred`) with the true labels from the test set (`y_test`).\n",
        "   - `confusion_matrix(y_test, y_pred)`: Computes the confusion matrix, which is a table showing the true positive, true negative, false positive, and false negative counts for each class.\n",
        "   - `classification_report(y_test, y_pred)`: Generates a detailed report showing precision, recall, F1-score, and support for each class.\n",
        "\n",
        "7. Print the results:\n",
        "   - The script prints the accuracy, confusion matrix, and classification report to evaluate the Naive Bayes classifier's performance.\n",
        "\n",
        "That's it! The code loads the Iris dataset, splits it into training and testing sets, trains a Naive Bayes classifier, makes predictions on the test set, and evaluates the classifier's accuracy using various metrics."
      ],
      "metadata": {
        "id": "MwhHFeO5xUKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "1E8Ymy8pz03X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Example: Diagnosing a Respiratory Infection**\n",
        "\n",
        "Imagine a healthcare provider wants to diagnose whether a patient has a respiratory infection (e.g., common cold, flu, or pneumonia) based on the patient's symptoms. The symptoms to consider are:\n",
        "\n",
        "1. Cough (Yes or No)\n",
        "2. Fever (Yes or No)\n",
        "3. Runny Nose (Yes or No)\n",
        "4. Shortness of Breath (Yes or No)\n",
        "\n",
        "Additionally, the healthcare provider has historical data of patients who were diagnosed with respiratory infections and the corresponding symptoms they exhibited.\n",
        "\n",
        "**Building the Naive Bayes Model**\n",
        "\n",
        "1. **Data Collection:** The healthcare provider collects data from a group of patients who were diagnosed with respiratory infections and notes their symptoms (cough, fever, runny nose, and shortness of breath). This data forms the positive class.\n",
        "\n",
        "2. **Data Collection for Negative Class:** The healthcare provider also collects data from a group of patients who were not diagnosed with respiratory infections and notes their symptoms. This data forms the negative class.\n",
        "\n",
        "3. **Data Preprocessing:** The data is preprocessed to encode categorical features (Yes or No) as binary values (1 or 0).\n",
        "\n",
        "4. **Training the Naive Bayes Model:** The healthcare provider uses the preprocessed data to train a Naive Bayes classifier. The Naive Bayes model calculates the probability of each symptom given the presence or absence of a respiratory infection (positive or negative class). It assumes independence among the symptoms, which is a common assumption in Naive Bayes.\n",
        "\n",
        "5. **Making Predictions:** Once the model is trained, the healthcare provider can use it to make predictions for new patients. When a new patient visits with specific symptoms, the model calculates the probability of each class (respiratory infection or no respiratory infection) based on the symptoms exhibited by the patient. The model predicts the class with the higher probability as the diagnosis for the patient.\n",
        "\n",
        "**Real-World Application**\n",
        "\n",
        "Suppose a new patient visits the healthcare provider with symptoms: cough (Yes), fever (Yes), runny nose (Yes), and shortness of breath (No). The Naive Bayes model calculates the probabilities for both classes (respiratory infection and no respiratory infection) based on the patient's symptoms. If the probability of respiratory infection is higher, the model predicts that the patient has a respiratory infection and prescribes appropriate treatment or further diagnostic tests.\n",
        "\n",
        "It's important to note that while Naive Bayes can be useful in certain scenarios, it may not capture complex relationships between symptoms, and its performance heavily relies on the quality and representativeness of the training data. In real-world healthcare settings, more sophisticated models and extensive data may be required for accurate and reliable medical diagnosis. Naive Bayes serves as an elementary example, but actual medical decision-making typically involves a more comprehensive approach."
      ],
      "metadata": {
        "id": "PEDPG6A0FWQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "XT3ifKTh3I7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the Naive Bayes model, and how does it work?\n",
        "   - The Naive Bayes model is a probabilistic machine learning algorithm used for classification tasks. It's based on Bayes' theorem and makes a \"naive\" assumption that features are conditionally independent given the class label.\n",
        "\n",
        "2. Why is it called \"Naive\" Bayes?\n",
        "   - It is called \"Naive\" because it makes a simplifying assumption that the features are independent, which is often not the case in real-world scenarios. Despite this assumption, Naive Bayes often performs surprisingly well in practice and is computationally efficient.\n",
        "\n",
        "3. In which applications is Naive Bayes commonly used?\n",
        "   - Naive Bayes is commonly used in text classification tasks like spam detection, sentiment analysis, document categorization, and language identification. It's also used in various other domains like medical diagnosis, recommendation systems, and fraud detection.\n",
        "\n",
        "4. How does Naive Bayes handle continuous and categorical features?\n",
        "   - Naive Bayes can handle both continuous and categorical features. For continuous features, it typically assumes a Gaussian (normal) distribution. For categorical features, it estimates probabilities using frequency counts.\n",
        "\n",
        "5. Is Naive Bayes suitable for high-dimensional data?\n",
        "   - Yes, Naive Bayes is particularly well-suited for high-dimensional data, like text data, where the number of features can be very large. Its performance often remains competitive even with a large number of features.\n",
        "\n",
        "6. What are the advantages of using Naive Bayes?\n",
        "   - Naive Bayes is simple, easy to implement, and computationally efficient. It works well with high-dimensional data and requires relatively little training data. It can also handle missing values gracefully.\n",
        "\n",
        "7. What are the limitations of the Naive Bayes model?\n",
        "   - The main limitation of Naive Bayes is its assumption of feature independence, which may not hold true in some cases. Due to this assumption, it may not perform well on datasets with strong interdependencies between features. However, it still performs reasonably well in practice.\n",
        "\n",
        "8. Can Naive Bayes handle multi-class classification?\n",
        "   - Yes, Naive Bayes can handle multi-class classification tasks. It assigns the class label with the highest probability among all classes.\n",
        "\n",
        "9. Is Naive Bayes sensitive to irrelevant features?\n",
        "   - Naive Bayes is generally considered to be somewhat robust to irrelevant features because it relies on probabilities rather than weights assigned to features. However, extremely irrelevant features may still have some impact on the model's performance.\n",
        "\n",
        "10. How does Laplace smoothing (additive smoothing) help in Naive Bayes?\n",
        "    - Laplace smoothing is used to address the problem of zero probabilities for unseen features. It adds a small value (usually 1) to all feature counts during probability estimation to ensure non-zero probabilities for all features, even those not seen in the training data. This prevents the model from assigning zero probabilities to unseen data during testing."
      ],
      "metadata": {
        "id": "JAVaha5r6o5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "oI76wd4Sgms1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:** What is the underlying principle behind the Naive Bayes algorithm?\n",
        "\n",
        "**a)** It uses complex probabilistic calculations to make predictions.\n",
        "**b)** It assumes that features are independent given the class.\n",
        "**c)** It relies on deep neural networks for feature extraction.\n",
        "**d)** It only works with binary classification problems.\n",
        "\n",
        "**Question 2:** In the context of a Naive Bayes classifier, what does the \"naive\" assumption refer to?\n",
        "\n",
        "**a)** The algorithm assumes that the features are irrelevant to the class labels.\n",
        "**b)** The algorithm assumes that the features are not correlated with each other.\n",
        "**c)** The algorithm assumes that all features have equal importance.\n",
        "**d)** The algorithm assumes that the class labels are randomly assigned.\n",
        "\n",
        "**Question 3:** When is the Naive Bayes algorithm particularly useful?\n",
        "\n",
        "**a)** When dealing with large datasets with complex relationships between features.\n",
        "**b)** When the assumption of feature independence does not hold true.\n",
        "**c)** When the dataset has a small number of features and limited training data.\n",
        "**d)** When the class distribution is highly imbalanced.\n",
        "\n",
        "**Question 4:** Which of the following types of Naive Bayes algorithms is suitable for text classification tasks, where the features are typically words or terms?\n",
        "\n",
        "**a)** Gaussian Naive Bayes\n",
        "**b)** Multinomial Naive Bayes\n",
        "**c)** Bernoulli Naive Bayes\n",
        "**d)** Poisson Naive Bayes\n",
        "\n",
        "**Question 5:** How does the Naive Bayes algorithm handle missing data during the prediction process?\n",
        "\n",
        "**a)** It replaces missing values with the mean of the available data.\n",
        "**b)** It removes the instances with missing values from the dataset.\n",
        "**c)** It assigns a probability of 0 to missing values and continues calculations.\n",
        "**d)** It imputes missing values based on the mode of the feature's distribution.\n",
        "\n",
        "**Question 6:** In a binary classification problem, if the probability of an instance belonging to class A is higher than class B according to the Naive Bayes algorithm, how is the instance classified?\n",
        "\n",
        "**a)** The instance is classified as class A.\n",
        "**b)** The instance is classified as class B.\n",
        "**c)** Both classes are assigned to the instance based on their probabilities.\n",
        "**d)** The instance is discarded due to uncertainty.\n",
        "\n",
        "**Question 7:** What is the formula for calculating the posterior probability using the Naive Bayes algorithm?\n",
        "\n",
        "**a) P(class|features) = P(features|class) * P(class) / P(features)**\n",
        "**b) P(features|class) = P(class|features) * P(features) / P(class)**\n",
        "**c) P(class) = P(features|class) * P(class|features) / P(features)**\n",
        "**d) P(features) = P(class|features) * P(features|class) / P(class)**\n",
        "\n",
        "**Question 8:** In text classification using Naive Bayes, how is the \"bag-of-words\" model represented?\n",
        "\n",
        "**a)** A sequential model of word tokens.\n",
        "**b)** A tree-based structure of syntactic relationships.\n",
        "**c)** A matrix representing word frequencies in each document.\n",
        "**d)** A graph of semantic connections between words.\n",
        "\n",
        "**Question 9:** Which assumption of the Naive Bayes algorithm can sometimes lead to suboptimal performance, especially in cases where features are not completely independent?\n",
        "\n",
        "**a)** The assumption of equal feature importance.\n",
        "**b)** The assumption of conditional feature dependence.\n",
        "**c)** The assumption of class label irrelevance.\n",
        "**d)** The assumption of feature distribution symmetry.\n",
        "\n",
        "**Question 10:** When can Laplace (additive) smoothing be useful in Naive Bayes?\n",
        "\n",
        "**a)** When dealing with categorical features only.\n",
        "**b)** When the dataset has a small number of instances.\n",
        "**c)** When features are strongly correlated with each other.\n",
        "**d)** When dealing with continuous numerical features.\n",
        "\n",
        "**Answers:**\n",
        "1. b\n",
        "2. b\n",
        "3. c\n",
        "4. b\n",
        "5. c\n",
        "6. a\n",
        "7. a\n",
        "8. c\n",
        "9. b\n",
        "10. b"
      ],
      "metadata": {
        "id": "DXGFC32ago-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "l15dHNrO0Qhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Disease Diagnosis**:\n",
        "    - Objective: Predict the likelihood of a patient having a specific disease (e.g., diabetes, heart disease) based on their medical history and test results.\n",
        "    - Data: Patient medical records (anonymized), symptoms, laboratory test results.\n",
        "\n",
        "2. **Medical Text Classification**:\n",
        "    - Objective: Classify medical research papers or clinical notes into categories such as diagnosis, treatment, prognosis, etc.\n",
        "    - Data: Collection of medical research papers, clinical notes, or medical transcriptions.\n",
        "\n",
        "3. **Predicting Patient Readmission**:\n",
        "    - Objective: Predict the likelihood of a patient being readmitted to the hospital within a certain timeframe.\n",
        "    - Data: Patient demographics, diagnosis details, treatments provided, previous hospitalizations.\n",
        "\n",
        "4. **Medication Side Effect Prediction**:\n",
        "    - Objective: Predict possible side effects a patient might experience when taking a medication, based on their medical history.\n",
        "    - Data: Medical histories and patient reports on medication side effects.\n",
        "\n",
        "5. **Sentiment Analysis on Patient Reviews**:\n",
        "    - Objective: Analyze sentiments (positive, negative, neutral) from patient reviews about hospitals, treatments, or doctors.\n",
        "    - Data: Reviews or feedback from patients from online forums, hospital websites, or satisfaction surveys.\n",
        "\n",
        "6. **Healthcare Product Reviews**:\n",
        "    - Objective: Classify user reviews of healthcare products into categories like effective, not effective, caused side effects, etc.\n",
        "    - Data: User reviews from e-commerce sites, forums, or surveys.\n",
        "\n",
        "7. **Predicting Disease Outbreaks**:\n",
        "    - Objective: Analyze data from different regions to predict potential disease outbreaks.\n",
        "    - Data: Historical data on disease incidence rates, vaccination rates, and environmental factors.\n",
        "\n",
        "8. **Genetic Trait Prediction**:\n",
        "    - Objective: Predict the likelihood of an individual having or passing on a genetic trait/disease based on genetic markers.\n",
        "    - Data: Genetic data samples, family history.\n",
        "\n",
        "9. **Spam Detection in Medical Literature**:\n",
        "    - Objective: Filter out irrelevant or potentially misleading medical content/advertisements from genuine research papers or articles.\n",
        "    - Data: Collection of medical articles, both genuine and promotional.\n",
        "\n",
        "10. **Mental Health Analysis from Social Media**:\n",
        "    - Objective: Analyze social media posts to predict potential mental health issues or states.\n",
        "    - Data: Anonymized social media posts, mental health self-reports.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "glaP1FLt0Sn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "aNf-MGz4D_55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple example demonstrating how to use the Naive Bayes classifier with the famous \"breast cancer\" dataset from the scikit-learn library. The dataset is used to classify breast cancer as malignant or benign based on certain features."
      ],
      "metadata": {
        "id": "dtC-O01bzHWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n"
      ],
      "metadata": {
        "id": "jyKA3g-Zy8R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple demonstration. In a real-world application, you would:\n",
        "\n",
        "Investigate the data and its features in more detail.\n",
        "Consider performing feature scaling or normalization if required.\n",
        "Maybe apply some feature selection techniques to pick the most relevant features.\n",
        "Consider other algorithms to benchmark the Naive Bayes performance against them.\n",
        "Remember, the quality of the predictions depends not only on the model you use, but also on the quality and quantity of your data and the features you choose."
      ],
      "metadata": {
        "id": "u9Ic-Y7yzMmd"
      }
    }
  ]
}