{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKmHr6vrZmlVnef3GUfPMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/models/blob/main/ml/Lasso_Classification_(Logistic_Regression_with_L1_Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "-J8-VBTI-rKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Classification, also known as Logistic Regression with L1 regularization or L1-penalized logistic regression, is a variant of logistic regression that incorporates L1 regularization. Regularization is a technique used to prevent overfitting and improve the generalization of the model. In L1 regularization, the penalty term added to the cost function is proportional to the absolute values of the model's coefficients.\n",
        "\n",
        "In Lasso Classification, the objective function can be represented as follows:\n",
        "\n",
        "Cost Function = -1/m * Σ(y * log(h(x)) + (1 - y) * log(1 - h(x))) + λ * Σ|θi|\n",
        "\n",
        "Where:\n",
        "- m is the number of training examples.\n",
        "- y is the true class labels.\n",
        "- h(x) is the sigmoid function, which predicts the probability of the positive class.\n",
        "- θi represents the model's coefficients (weights).\n",
        "- λ is the regularization parameter, controlling the strength of the penalty term.\n",
        "\n",
        "**Pros of Lasso Classification (Logistic Regression with L1 Regularization)**:\n",
        "\n",
        "1. Feature Selection: L1 regularization has the property of driving some coefficients to exactly zero. This leads to automatic feature selection, as some features become irrelevant and effectively excluded from the model. This can be helpful in situations where you suspect that only a subset of features are relevant for making predictions.\n",
        "\n",
        "2. Simplicity and Interpretability: By eliminating irrelevant features, Lasso Classification can lead to simpler and more interpretable models, making it easier to understand the impact of individual features on the predicted outcomes.\n",
        "\n",
        "3. Robustness to Overfitting: L1 regularization helps prevent overfitting by discouraging overly complex models. It achieves a form of dimensionality reduction, reducing the risk of memorizing noise in the training data.\n",
        "\n",
        "**Cons of Lasso Classification**:\n",
        "\n",
        "1. Tackling Multicollinearity: L1 regularization may struggle to handle highly correlated features (multicollinearity). In such cases, it tends to keep only one of the correlated features and set the coefficients of the rest to zero arbitrarily, which may not always be the desired behavior.\n",
        "\n",
        "2. Selection Bias: While L1 regularization can be useful for feature selection, it is important to be cautious as the selection process might introduce bias by excluding potentially important features.\n",
        "\n",
        "3. Sensitivity to the Regularization Parameter: The performance of Lasso Classification depends on the choice of the regularization parameter (λ). Selecting the right value requires tuning, and an inappropriate choice can lead to suboptimal results.\n",
        "\n",
        "**When to use Lasso Classification**:\n",
        "\n",
        "Lasso Classification is particularly useful in situations where you have a high-dimensional dataset with many features, and you suspect that only a subset of those features are truly relevant for predicting the outcome. Some common scenarios where Lasso Classification might be a good choice include:\n",
        "\n",
        "1. When you want to perform feature selection automatically and reduce the number of features in your model.\n",
        "\n",
        "2. When you are dealing with a dataset that has many irrelevant or redundant features, and you want to enhance model interpretability.\n",
        "\n",
        "3. When you want to prevent overfitting and improve the generalization of your logistic regression model.\n",
        "\n",
        "It's essential to experiment with different regularization strengths (λ values) and potentially compare the performance of L1 regularization with other regularization techniques like L2 regularization (ridge regression) to find the best approach for your specific problem."
      ],
      "metadata": {
        "id": "g1tdzCMOFvY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "kB9s3AKuAtDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load your dataset or create a sample dataset (X and y)\n",
        "# Replace this with your data loading code or dataset\n",
        "# For demonstration purposes, we'll create a sample dataset\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "num_features = 5\n",
        "\n",
        "X = np.random.randn(num_samples, num_features)\n",
        "y = (X[:, 0] + X[:, 1] - 2*X[:, 2] + 0.5*X[:, 3] > 0).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Lasso Logistic Regression model\n",
        "lasso_logreg = LogisticRegression(penalty='l1', solver='saga', random_state=42, max_iter=10000)\n",
        "lasso_logreg.fit(X_train_std, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = lasso_logreg.score(X_test_std, y_test)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "5IAGviZsSoiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "afJ7mmBuG3eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import necessary libraries:** Import required libraries for data handling, modeling, and evaluation, including `numpy`, `pandas`, `train_test_split` from `sklearn.model_selection`, `StandardScaler` from `sklearn.preprocessing`, and `LogisticRegression` from `sklearn.linear_model`.\n",
        "\n",
        "2. **Load or create a dataset:** In this code, a sample dataset is created for demonstration purposes. It generates `num_samples` (1000) random samples with `num_features` (5) features. The target variable `y` is generated based on a linear combination of the features.\n",
        "\n",
        "3. **Split the data into training and testing sets:** The `train_test_split` function from `sklearn.model_selection` is used to split the dataset into training and testing subsets. It assigns 80% of the data to the training set (`X_train` and `y_train`) and 20% to the testing set (`X_test` and `y_test`). The `random_state` parameter is set to 42 to ensure reproducibility.\n",
        "\n",
        "4. **Standardize the features:** The `StandardScaler` from `sklearn.preprocessing` is used to standardize the feature values. Standardization scales the features to have a mean of 0 and a standard deviation of 1. This step is essential for models like logistic regression, which are sensitive to the scale of the input features. The `fit_transform` method is used to compute the mean and standard deviation of the training data and transform the training set (`X_train_std`). The `transform` method is then used to standardize the test set (`X_test_std`) using the same mean and standard deviation computed from the training set.\n",
        "\n",
        "5. **Create and train the Lasso Logistic Regression model:** In this step, a logistic regression model with L1 regularization (Lasso) is created. Lasso regularization helps in feature selection by encouraging some of the model coefficients to be exactly zero. This results in a sparse model where some features are disregarded. The `LogisticRegression` class from `sklearn.linear_model` is used to create the model. The `penalty='l1'` argument specifies L1 regularization, and the `solver='saga'` argument specifies the optimization algorithm to handle the L1 regularization. The `random_state` parameter is set to 42 for reproducibility. The model is then trained on the standardized training data (`X_train_std` and `y_train`) using the `fit` method.\n",
        "\n",
        "6. **Evaluate the model on the test set:** After training the model, it is evaluated on the test set (`X_test_std` and `y_test`) to assess its performance. The `score` method of the logistic regression model is used to calculate the accuracy of the model on the test data. The accuracy is the proportion of correctly predicted instances over the total number of instances in the test set. The accuracy is then printed to the console using the `print` function.\n",
        "\n",
        "7. **Print test accuracy:** The code prints the test accuracy of the Lasso Logistic Regression model on the test set in percentage format. The accuracy indicates how well the model generalizes to unseen data.\n",
        "\n",
        "Note: The sample dataset and the linear combination used to generate the target variable (`y`) in this code are for demonstration purposes only. In real-world scenarios, you would replace this code with your data loading and preprocessing steps to work with your specific dataset and task."
      ],
      "metadata": {
        "id": "IypVWRY6utnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "BeSkELItSJSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a healthcare setting, Lasso Classification (Logistic Regression with L1 regularization) can be applied for a variety of tasks. Let's consider a real-world example of using Lasso Classification for predicting the risk of heart disease based on patient data.\n",
        "\n",
        "**Example: Heart Disease Risk Prediction**\n",
        "\n",
        "**Problem:**\n",
        "Suppose you have a dataset containing various medical features of patients, such as age, gender, blood pressure, cholesterol levels, etc., along with binary labels indicating whether each patient has heart disease (1) or not (0). The goal is to build a predictive model that can accurately classify patients as having heart disease or not based on their medical features.\n",
        "\n",
        "**Solution:**\n",
        "We can use Lasso Classification, which is a logistic regression model with L1 regularization, to predict the risk of heart disease for each patient. L1 regularization adds a penalty term to the logistic regression cost function, encouraging the model to reduce the coefficients of less important features to zero. This feature selection property of L1 regularization makes it suitable for selecting the most relevant features in the presence of a large number of potential predictors.\n",
        "\n",
        "**Steps:**\n",
        "1. **Data Collection:** Collect data from patients, including various medical features and corresponding binary labels indicating heart disease presence or absence.\n",
        "\n",
        "2. **Data Preprocessing:** Preprocess the data by handling missing values, normalizing numerical features, and encoding categorical variables.\n",
        "\n",
        "3. **Feature Selection:** Apply L1 regularization (Lasso) during logistic regression model training to automatically select the most relevant features from the dataset. The regularization parameter can be adjusted to control the strength of feature selection.\n",
        "\n",
        "4. **Train-Test Split:** Split the dataset into training and testing sets to evaluate the model's performance.\n",
        "\n",
        "5. **Model Training:** Train the Lasso Classification (logistic regression with L1 regularization) model on the training data using a library or framework that supports L1 regularization.\n",
        "\n",
        "6. **Model Evaluation:** Evaluate the model's performance on the testing data using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "7. **Interpretability:** Analyze the model's coefficients to understand the importance of each feature in predicting heart disease risk. Features with non-zero coefficients are considered important predictors of heart disease risk.\n",
        "\n",
        "**Advantages:**\n",
        "- Lasso Classification can handle datasets with a large number of potential predictors (features) by automatically selecting the most relevant ones.\n",
        "- The model provides interpretability, as the non-zero coefficients indicate the importance of each feature in predicting heart disease risk.\n",
        "\n",
        "**Note:**\n",
        "In practice, data privacy and ethical considerations must be taken into account when working with healthcare data. Additionally, to build a robust and accurate model, it is essential to validate the model on a diverse and representative dataset and involve domain experts to ensure the clinical relevance of the chosen features."
      ],
      "metadata": {
        "id": "fep0CAfgCuAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "9Xd66jPNY3HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Lasso Classification, and how does it differ from traditional Logistic Regression?\n",
        "   - Lasso Classification is an extension of traditional Logistic Regression that incorporates L1 regularization. Regularization helps to control the model's complexity and prevent overfitting. L1 regularization specifically adds the absolute value of the coefficient weights to the cost function, promoting sparsity and encouraging some coefficients to be exactly zero.\n",
        "\n",
        "2. How does L1 regularization promote sparsity in the model?\n",
        "   - L1 regularization adds the absolute values of the coefficient weights to the cost function. By doing so, it introduces a penalty for large coefficients, which tends to shrink some of the coefficients to exactly zero. As a result, the Lasso model can effectively select a subset of features that are most relevant to the classification task, making the model more interpretable and efficient.\n",
        "\n",
        "3. When should I use Lasso Classification instead of traditional Logistic Regression?\n",
        "   - Lasso Classification is particularly useful when dealing with high-dimensional datasets with many features. If you suspect that only a few features are relevant for the classification task, Lasso can be a better choice as it can automatically perform feature selection and reduce the model's complexity.\n",
        "\n",
        "4. What is the main advantage of Lasso Classification over other regularization techniques?\n",
        "   - The main advantage of Lasso Classification over other regularization techniques, such as Ridge Regression (L2 regularization), is its ability to induce sparsity. Ridge Regression can shrink the coefficient weights towards zero but rarely sets them to exactly zero. Lasso, on the other hand, can set some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "5. Does Lasso Classification have any limitations or challenges?\n",
        "   - One limitation of Lasso is that when dealing with highly correlated features, it tends to arbitrarily select one of the correlated features and set others to zero. This may lead to instability in the model's performance and makes it less suitable for dealing with highly collinear predictors.\n",
        "\n",
        "6. How do I choose the optimal regularization strength (lambda) for Lasso Classification?\n",
        "   - The optimal regularization strength (lambda) can be determined through cross-validation. By trying different values of lambda and evaluating the model's performance on validation data, you can select the lambda that gives the best trade-off between bias and variance, resulting in the best generalization to new data.\n",
        "\n",
        "7. Can Lasso Classification be used for multi-class classification problems?\n",
        "   - Yes, Lasso Classification can be extended to handle multi-class classification problems using techniques like one-vs-all or softmax regression. The L1 regularization would be applied separately to each class's logistic regression model.\n",
        "\n",
        "8. Are there any libraries that support Lasso Classification in Python or R?\n",
        "   - Yes, many machine learning libraries in Python and R support Lasso Classification. In Python, you can use scikit-learn, and in R, you can use glmnet or glmnetUtils packages to perform Lasso Classification."
      ],
      "metadata": {
        "id": "F5xv4p-A4Qof"
      }
    }
  ]
}