{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+OjuIB4e7WLdcX41BGd9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/AI-models/blob/main/ml/Lasso_Classification_(Logistic_Regression_with_L1_Regularization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Classification (Logistic Regression with L1 Regularization) Model Background"
      ],
      "metadata": {
        "id": "-J8-VBTI-rKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Classification, also known as Logistic Regression with L1 regularization or L1-penalized logistic regression, is a variant of logistic regression that incorporates L1 regularization. Regularization is a technique used to prevent overfitting and improve the generalization of the model. In L1 regularization, the penalty term added to the cost function is proportional to the absolute values of the model's coefficients.\n",
        "\n",
        "In Lasso Classification, the objective function can be represented as follows:\n",
        "\n",
        "Cost Function = -1/m * Σ(y * log(h(x)) + (1 - y) * log(1 - h(x))) + λ * Σ|θi|\n",
        "\n",
        "Where:\n",
        "- m is the number of training examples.\n",
        "- y is the true class labels.\n",
        "- h(x) is the sigmoid function, which predicts the probability of the positive class.\n",
        "- θi represents the model's coefficients (weights).\n",
        "- λ is the regularization parameter, controlling the strength of the penalty term.\n",
        "\n",
        "**Pros of Lasso Classification (Logistic Regression with L1 Regularization)**:\n",
        "\n",
        "1. Feature Selection: L1 regularization has the property of driving some coefficients to exactly zero. This leads to automatic feature selection, as some features become irrelevant and effectively excluded from the model. This can be helpful in situations where you suspect that only a subset of features are relevant for making predictions.\n",
        "\n",
        "2. Simplicity and Interpretability: By eliminating irrelevant features, Lasso Classification can lead to simpler and more interpretable models, making it easier to understand the impact of individual features on the predicted outcomes.\n",
        "\n",
        "3. Robustness to Overfitting: L1 regularization helps prevent overfitting by discouraging overly complex models. It achieves a form of dimensionality reduction, reducing the risk of memorizing noise in the training data.\n",
        "\n",
        "**Cons of Lasso Classification**:\n",
        "\n",
        "1. Tackling Multicollinearity: L1 regularization may struggle to handle highly correlated features (multicollinearity). In such cases, it tends to keep only one of the correlated features and set the coefficients of the rest to zero arbitrarily, which may not always be the desired behavior.\n",
        "\n",
        "2. Selection Bias: While L1 regularization can be useful for feature selection, it is important to be cautious as the selection process might introduce bias by excluding potentially important features.\n",
        "\n",
        "3. Sensitivity to the Regularization Parameter: The performance of Lasso Classification depends on the choice of the regularization parameter (λ). Selecting the right value requires tuning, and an inappropriate choice can lead to suboptimal results.\n",
        "\n",
        "**When to use Lasso Classification**:\n",
        "\n",
        "Lasso Classification is particularly useful in situations where you have a high-dimensional dataset with many features, and you suspect that only a subset of those features are truly relevant for predicting the outcome. Some common scenarios where Lasso Classification might be a good choice include:\n",
        "\n",
        "1. When you want to perform feature selection automatically and reduce the number of features in your model.\n",
        "\n",
        "2. When you are dealing with a dataset that has many irrelevant or redundant features, and you want to enhance model interpretability.\n",
        "\n",
        "3. When you want to prevent overfitting and improve the generalization of your logistic regression model.\n",
        "\n",
        "It's essential to experiment with different regularization strengths (λ values) and potentially compare the performance of L1 regularization with other regularization techniques like L2 regularization (ridge regression) to find the best approach for your specific problem."
      ],
      "metadata": {
        "id": "g1tdzCMOFvY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "kB9s3AKuAtDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load your dataset or create a sample dataset (X and y)\n",
        "# Replace this with your data loading code or dataset\n",
        "# For demonstration purposes, we'll create a sample dataset\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "num_features = 5\n",
        "\n",
        "X = np.random.randn(num_samples, num_features)\n",
        "y = (X[:, 0] + X[:, 1] - 2*X[:, 2] + 0.5*X[:, 3] > 0).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Lasso Logistic Regression model\n",
        "lasso_logreg = LogisticRegression(penalty='l1', solver='saga', random_state=42, max_iter=10000)\n",
        "lasso_logreg.fit(X_train_std, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = lasso_logreg.score(X_test_std, y_test)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "5IAGviZsSoiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "afJ7mmBuG3eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import necessary libraries:** Import required libraries for data handling, modeling, and evaluation, including `numpy`, `pandas`, `train_test_split` from `sklearn.model_selection`, `StandardScaler` from `sklearn.preprocessing`, and `LogisticRegression` from `sklearn.linear_model`.\n",
        "\n",
        "2. **Load or create a dataset:** In this code, a sample dataset is created for demonstration purposes. It generates `num_samples` (1000) random samples with `num_features` (5) features. The target variable `y` is generated based on a linear combination of the features.\n",
        "\n",
        "3. **Split the data into training and testing sets:** The `train_test_split` function from `sklearn.model_selection` is used to split the dataset into training and testing subsets. It assigns 80% of the data to the training set (`X_train` and `y_train`) and 20% to the testing set (`X_test` and `y_test`). The `random_state` parameter is set to 42 to ensure reproducibility.\n",
        "\n",
        "4. **Standardize the features:** The `StandardScaler` from `sklearn.preprocessing` is used to standardize the feature values. Standardization scales the features to have a mean of 0 and a standard deviation of 1. This step is essential for models like logistic regression, which are sensitive to the scale of the input features. The `fit_transform` method is used to compute the mean and standard deviation of the training data and transform the training set (`X_train_std`). The `transform` method is then used to standardize the test set (`X_test_std`) using the same mean and standard deviation computed from the training set.\n",
        "\n",
        "5. **Create and train the Lasso Logistic Regression model:** In this step, a logistic regression model with L1 regularization (Lasso) is created. Lasso regularization helps in feature selection by encouraging some of the model coefficients to be exactly zero. This results in a sparse model where some features are disregarded. The `LogisticRegression` class from `sklearn.linear_model` is used to create the model. The `penalty='l1'` argument specifies L1 regularization, and the `solver='saga'` argument specifies the optimization algorithm to handle the L1 regularization. The `random_state` parameter is set to 42 for reproducibility. The model is then trained on the standardized training data (`X_train_std` and `y_train`) using the `fit` method.\n",
        "\n",
        "6. **Evaluate the model on the test set:** After training the model, it is evaluated on the test set (`X_test_std` and `y_test`) to assess its performance. The `score` method of the logistic regression model is used to calculate the accuracy of the model on the test data. The accuracy is the proportion of correctly predicted instances over the total number of instances in the test set. The accuracy is then printed to the console using the `print` function.\n",
        "\n",
        "7. **Print test accuracy:** The code prints the test accuracy of the Lasso Logistic Regression model on the test set in percentage format. The accuracy indicates how well the model generalizes to unseen data.\n",
        "\n",
        "Note: The sample dataset and the linear combination used to generate the target variable (`y`) in this code are for demonstration purposes only. In real-world scenarios, you would replace this code with your data loading and preprocessing steps to work with your specific dataset and task."
      ],
      "metadata": {
        "id": "IypVWRY6utnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "BeSkELItSJSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a healthcare setting, Lasso Classification (Logistic Regression with L1 regularization) can be applied for a variety of tasks. Let's consider a real-world example of using Lasso Classification for predicting the risk of heart disease based on patient data.\n",
        "\n",
        "**Example: Heart Disease Risk Prediction**\n",
        "\n",
        "**Problem:**\n",
        "Suppose you have a dataset containing various medical features of patients, such as age, gender, blood pressure, cholesterol levels, etc., along with binary labels indicating whether each patient has heart disease (1) or not (0). The goal is to build a predictive model that can accurately classify patients as having heart disease or not based on their medical features.\n",
        "\n",
        "**Solution:**\n",
        "We can use Lasso Classification, which is a logistic regression model with L1 regularization, to predict the risk of heart disease for each patient. L1 regularization adds a penalty term to the logistic regression cost function, encouraging the model to reduce the coefficients of less important features to zero. This feature selection property of L1 regularization makes it suitable for selecting the most relevant features in the presence of a large number of potential predictors.\n",
        "\n",
        "**Steps:**\n",
        "1. **Data Collection:** Collect data from patients, including various medical features and corresponding binary labels indicating heart disease presence or absence.\n",
        "\n",
        "2. **Data Preprocessing:** Preprocess the data by handling missing values, normalizing numerical features, and encoding categorical variables.\n",
        "\n",
        "3. **Feature Selection:** Apply L1 regularization (Lasso) during logistic regression model training to automatically select the most relevant features from the dataset. The regularization parameter can be adjusted to control the strength of feature selection.\n",
        "\n",
        "4. **Train-Test Split:** Split the dataset into training and testing sets to evaluate the model's performance.\n",
        "\n",
        "5. **Model Training:** Train the Lasso Classification (logistic regression with L1 regularization) model on the training data using a library or framework that supports L1 regularization.\n",
        "\n",
        "6. **Model Evaluation:** Evaluate the model's performance on the testing data using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "7. **Interpretability:** Analyze the model's coefficients to understand the importance of each feature in predicting heart disease risk. Features with non-zero coefficients are considered important predictors of heart disease risk.\n",
        "\n",
        "**Advantages:**\n",
        "- Lasso Classification can handle datasets with a large number of potential predictors (features) by automatically selecting the most relevant ones.\n",
        "- The model provides interpretability, as the non-zero coefficients indicate the importance of each feature in predicting heart disease risk.\n",
        "\n",
        "**Note:**\n",
        "In practice, data privacy and ethical considerations must be taken into account when working with healthcare data. Additionally, to build a robust and accurate model, it is essential to validate the model on a diverse and representative dataset and involve domain experts to ensure the clinical relevance of the chosen features."
      ],
      "metadata": {
        "id": "fep0CAfgCuAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "9Xd66jPNY3HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Lasso Classification, and how does it differ from traditional Logistic Regression?\n",
        "   - Lasso Classification is an extension of traditional Logistic Regression that incorporates L1 regularization. Regularization helps to control the model's complexity and prevent overfitting. L1 regularization specifically adds the absolute value of the coefficient weights to the cost function, promoting sparsity and encouraging some coefficients to be exactly zero.\n",
        "\n",
        "2. How does L1 regularization promote sparsity in the model?\n",
        "   - L1 regularization adds the absolute values of the coefficient weights to the cost function. By doing so, it introduces a penalty for large coefficients, which tends to shrink some of the coefficients to exactly zero. As a result, the Lasso model can effectively select a subset of features that are most relevant to the classification task, making the model more interpretable and efficient.\n",
        "\n",
        "3. When should I use Lasso Classification instead of traditional Logistic Regression?\n",
        "   - Lasso Classification is particularly useful when dealing with high-dimensional datasets with many features. If you suspect that only a few features are relevant for the classification task, Lasso can be a better choice as it can automatically perform feature selection and reduce the model's complexity.\n",
        "\n",
        "4. What is the main advantage of Lasso Classification over other regularization techniques?\n",
        "   - The main advantage of Lasso Classification over other regularization techniques, such as Ridge Regression (L2 regularization), is its ability to induce sparsity. Ridge Regression can shrink the coefficient weights towards zero but rarely sets them to exactly zero. Lasso, on the other hand, can set some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "5. Does Lasso Classification have any limitations or challenges?\n",
        "   - One limitation of Lasso is that when dealing with highly correlated features, it tends to arbitrarily select one of the correlated features and set others to zero. This may lead to instability in the model's performance and makes it less suitable for dealing with highly collinear predictors.\n",
        "\n",
        "6. How do I choose the optimal regularization strength (lambda) for Lasso Classification?\n",
        "   - The optimal regularization strength (lambda) can be determined through cross-validation. By trying different values of lambda and evaluating the model's performance on validation data, you can select the lambda that gives the best trade-off between bias and variance, resulting in the best generalization to new data.\n",
        "\n",
        "7. Can Lasso Classification be used for multi-class classification problems?\n",
        "   - Yes, Lasso Classification can be extended to handle multi-class classification problems using techniques like one-vs-all or softmax regression. The L1 regularization would be applied separately to each class's logistic regression model.\n",
        "\n",
        "8. Are there any libraries that support Lasso Classification in Python or R?\n",
        "   - Yes, many machine learning libraries in Python and R support Lasso Classification. In Python, you can use scikit-learn, and in R, you can use glmnet or glmnetUtils packages to perform Lasso Classification."
      ],
      "metadata": {
        "id": "F5xv4p-A4Qof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz"
      ],
      "metadata": {
        "id": "XcWgHPFWeskR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:** What is the main goal of using Lasso regularization in logistic regression?\n",
        "\n",
        "a) To improve model performance by increasing the model complexity.\n",
        "b) To reduce the bias in the model predictions.\n",
        "c) To prevent overfitting and perform feature selection.\n",
        "d) To increase the interpretability of the model.\n",
        "\n",
        "**Question 2:** How does Lasso regularization achieve feature selection in logistic regression?\n",
        "\n",
        "a) By adding a penalty term proportional to the square of the coefficients.\n",
        "b) By adding a penalty term proportional to the absolute value of the coefficients.\n",
        "c) By removing random features from the dataset.\n",
        "d) By increasing the number of iterations in the logistic regression algorithm.\n",
        "\n",
        "**Question 3:** In Lasso regularization, as the value of the regularization parameter (λ) increases:\n",
        "\n",
        "a) The model complexity increases.\n",
        "b) The penalty term becomes less effective.\n",
        "c) More features are retained in the model.\n",
        "d) The coefficients of less important features are pushed towards zero.\n",
        "\n",
        "**Question 4:** Which of the following is a potential disadvantage of using a very large value of the regularization parameter (λ) in Lasso regularization?\n",
        "\n",
        "a) Increased risk of overfitting.\n",
        "b) Loss of important information from the data.\n",
        "c) Faster convergence of the optimization algorithm.\n",
        "d) Improved interpretability of the model.\n",
        "\n",
        "**Question 5:** Lasso regression can be used for:\n",
        "\n",
        "a) Only binary classification problems.\n",
        "b) Both binary and multi-class classification problems.\n",
        "c) Only regression problems, not classification.\n",
        "d) Time series forecasting.\n",
        "\n",
        "**Question 6:** In Lasso classification, what happens when the regularization parameter (λ) is set to zero?\n",
        "\n",
        "a) All coefficients are set to zero.\n",
        "b) No regularization is applied.\n",
        "c) The model becomes more complex.\n",
        "d) The model becomes unstable.\n",
        "\n",
        "**Question 7:** Which type of regularization term does Lasso regression add to the cost function during optimization?\n",
        "\n",
        "a) L1 regularization term.\n",
        "b) L2 regularization term.\n",
        "c) L1 and L2 regularization terms.\n",
        "d) L0 regularization term.\n",
        "\n",
        "**Question 8:** Compared to traditional logistic regression without regularization, Lasso classification:\n",
        "\n",
        "a) Always performs worse.\n",
        "b) Always performs better.\n",
        "c) Can perform either better or worse depending on the dataset.\n",
        "d) Produces the same results regardless of the dataset.\n",
        "\n",
        "**Question 9:** When should you consider using Lasso regularization in your logistic regression model?\n",
        "\n",
        "a) When you want to make your model more complex.\n",
        "b) When you have a small number of features and want to prevent overfitting.\n",
        "c) When you want to ignore the relationships between features.\n",
        "d) When you want to use only L2 regularization.\n",
        "\n",
        "**Question 10:** What is a key advantage of Lasso regularization over Ridge regularization?\n",
        "\n",
        "a) Lasso works better with a large number of features.\n",
        "b) Lasso is computationally more efficient.\n",
        "c) Lasso can handle multicollinearity between features.\n",
        "d) Lasso is less sensitive to the choice of the regularization parameter.\n",
        "\n",
        "**Answers:**\n",
        "1. c) To prevent overfitting and perform feature selection.\n",
        "2. b) By adding a penalty term proportional to the absolute value of the coefficients.\n",
        "3. d) The coefficients of less important features are pushed towards zero.\n",
        "4. b) Loss of important information from the data.\n",
        "5. b) Both binary and multi-class classification problems.\n",
        "6. b) No regularization is applied.\n",
        "7. a) L1 regularization term.\n",
        "8. c) Can perform either better or worse depending on the dataset.\n",
        "9. b) When you have a small number of features and want to prevent overfitting.\n",
        "10. c) Lasso can handle multicollinearity between features."
      ],
      "metadata": {
        "id": "nDufsia4et2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ideas"
      ],
      "metadata": {
        "id": "lYhT9MlyxO_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Predicting Disease Onset**\n",
        "   - **Data**: Patient data with various health metrics (e.g., blood pressure, cholesterol levels, BMI) and a binary outcome for a specific disease (e.g., diabetes: yes/no).\n",
        "   - **Task**: Use Lasso Classification to predict the likelihood of disease onset and determine which health metrics are the most significant predictors.\n",
        "\n",
        "2. **Drug Response Prediction**\n",
        "   - **Data**: Patient demographic data, genetic information, and drug response (effective/not effective).\n",
        "   - **Task**: Predict drug efficacy based on patient's information and find out the most important factors affecting drug response using Lasso.\n",
        "\n",
        "3. **Patient Readmission Prediction**\n",
        "   - **Data**: Hospital readmission data, including patient demographics, diagnosis, treatment details, and readmission status.\n",
        "   - **Task**: Predict whether a patient will be readmitted within 30 days and identify the key variables responsible for readmissions.\n",
        "\n",
        "4. **Modeling Disease Progression**\n",
        "   - **Data**: Longitudinal data for patients with progressive diseases, e.g., Alzheimer's or Parkinson's, with various measurements taken over time.\n",
        "   - **Task**: Use Lasso to model the progression of the disease and find out which variables are key predictors of accelerated progression.\n",
        "\n",
        "5. **Optimizing Hospital Resource Allocation**\n",
        "   - **Data**: Hospital data including patient inflow, type of treatments, bed occupancy, and treatment outcomes.\n",
        "   - **Task**: Predict the requirement of resources (like beds in ICU) in the near future, using Lasso to find out the most influential predictors of resource demand.\n",
        "\n",
        "6. **Predictive Modeling for Medical Expenditure**\n",
        "   - **Data**: Patient demographic data, health status, and annual medical expenditure.\n",
        "   - **Task**: Predict a patient's annual medical expenses and discover the major health and demographic predictors of high medical costs.\n",
        "\n",
        "7. **Classification of Medical Imaging**\n",
        "   - **Data**: Extracted features from medical images (like MRI or CT scans) and diagnostic outcomes.\n",
        "   - **Task**: Classify the images into categories (e.g., tumor present/absent) and identify the most crucial features in these images that determine the diagnosis.\n",
        "\n",
        "8. **Genetic Data and Disease Risk**\n",
        "   - **Data**: Genetic data (e.g., SNP data) of individuals and their disease status for specific genetic diseases.\n",
        "   - **Task**: Use Lasso to classify individuals at risk based on their genetic makeup and highlight the most influential genes or markers.\n",
        "\n",
        "9. **Healthcare Fraud Detection**\n",
        "   - **Data**: Claims data, patient information, treatment details, and fraud status (fraudulent/legitimate).\n",
        "   - **Task**: Predict fraudulent claims and determine which variables are the most telling signs of fraudulent activity.\n",
        "\n",
        "10. **Treatment Effectiveness**\n",
        "   - **Data**: Data on various treatments given to patients for a particular condition and the outcomes.\n",
        "   - **Task**: Predict the effectiveness of treatments and find out the most significant predictors of positive treatment outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "HQfi-EKRxanm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Example"
      ],
      "metadata": {
        "id": "GXHneZzLDIbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple example of implementing a Lasso Classification (Logistic Regression with L1 Regularization) model using a real-world health dataset. In this example, I'll use the Breast Cancer Wisconsin dataset, which is available in scikit-learn's datasets module. The goal of this dataset is to classify breast cancer tumors as malignant or benign based on various features."
      ],
      "metadata": {
        "id": "hY36lvixtSWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Lasso Logistic Regression model\n",
        "lasso_model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "jx-7PJRCs-dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Breast Cancer Wisconsin dataset and split it into training and testing sets. We then standardize the features using StandardScaler to ensure that they have zero mean and unit variance.\n",
        "\n",
        "We create a LogisticRegression model with L1 regularization by setting penalty='l1' and using the 'liblinear' solver, which is suitable for small datasets like this one. We train the model on the scaled training data and make predictions on the scaled test data. Finally, we calculate and print the accuracy of the model on the test set.\n",
        "\n",
        "Keep in mind that this example is quite simplified. In practice, you might want to perform hyperparameter tuning and cross-validation to optimize the model's performance. Also, working with real-world datasets often involves data preprocessing, feature selection, and more extensive evaluation of the model's performance."
      ],
      "metadata": {
        "id": "CSXhlhmktX49"
      }
    }
  ]
}